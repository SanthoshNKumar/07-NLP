{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Word Embedding is a representation of text where words that have the same meaning have a similar representation. \n",
    "In other words it represents words in a coordinate system where related words, based on a corpus of relationships, \n",
    "are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled \n",
    "by an embedding layer which stores a lookup table to map the words represented by numeric indexes to their dense \n",
    "vectorrepresentations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Deep network takes the sequence of embedding vectors as input and converts them to a compressed representation. \n",
    "The compressed representation effectively captures all the information in the sequence of words in the text. \n",
    "The deep neywrok part is usually an RNN or some forms of it like LSTM/GRU. The dropout is added to overcome the \n",
    "tendency to overfit, a very common problem with RNN based networks. Please refer here for detailed discussion on LSTM,GRU.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The fully connected layer takes the deep representation from the RNN/LSTM/GRU and transforms \n",
    "it into the final output classes or class scores. This component is comprised of fully connected layers \n",
    "along with batch normalization and optionally dropout layers for regularization.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Based on the problem at hand, this layer can have either Sigmoid for binary classification or Softmax for both\n",
    "binary and multi classification output.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\MyWork\\\\MyLearning\\\\ML\\\\Files\\\\DataSet\\\\movie_data.csv\",encoding='utf-8').sample(n=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26779</td>\n",
       "      <td>I was impressed with this film because of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47092</td>\n",
       "      <td>The best bond game made of all systems. It was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17779</td>\n",
       "      <td>Beautifully made with a wonderful performance ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39526</td>\n",
       "      <td>What a stupid idea. Ewoks should be enslaved a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34147</td>\n",
       "      <td>Please don't waste your money on this sorry ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "26779  I was impressed with this film because of the ...          1\n",
       "47092  The best bond game made of all systems. It was...          1\n",
       "17779  Beautifully made with a wonderful performance ...          1\n",
       "39526  What a stupid idea. Ewoks should be enslaved a...          0\n",
       "34147  Please don't waste your money on this sorry ex...          0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = df.review.iloc[:499]\n",
    "y_train = df.sentiment.iloc[:499]\n",
    "\n",
    "X_test = df.review.iloc[500:]\n",
    "y_test = df.sentiment.iloc[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "total_reviews  = df.review\n",
    "\n",
    "tokenizer_obj.fit_on_texts(total_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19170\n"
     ]
    }
   ],
   "source": [
    "# Define Vocabulary Size\n",
    "vocab_size = len(tokenizer_obj.word_index) +1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723\n"
     ]
    }
   ],
   "source": [
    "# pad sequence\n",
    "max_length = max([len(s.split()) for s in total_reviews])\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_tokens,maxlen=max_length,padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens,maxlen=max_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1723, 100)         1917000   \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,929,801\n",
      "Trainable params: 1,929,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding,GRU\n",
    "\n",
    "# Initialize Model\n",
    "model = Sequential()\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Add Model Layer\n",
    "model.add(Embedding(vocab_size,EMBEDDING_DIM,input_length=max_length))\n",
    "model.add(GRU(units=32,dropout = 0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer = 'adam',loss='binary_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 499 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "499/499 - 8s - loss: 0.6943 - acc: 0.4810 - val_loss: 0.6932 - val_acc: 0.4940\n",
      "Epoch 2/25\n",
      "499/499 - 7s - loss: 0.6935 - acc: 0.5251 - val_loss: 0.6947 - val_acc: 0.4940\n",
      "Epoch 3/25\n",
      "499/499 - 7s - loss: 0.6929 - acc: 0.5210 - val_loss: 0.6956 - val_acc: 0.4940\n",
      "Epoch 4/25\n",
      "499/499 - 7s - loss: 0.6927 - acc: 0.5210 - val_loss: 0.6958 - val_acc: 0.4940\n",
      "Epoch 5/25\n",
      "499/499 - 7s - loss: 0.6922 - acc: 0.5210 - val_loss: 0.6951 - val_acc: 0.4940\n",
      "Epoch 6/25\n",
      "499/499 - 7s - loss: 0.6915 - acc: 0.5210 - val_loss: 0.6946 - val_acc: 0.4940\n",
      "Epoch 7/25\n",
      "499/499 - 7s - loss: 0.6917 - acc: 0.5210 - val_loss: 0.6944 - val_acc: 0.4940\n",
      "Epoch 8/25\n",
      "499/499 - 7s - loss: 0.6924 - acc: 0.5210 - val_loss: 0.6942 - val_acc: 0.4940\n",
      "Epoch 9/25\n",
      "499/499 - 7s - loss: 0.6917 - acc: 0.5210 - val_loss: 0.6942 - val_acc: 0.4940\n",
      "Epoch 10/25\n",
      "499/499 - 7s - loss: 0.6928 - acc: 0.5210 - val_loss: 0.6941 - val_acc: 0.4940\n",
      "Epoch 11/25\n",
      "499/499 - 7s - loss: 0.6927 - acc: 0.5190 - val_loss: 0.6943 - val_acc: 0.4940\n",
      "Epoch 12/25\n",
      "499/499 - 7s - loss: 0.6926 - acc: 0.5210 - val_loss: 0.6944 - val_acc: 0.4940\n",
      "Epoch 13/25\n",
      "499/499 - 7s - loss: 0.6926 - acc: 0.5210 - val_loss: 0.6945 - val_acc: 0.4940\n",
      "Epoch 14/25\n",
      "499/499 - 7s - loss: 0.6927 - acc: 0.5210 - val_loss: 0.6948 - val_acc: 0.4940\n",
      "Epoch 15/25\n",
      "499/499 - 7s - loss: 0.6921 - acc: 0.5210 - val_loss: 0.6948 - val_acc: 0.4940\n",
      "Epoch 16/25\n",
      "499/499 - 7s - loss: 0.6924 - acc: 0.5210 - val_loss: 0.6945 - val_acc: 0.4940\n",
      "Epoch 17/25\n",
      "499/499 - 8s - loss: 0.6927 - acc: 0.5210 - val_loss: 0.6945 - val_acc: 0.4940\n",
      "Epoch 18/25\n",
      "499/499 - 7s - loss: 0.6924 - acc: 0.5210 - val_loss: 0.6940 - val_acc: 0.4940\n",
      "Epoch 19/25\n",
      "499/499 - 8s - loss: 0.6919 - acc: 0.5190 - val_loss: 0.6940 - val_acc: 0.4940\n",
      "Epoch 20/25\n",
      "499/499 - 8s - loss: 0.6925 - acc: 0.5210 - val_loss: 0.6942 - val_acc: 0.4940\n",
      "Epoch 21/25\n",
      "499/499 - 8s - loss: 0.6918 - acc: 0.5210 - val_loss: 0.6942 - val_acc: 0.4940\n",
      "Epoch 22/25\n",
      "499/499 - 8s - loss: 0.6924 - acc: 0.5210 - val_loss: 0.6944 - val_acc: 0.4940\n",
      "Epoch 23/25\n",
      "499/499 - 8s - loss: 0.6925 - acc: 0.5210 - val_loss: 0.6945 - val_acc: 0.4940\n",
      "Epoch 24/25\n",
      "499/499 - 8s - loss: 0.6923 - acc: 0.5210 - val_loss: 0.6945 - val_acc: 0.4940\n",
      "Epoch 25/25\n",
      "499/499 - 8s - loss: 0.6914 - acc: 0.5210 - val_loss: 0.6945 - val_acc: 0.4940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29c45932b88>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "model.fit(X_train_pad,y_train,batch_size=128,epochs=25,validation_data=(X_test_pad,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model\n",
    "\n",
    "test_sample_1 = \"This movie is fantastic! I really like it because it is so  good!\"\n",
    "test_sample_2 = \"Good Movie!\"\n",
    "test_sample_3 = \"Maybe I Like this movie\"\n",
    "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
    "test_sample_5 = \"if you like action, then this movie might be good for you\"\n",
    "test_sample_6 = \"Bad movie!\"\n",
    "test_sample_7 = \"Not a good movie!\"\n",
    "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "\n",
    "test_samples = [test_sample_1,test_sample_2,test_sample_3,test_sample_4,test_sample_5,test_sample_6,test_sample_7,test_sample_8]\n",
    "\n",
    "test_sample_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "\n",
    "test_sample_token_pad = pad_sequences(test_sample_tokens,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49367246],\n",
       "       [0.4885422 ],\n",
       "       [0.49148762],\n",
       "       [0.494942  ],\n",
       "       [0.48225394],\n",
       "       [0.4866268 ],\n",
       "       [0.5055415 ],\n",
       "       [0.4927562 ]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output gives the prediction of the word either to be 1 (positive sentiment) or 0 (negative sentiment).\n",
    "\n",
    "# Predict\n",
    "model.predict(test_sample_token_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
