{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Quo usque tandem abutere, Catilina, patientia nostra? Quamdiu etiam furor iste tuus nos eludet? Quem ad finem sese effrenata iactabit audacia?\"\n",
    "\n",
    "tokenized = re.sub('[,?.]','', text).lower().split(' ') #Let's tokenize our text by just take each word\n",
    "vocab = {k:v for v,k in enumerate(np.unique(tokenized))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abutere': 0,\n",
       " 'ad': 1,\n",
       " 'audacia': 2,\n",
       " 'catilina': 3,\n",
       " 'effrenata': 4,\n",
       " 'eludet': 5,\n",
       " 'etiam': 6,\n",
       " 'finem': 7,\n",
       " 'furor': 8,\n",
       " 'iactabit': 9,\n",
       " 'iste': 10,\n",
       " 'nos': 11,\n",
       " 'nostra': 12,\n",
       " 'patientia': 13,\n",
       " 'quamdiu': 14,\n",
       " 'quem': 15,\n",
       " 'quo': 16,\n",
       " 'sese': 17,\n",
       " 'tandem': 18,\n",
       " 'tuus': 19,\n",
       " 'usque': 20}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to define the Embedding size, so the dimension of each vector, in our case 50, and the vocabulary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 50\n",
    "VOCAB_LEN = len(vocab.keys())\n",
    "\n",
    "print(VOCAB_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know need to define the ids of the words we want to embed. Just for example, we are going to take abutere and patientia\n",
    "\n",
    "words_ids = tf.constant([vocab[\"abutere\"], vocab[\"patientia\"]])\n",
    "\n",
    "# words_ids represent the ids of some words in a vocabulary. A vocabulary is a map of words (tokens) to ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94159853 0.47791886 0.6757828  0.7451253  0.67886007 0.67146075\n",
      "  0.14859271 0.42767048 0.9956533  0.6282997  0.5976262  0.02285182\n",
      "  0.94600546 0.9259826  0.5596858  0.74445415 0.9257264  0.00419676\n",
      "  0.1787237  0.72174835 0.25012636 0.5517918  0.7234329  0.8347409\n",
      "  0.25397623 0.5264813  0.83136034 0.04862249 0.8449614  0.03566229\n",
      "  0.5097456  0.7491182  0.26026833 0.8288218  0.18477559 0.82419276\n",
      "  0.8491255  0.3262117  0.60015523 0.6737932  0.38296258 0.41750813\n",
      "  0.1519301  0.595335   0.4984163  0.6633427  0.37720454 0.3542471\n",
      "  0.8745059  0.38002026]\n",
      " [0.67347205 0.7986989  0.96490455 0.74781907 0.16274679 0.57981133\n",
      "  0.13682342 0.8484597  0.6320871  0.44620407 0.7793801  0.95069027\n",
      "  0.19091463 0.5246465  0.19539988 0.17891133 0.21647453 0.5682733\n",
      "  0.5725316  0.71901083 0.8867103  0.14238453 0.17147994 0.9695312\n",
      "  0.5529585  0.26986146 0.67527103 0.73019993 0.8661417  0.52650523\n",
      "  0.7738441  0.5870944  0.73371124 0.83653367 0.5228075  0.0424813\n",
      "  0.10963345 0.9285134  0.6804383  0.38136733 0.6388742  0.90614533\n",
      "  0.24389291 0.5394106  0.5376055  0.44956112 0.26422346 0.09157896\n",
      "  0.25812995 0.52459383]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_LEN, EMBED_SIZE]))\n",
    "embed = tf.nn.embedding_lookup(embeddings, words_ids)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00303479 -0.02560792 -0.00341245  0.02672969 -0.02263589  0.00113025\n",
      "   0.03898313  0.00759194  0.03284452  0.03830725  0.0154817   0.00705725\n",
      "  -0.01198811 -0.04363779 -0.03399888 -0.0059553   0.00060582  0.02232868\n",
      "   0.03099075 -0.04960848  0.04739812 -0.00901444  0.04343739 -0.02876884\n",
      "  -0.00928445 -0.01474331 -0.03673764 -0.00927904  0.02522433 -0.00591274\n",
      "   0.04068582  0.0291919   0.04751973  0.02993834  0.00733216  0.02753672\n",
      "   0.02186326 -0.02682384 -0.02130427  0.0350327  -0.0122747  -0.00767615\n",
      "  -0.03839944  0.04837588  0.00409646 -0.03049152 -0.03555983  0.0379563\n",
      "  -0.04907681 -0.00277138]\n",
      " [-0.03026829 -0.00636082 -0.02106096  0.00889608  0.03470941  0.01518322\n",
      "   0.03276154 -0.04161562 -0.01552086  0.00311099  0.01026328  0.04158516\n",
      "  -0.02565833  0.00613364  0.00676235 -0.01882094  0.03703779  0.0442528\n",
      "  -0.0229293  -0.02409744  0.03482821  0.00021745 -0.03824664 -0.0464901\n",
      "   0.00405698  0.00394703  0.0169821   0.02284623  0.01580958 -0.04783583\n",
      "   0.02679542 -0.04436096  0.03860912 -0.04293443 -0.00840297 -0.00269578\n",
      "  -0.0211836   0.0030141  -0.00209279  0.00245887  0.03262271  0.00754631\n",
      "  -0.00498859 -0.01461409  0.04959695  0.04338403 -0.02373906 -0.03444263\n",
      "  -0.025852   -0.01445777]]\n"
     ]
    }
   ],
   "source": [
    "# Using Keras Layer\n",
    "import tensorflow as tf\n",
    "embeddings = tf.keras.layers.Embedding(VOCAB_LEN, EMBED_SIZE)\n",
    "embed = embeddings(words_ids)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "[[-1.60786927e-01 -1.75933480e-01 -8.23026001e-02 -1.92231983e-01\n",
      "  -2.39409119e-01 -1.46477431e-01  1.53587371e-01  1.55653387e-01\n",
      "  -7.78972208e-02  1.20996445e-01 -2.74368942e-01  1.66701645e-01\n",
      "   2.82338828e-01 -1.49694175e-01  1.44037008e-02  1.23237461e-01\n",
      "   1.99615955e-04 -2.34681368e-01 -1.78386301e-01  2.66641170e-01\n",
      "   9.37233269e-02  2.49281377e-01 -1.17825568e-01  1.73913747e-01\n",
      "  -2.60450423e-01  1.93151921e-01 -2.54842252e-01  1.86541617e-01\n",
      "   1.90132916e-01  3.91776860e-02  6.50626719e-02  2.72292644e-01\n",
      "   2.06192672e-01 -1.86209768e-01 -7.54587352e-02 -2.16853917e-01\n",
      "  -8.09297413e-02 -9.94452238e-02  4.15234864e-02  1.32735699e-01\n",
      "   4.18059230e-02 -2.34100014e-01 -2.41471037e-01  2.83372968e-01\n",
      "  -2.88663268e-01 -2.45274901e-01 -1.59731016e-01  1.04475856e-01\n",
      "  -1.21344775e-01 -1.72360897e-01]\n",
      " [ 5.21044731e-02 -1.30804211e-01 -2.35195160e-01 -3.15710008e-02\n",
      "  -2.55173534e-01 -3.25465202e-03 -6.89096302e-02  1.14385098e-01\n",
      "  -2.14025766e-01 -3.40463519e-02  1.09846085e-01  9.11478102e-02\n",
      "   2.67731875e-01 -3.10675502e-02 -2.90149659e-01  1.64583296e-01\n",
      "  -6.39232993e-02  1.87836945e-01 -2.23203182e-01 -2.71618724e-01\n",
      "   7.69436061e-02 -4.72385287e-02  2.35746175e-01  1.36810333e-01\n",
      "  -1.41128406e-01  5.24935126e-03  1.80278152e-01  9.57844853e-02\n",
      "  -1.16864115e-01 -7.80825466e-02  1.38834924e-01  2.18142539e-01\n",
      "  -6.99960440e-02  2.43432730e-01 -1.28592849e-01  2.00241089e-01\n",
      "  -1.63778067e-02  3.13702822e-02 -2.50266969e-01  1.89339757e-01\n",
      "  -6.20930791e-02 -6.70308173e-02 -2.06904054e-01  7.57922530e-02\n",
      "  -8.37645233e-02  2.30285615e-01 -9.70145166e-02 -2.06418037e-02\n",
      "  -1.48872659e-01  2.32263476e-01]]\n"
     ]
    }
   ],
   "source": [
    "# using Tensorflow Layers\n",
    "\n",
    "embed = tf.contrib.layers.embed_sequence(ids=words_ids, vocab_size=VOCAB_LEN, embed_dim=EMBED_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(embed))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
