{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data = [['The', 'King', 'James', 'Bible'], \n",
    "        ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'], \n",
    "        ['The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis'], \n",
    "        ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth'], \n",
    "        ['And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.'], \n",
    "        ['And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.'], \n",
    "        ['And', 'God', 'said', ',', 'Let', 'there', 'be', 'light', ':', 'and', 'there', 'was', 'light', '.'], \n",
    "        ['And', 'God', 'saw', 'the', 'light', ',', 'that', 'it', 'was', 'good', ':', 'and', 'God', 'divided', 'the', 'light', 'from', 'the', 'darkness', '.'], \n",
    "        ['And', 'God', 'called', 'the', 'light', 'Day', ',', 'and', 'the', 'darkness', 'he', 'called', 'Night', '.'], \n",
    "        ['And', 'the', 'evening', 'and', 'the', 'morning', 'were', 'the', 'first', 'day', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [1, 5, 6, 7, 8, 9, 10]]\n",
      "vocab_size =10\n",
      "Vocabulary Sample: [('the', 1), ('king', 2), ('james', 3), ('bible', 4), ('first', 5), ('book', 6), ('of', 7), ('moses', 8), ('called', 9), ('genesis', 10)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "sdff = [['The', 'King', 'James', 'Bible'],\n",
    "        ['The', 'First', 'Book', 'of', 'Moses', 'Called', 'Genesis']]\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sdff)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id.items()\n",
    "\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "wids = [[word2id[y.lower()] for y in x] for x in sdff]\n",
    "\n",
    "print(wids)\n",
    "vocab_size = len(word2id)\n",
    "print(\"vocab_size ={0}\".format(vocab_size))\n",
    "print('Vocabulary Sample:', list(word2id.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 11\n",
      "Vocabulary Sample: [('the', 1), ('king', 2), ('james', 3), ('bible', 4), ('first', 5), ('book', 6), ('of', 7), ('moses', 8), ('called', 9), ('genesis', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Build the corpus vocabulary\n",
    "\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sdff)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids= [[word2id[y.lower()] for y in x] for x in sdff]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(james (3), genesis (10)) -> 0\n",
      "(james (3), bible (4)) -> 1\n",
      "(king (2), book (6)) -> 0\n",
      "(king (2), james (3)) -> 1\n",
      "(king (2), bible (4)) -> 1\n",
      "(the (1), bible (4)) -> 1\n",
      "(bible (4), the (1)) -> 1\n",
      "(bible (4), james (3)) -> 1\n",
      "(james (3), the (1)) -> 1\n",
      "(king (2), james (3)) -> 0\n"
     ]
    }
   ],
   "source": [
    "# Build a skip-gram [(target, context), relevancy] generators\n",
    "\n",
    "# - (word, word in the same window), with label 1 (positive samples).\n",
    "# - (word, random word from the vocabulary), with label 0 (negative samples).\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the skip-gram model architecture\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       1100        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       1100        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200)          0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            201         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,401\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       1100        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       1100        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 100)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200)          0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            201         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,401\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 1 Loss: 0.5126980245113373\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 2 Loss: 0.5049927234649658\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 3 Loss: 0.5003585815429688\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 4 Loss: 0.49679189920425415\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 5 Loss: 0.4937979727983475\n"
     ]
    }
   ],
   "source": [
    "# Define Two sets of Image\n",
    "inputA = Input(shape = (1,))\n",
    "inputB = Input(shape = (1,))\n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x1 = Embedding(vocab_size, embed_size,embeddings_initializer=\"glorot_uniform\")(inputA)\n",
    "x1 = Reshape((embed_size, ))(x1)\n",
    "x1 = Model(inputs=inputA, outputs=x1)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "x2 = Embedding(vocab_size, embed_size,embeddings_initializer=\"glorot_uniform\")(inputB)\n",
    "x2 = Reshape((embed_size,))(x2)\n",
    "x2 = Model(inputs=inputB, outputs=x2)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x1.output, x2.output])\n",
    "\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(1, kernel_initializer=\"glorot_uniform\",activation=\"sigmoid\")(combined)\n",
    "\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x1.input, x2.input], outputs=z)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "# model_to_dot(model, show_shapes=True, show_layer_names=False, rankdir='TB')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>king</th>\n",
       "      <th>james</th>\n",
       "      <th>bible</th>\n",
       "      <th>first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123286</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>0.027768</td>\n",
       "      <td>0.131940</td>\n",
       "      <td>-0.003307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022101</td>\n",
       "      <td>-0.226642</td>\n",
       "      <td>-0.030477</td>\n",
       "      <td>0.195855</td>\n",
       "      <td>0.196451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208757</td>\n",
       "      <td>0.102711</td>\n",
       "      <td>-0.121820</td>\n",
       "      <td>-0.011726</td>\n",
       "      <td>0.184570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.035796</td>\n",
       "      <td>0.055008</td>\n",
       "      <td>-0.195515</td>\n",
       "      <td>0.234745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152177</td>\n",
       "      <td>0.031311</td>\n",
       "      <td>-0.126345</td>\n",
       "      <td>0.115843</td>\n",
       "      <td>0.175388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.181966</td>\n",
       "      <td>0.068975</td>\n",
       "      <td>-0.171424</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.049449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.019837</td>\n",
       "      <td>-0.163700</td>\n",
       "      <td>-0.152337</td>\n",
       "      <td>-0.097319</td>\n",
       "      <td>-0.089524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.045612</td>\n",
       "      <td>-0.039000</td>\n",
       "      <td>-0.070202</td>\n",
       "      <td>0.080994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.107047</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.132917</td>\n",
       "      <td>-0.001371</td>\n",
       "      <td>0.168162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.170366</td>\n",
       "      <td>-0.129156</td>\n",
       "      <td>-0.012264</td>\n",
       "      <td>0.059383</td>\n",
       "      <td>-0.122593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         the      king     james     bible     first\n",
       "0   0.123286  0.226532  0.027768  0.131940 -0.003307\n",
       "1   0.022101 -0.226642 -0.030477  0.195855  0.196451\n",
       "2  -0.208757  0.102711 -0.121820 -0.011726  0.184570\n",
       "3   0.013072  0.035796  0.055008 -0.195515  0.234745\n",
       "4  -0.152177  0.031311 -0.126345  0.115843  0.175388\n",
       "..       ...       ...       ...       ...       ...\n",
       "95  0.181966  0.068975 -0.171424  0.041691 -0.049449\n",
       "96  0.019837 -0.163700 -0.152337 -0.097319 -0.089524\n",
       "97  0.085056  0.045612 -0.039000 -0.070202  0.080994\n",
       "98 -0.107047  0.001192  0.132917 -0.001371  0.168162\n",
       "99 -0.170366 -0.129156 -0.012264  0.059383 -0.122593\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layer_weights = model.get_layer('embedding_1').get_weights()[0][1:]\n",
    "\n",
    "print(embed_layer_weights.shape)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(embed_layer_weights, index=id2word.values()).head().T\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12328565,  0.02210053, -0.20875722,  0.01307229, -0.1521767 ,\n",
       "       -0.24242339, -0.07706003,  0.19345967,  0.1767865 ,  0.1290968 ,\n",
       "        0.17328714, -0.00236644, -0.03918898,  0.09456581,  0.1327328 ,\n",
       "        0.21905482, -0.2029304 , -0.02875277,  0.07935661,  0.18367337,\n",
       "       -0.18560782, -0.04930238, -0.139519  ,  0.22684693, -0.17525063,\n",
       "        0.19498321,  0.23099068, -0.11924646, -0.06498007,  0.0048749 ,\n",
       "       -0.06942524, -0.04754084,  0.18031433,  0.05124881,  0.21463886,\n",
       "       -0.05357247, -0.08549824, -0.06013143,  0.05521685, -0.05813576,\n",
       "       -0.06586694,  0.14669493, -0.17053379, -0.09111465,  0.23243658,\n",
       "        0.07134292,  0.19292793, -0.07645305,  0.06962932, -0.12814562,\n",
       "       -0.0298083 , -0.09485845, -0.05009864,  0.07699689, -0.10205524,\n",
       "       -0.23190635, -0.12969269,  0.11704001,  0.06302093,  0.09081487,\n",
       "        0.11008994,  0.04399976, -0.21099812,  0.18399477,  0.06157643,\n",
       "        0.0285271 ,  0.00470413, -0.1122279 ,  0.03737529, -0.0473179 ,\n",
       "        0.16412222,  0.00914694, -0.229584  , -0.21537657, -0.1453808 ,\n",
       "        0.01132706, -0.04191545, -0.09247701,  0.09422218, -0.05721609,\n",
       "        0.09912506,  0.11183786,  0.1101548 , -0.17072248, -0.2021843 ,\n",
       "       -0.14046317,  0.07081516,  0.01013533, -0.23762287,  0.05044448,\n",
       "        0.07895204, -0.03970392, -0.21293749, -0.16302915,  0.13178161,\n",
       "        0.1819663 ,  0.01983739,  0.08505622, -0.10704674, -0.17036566],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seneten = 'the'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'king': ['james', 'book', 'the', 'genesis', 'moses'],\n",
       " 'bible': ['king', 'first', 'the', 'moses', 'of'],\n",
       " 'james': ['king', 'the', 'of', 'moses', 'called']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(embed_layer_weights)\n",
    "\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['king','bible', 'james']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentences = [[\"james\", \"first\", \"king\"]]\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        tokens.append(df[word].values)\n",
    "        labels.append(word)\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "plt.figure(figsize=(5, 3)) \n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    \n",
    "    plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2),textcoords='offset points',ha='right', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
