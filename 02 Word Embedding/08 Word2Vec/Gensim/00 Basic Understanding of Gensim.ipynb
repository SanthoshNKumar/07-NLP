{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word Embedding\n",
    "    - Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. \n",
    "    - It represents words or phrases in vector space with several dimensions.\n",
    "    - Word embeddings can be generated using various methods like neural networks, co-occurrence matrix, probabilistic models, \n",
    "      etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What is Word2Vec?\n",
    "    - Word2Vec consists of models for generating word embedding. \n",
    "    - Word2vec is the technique/model to produce word embedding for better word representation. \n",
    "    - It captures a large number of precise syntactic and semantic word relationship.\n",
    "    \n",
    "What word2vec does?\n",
    "    - Word2vec represents words in vector space representation. \n",
    "    - Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear \n",
    "      together and dissimilar words are located far away.This is also termed as a semantic relationship. \n",
    "    - Neural networks do not understand text instead they understand only numbers. \n",
    "\n",
    "Word Embedding provides a way to convert text to a numeric vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Why Word2Vec?\n",
    "    - Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is \n",
    "      done in such a way that similar meaning words appear together and dissimilar words are located far away.This is also \n",
    "      termed as a semantic relationship\n",
    "    - Word2vec reconstructs the linguistic context of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word2vec learns word by predicting its surrounding context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word2vec is not a single algorithm but a combination of two techniques\n",
    "    - CBOW(Continuous bag of words)\n",
    "    - Skip-gram model.\n",
    "\n",
    "Both of these are shallow neural networks(One Hidden layer) which map word(s) to the target variable which is also a word(s).\n",
    "Both of these techniques learn weights which act as word vector representations. \n",
    "Let us discuss both these methods separately and gain intuition into their working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What is FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gensim  : \"Generate Similar\"\n",
    "    - Gensim is a free open-source Python library for representing documents as semantic vectors\n",
    "    - Gensim is billed as a Natural Language Processing package that does ‘Topic Modeling for Humans’.\n",
    "    - The training algorithms in the Gensim package were actually ported from the original Word2Vec implementation by Google \n",
    "      and extended with additional functionality.\n",
    "    - Gensim toolkit allows users to import Word2vec for topic modeling to discover hidden structure in the text body.\n",
    "    - Gensim provides not only an implementation of Word2vec but also for Doc2vec and FastText as well.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Representation of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# define training data\n",
    "sentences = [['this','is','first','sentence','of','the','document']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "print(model)\n",
    "print(list(model.wv.vocab))\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "X.shape  # There are seven words with dimention of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training data\n",
    "\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences,min_count=1)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the vector (Word Embedding) for the word 'and' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['and'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate BOW and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]\n",
      "\n",
      "my_dictionary: Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "\n",
      "BOW_corpus: \n",
      " [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)], [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(7, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(23, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]\n",
      "\n",
      "\n",
      "BOW: [['Saudis', 1], ['The', 1], ['a', 1], ['acknowledge', 1], ['are', 1], ['preparing', 1], ['report', 1], ['that', 2], ['will', 1], ['Jamal', 1], [\"Khashoggi's\", 1], ['Saudi', 1], ['an', 1], ['death', 1], ['journalist', 1], ['of', 1], ['result', 1], ['the', 1], ['was', 1], ['that', 2], ['was', 1], ['intended', 1], ['interrogation', 1], ['lead', 1], ['one', 1], ['to', 1], ['went', 1], ['wrong,', 1], ['to', 2], ['Turkey,', 1], ['abduction', 1], ['according', 1], ['from', 1], ['his', 1], ['sources.', 1], ['two', 1]]\n",
      "\n",
      "\n",
      "weight_tfidf: [['Saudis', 0.328], ['The', 0.328], ['a', 0.328], ['acknowledge', 0.328], ['are', 0.328], ['preparing', 0.328], ['report', 0.328], ['that', 0.373], ['will', 0.328], ['Jamal', 0.327], [\"Khashoggi's\", 0.327], ['Saudi', 0.327], ['an', 0.327], ['death', 0.327], ['journalist', 0.327], ['of', 0.327], ['result', 0.327], ['the', 0.327], ['was', 0.186], ['that', 0.404], ['was', 0.202], ['intended', 0.355], ['interrogation', 0.355], ['lead', 0.355], ['one', 0.355], ['to', 0.202], ['went', 0.355], ['wrong,', 0.355], ['to', 0.395], ['Turkey,', 0.347], ['abduction', 0.347], ['according', 0.347], ['from', 0.347], ['his', 0.347], ['sources.', 0.347], ['two', 0.347]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "# Preprocess the Dataset\n",
    "\n",
    "# List of Tokens\n",
    "tokenized  = [doc.split() for doc in documents]\n",
    "\n",
    "print(tokenized)\n",
    "\n",
    "# Create a Dictionary\n",
    "\n",
    "# storing the extracted tokens into the dictionary\n",
    "my_dictionary = corpora.Dictionary(tokenized)\n",
    "\n",
    "print(\"\\nmy_dictionary:\",my_dictionary)\n",
    "\n",
    "# convertig to a bag of word corpus\n",
    "\n",
    "BOW_corpus = [my_dictionary.doc2bow(doc, allow_update = True) for doc in tokenized]\n",
    "print(\"\\nBOW_corpus: \\n\",BOW_corpus)\n",
    "\n",
    "print(\"\\n\")\n",
    "#  Create a TFIDF matrix in Gensim\n",
    "import numpy as np\n",
    "word_weight =[]\n",
    "for doc in BoW_corpus:\n",
    "    for id, freq in doc:\n",
    "        word_weight.append([my_dictionary[id], freq])\n",
    "\n",
    "print(\"BOW:\",word_weight)\n",
    "\n",
    "print(\"\\n\")\n",
    "# create TF-IDF model\n",
    "tfIdf = models.TfidfModel(BoW_corpus, smartirs ='ntc')\n",
    "\n",
    "# TF-IDF Word Weight\n",
    "weight_tfidf =[]\n",
    "for doc in tfIdf[BoW_corpus]:\n",
    "    for id, freq in doc:\n",
    "        weight_tfidf.append([my_dictionary[id], np.around(freq, decimals = 3)])\n",
    "        \n",
    "print(\"weight_tfidf:\",weight_tfidf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similarity between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "print(common_texts)\n",
    "\n",
    "# Similarity between two different words\n",
    "model = Word2Vec(common_texts,size =100,window=2,min_count=1,workers=4)\n",
    "model.wv.similarity('minors','human')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most Similar words Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "# Look up top 6 words similar to 'minors'\n",
    "\n",
    "model = Word2Vec(common_texts,size =100,window=2,min_count=1,workers=4)\n",
    "model.wv.most_similar('minors',topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('brown')\n",
    "#nltk.download('movie_reviews')\n",
    "#nltk.download('treebank')\n",
    "\n",
    "import gensim\n",
    "from nltk.corpus import brown,movie_reviews,treebank\n",
    "\n",
    "b = Word2Vec(brown.sents())\n",
    "mr = Word2Vec(movie_reviews.sents())\n",
    "t = Word2Vec(treebank.sents())\n",
    "\n",
    "b.most_similar('money', topn=5)\n",
    "t.most_similar('money', topn=5)\n",
    "b.most_similar('great', topn=5)\n",
    "mr.most_similar('great', topn=5)\n",
    "t.most_similar('great', topn=5)\n",
    "\n",
    "b.most_similar('company', topn=5)\n",
    "mr.most_similar('company', topn=5)\n",
    "t.most_similar('company', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similarity between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the vector for the Words to compare\n",
    "w1 = model['sentence']\n",
    "w2 = model['word2vec']\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(w1.reshape(1,-1),w2.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Relationship Find Most Similar words in Corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('abc')\n",
    "\n",
    "import gensim\n",
    "from nltk.corpus import abc\n",
    "\n",
    "print(\"Total Sentence:\",len(abc.sents()))\n",
    "\n",
    "# Create a Word2vec Model for the 'Sentences' in abc Corpus\n",
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "\n",
    "X= list(model.wv.vocab)\n",
    "\n",
    "print(\"Total Vocabulary List:\",len(X))\n",
    "\n",
    "# Get the most Similar words\n",
    "data=model.wv.most_similar('science')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Vector of Words in 2D using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the words\n",
    "\n",
    "#  Retrieve all of the vectors from a trained model\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "            ['this', 'is', 'the', 'second', 'sentence'],\n",
    "            ['yet', 'another', 'sentence'],\n",
    "            ['one', 'more', 'sentence'],\n",
    "            ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "# If you save the model you can continue training it later:\n",
    "# model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Settings while Creatin Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec(common_texts,size=150, window=10, min_count=2, workers=10, iter=10)\n",
    "\n",
    "# Size : The size of the Dence vector to represent each token or word\n",
    "\n",
    "# Widnow : The maximum distance between the target word and its neighboring word default value is 5\n",
    "\n",
    "# min_count : Ignore all the words where frequency of each word is less than min_count, default value is 5. \n",
    "#             As we wanted to add all words in corpus, so value we provided is 1\n",
    "\n",
    "# Workers : How Many Threadd to use behind the scence\n",
    "\n",
    "# Iter : Number of iterations(Epochs) over the corpus\n",
    "\n",
    "# sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], \n",
    "             [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "#\"Creates and TSNE model and plots it\"\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = []\n",
    "tokens = []\n",
    "\n",
    "for word in model.wv.vocab:\n",
    "    tokens.append(model[word])\n",
    "    labels.append(word)\n",
    "    \n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "    \n",
    "plt.figure(figsize=(5, 3)) \n",
    "\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    \n",
    "    plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2),textcoords='offset points',ha='right', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models,corpora\n",
    "\n",
    "# File Name\n",
    "input_file = \"data_topic_modeling.txt\"\n",
    "\n",
    "# Load input file\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    f = open(input_file,'r')\n",
    "    for line in f.readlines():\n",
    "        data.append(line[:-1])\n",
    "    return data\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        \n",
    "    def process(self,input_text):\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
    "        \n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
    "        \n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
    "        \n",
    "        return tokens_stemmed\n",
    "        \n",
    "    if __name__=='__main__':\n",
    "        input_file = input_file\n",
    "        data = load_data(input_file)\n",
    "        \n",
    "# Load the Data\n",
    "data = load_data(input_file)\n",
    "\n",
    "print(\"Raw Text: \\n\",data,\"\\n\")\n",
    "\n",
    "# Preprocess the data\n",
    "# Create a preprocessor object\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "# Create a list for processed documents\n",
    "processed_tokens = [preprocessor.process(x) for x in data]\n",
    "\n",
    "# Create a dictionary based on the tokenized documents\n",
    "dict_tokens = corpora.Dictionary(processed_tokens)\n",
    "\n",
    "corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "num_topics = 2\n",
    "num_words = 4\n",
    "\n",
    "# Build Model\n",
    "ldamodel = models.ldamodel.LdaModel(corpus,num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "print(\"\\nMost contributing words to the topics\")\n",
    "\n",
    "for item in ldamodel.print_topics(num_topics=num_topics,num_words=num_words):\n",
    "    print(\"Topic\",item[0], \"==>\",item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\",encoding='utf-8').sample(n=1000, random_state=1)\n",
    "\n",
    "lines = df['review'].values.tolist()\n",
    "\n",
    "\n",
    "print(\"Raw data Sample:\\n\",lines[0])\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "review_lines  = list()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    \n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    review_lines.append(words)\n",
    "    \n",
    "print(\"\\n Preprocessed Data Sample:\\n\",review_lines[0])\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Build Model\n",
    "model = gensim.models.Word2Vec(sentences = review_lines,size =100,window=5,workers =4,min_count =1)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "\n",
    "# Vocab Size\n",
    "list(model.wv.vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['powerful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('powerful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_cosmul(positive=['women','king'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odd word out\n",
    "model.wv.doesnt_match('woman king queen movie'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "filename = 'imdb_embedding_word2vec.text'\n",
    "model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
