{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim = “Generate Similar” \n",
    "\n",
    "# Core Concepts of Gensim\n",
    "\n",
    "# Document : if referes to some text\n",
    "# Corpus : Collection of documents\n",
    "# Vector : Mathematical representation of a document is called vector\n",
    "# Model : An algorithm used for transforming vectors from one representation to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding words\n",
    "    - It is the process of representing each word as a vector\n",
    "\n",
    "Word Embeddings : \n",
    "    - Word Embeddings are the texts converted into numbers and there may be different numerical representations of the \n",
    "      same text.\n",
    "    - Neural network doesn’t understand text instead they understand only numbers.\n",
    "    - Word embedding is one of the most popular representation of document vocabulary. \n",
    "    - It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, \n",
    "      etc.\n",
    "      \n",
    "Word Embedding:\n",
    "    - Word Embedding is a type of word representation that allows words with similar meaning to be understood by machine \n",
    "      learning algorithms.\n",
    "    - Technically speaking, it is a mapping of words into vectors of real numbers using the neural network.\n",
    "    - Word embedding is a way to perform mapping using a neural network.\n",
    "    - There are various word embedding models available such as word2vec (Google), Glove (Stanford) and fastest (Facebook).\n",
    "    - Word Embedding is also called as distributed semantic model or distributed represented or semantic vector space or \n",
    "      vector space model.\n",
    "      \n",
    "Word Embedding:\n",
    "    - Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. \n",
    "    - It represents words or phrases in vector space with several dimensions.\n",
    "    - Word embeddings can be generated using various methods like neural networks, co-occurrence matrix, probabilistic models,\n",
    "      etc.\n",
    "\n",
    "    - Word2Vec consists of models for generating word embedding. \n",
    "    - These models are shallow two layer neural networks having one input layer,\n",
    "    - One hidden layer and one output layer. Word2Vec utilizes two architectures :\n",
    "\n",
    "*** Note: The idea of Word Embeddings is to take a corpus of text, and figure out distributed vector representations of words \n",
    "          that retain some level of semantic similarity between them.\n",
    "          \n",
    "Where is Word Embedding used?\n",
    "    - Word embedding helps in feature generation, document clustering, text classification, and natural language processing \n",
    "      tasks. Let us list them and have some discussion on each of these applications.\n",
    "      \n",
    "Neural Word Embeddings: \n",
    "    - The vectors we use to represent words are called neural word embeddings, and representations are strange.\n",
    "    - It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, \n",
    "      or CBOW), or using a word to predict a target context, which is called skip-gram.\n",
    "    \n",
    "A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 \n",
    "everywhere else. \n",
    "The vector representation of “numbers” in this format according to the above dictionary is [0,0,0,0,0,1] and of \n",
    "converted is[0,0,0,1,0,0].\n",
    "\n",
    "Frequency Distribution:\n",
    "    - It is used to find the frequency of each word occurring in a document.\n",
    "    \n",
    "Word Collocations: \n",
    "    - Collocations are the pair of words occurring together many times in a document.\n",
    "    - Identifying phrases that act like single words in NLP\n",
    "    - Collocations are two or more words that tend to appear frequently together, for example – United States.\n",
    "    \n",
    "What is N- Gram, Unigram, Bigram  and Trigram?\n",
    "    - It’s about word analysis, unigram means single word, bigram means double words and trigram means triple word.\n",
    "    \n",
    "Categories of Collocations :\n",
    "    1. Bigrams : Combination of two words\n",
    "    2. Trigrams : Combination of three words\n",
    "    \n",
    "N Grams :\n",
    "    - It refers to a sequence of n items from a given text.\n",
    "\n",
    "N Grams : \n",
    "    - A sequence of characters or words forms an N Grams.\n",
    "    - Character unigram consist of a single charcater, a bigram consists of a sequence of two characters \n",
    "\n",
    "Different Types of Embedding:\n",
    "    1. Frequency based Embedding\n",
    "    2. Prediction based Embedding\n",
    "    \n",
    "Frequency based Embedding :\n",
    "    1. Count Vector\n",
    "    2. TF-IDF Vector\n",
    "    3. Co-occurrence vector\n",
    "    \n",
    "Count Vector :\n",
    "    - Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. \n",
    "    - The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N.\n",
    "    - Each row in the matrix M contains the frequency of tokens in document D(i).\n",
    "    \n",
    "Prediction based Embedding:(Word2vec)\n",
    "\n",
    "Word2vec Architecture : There are two architectures used by word2vec\n",
    "    1. CBOW (Continuous Bag Of words)\n",
    "    2. SKIP- Gram model\n",
    "    \n",
    "Word2vec : \n",
    "    - Word2vec is the technique/model to produce word embedding for better word representations. \n",
    "    - It captures a large number of precise syntactic and semantic word relationships\n",
    "    - It is a shallow two- layered neural network\n",
    "    - Word2vec represents words in vector space representation. \n",
    "    - Words are represented in the form of vectors and placement in done in such a way that similar words appear together and \n",
    "      dissimilar words are located far away. \n",
    "    - This is also termed as a semantic relationship.\n",
    "    \n",
    "Word2vec : \n",
    "    - Word2vec is a two-layer neural net that processes text.\n",
    "    - Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus.\n",
    "    - While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand.\n",
    "\n",
    "CBOW (Continuous Bag of Words) : \n",
    "    - In CBOW, the current word is predicted using the window of surrounding context windows.\n",
    "    - CBOW model predicts the current word given context words within specific window. \n",
    "    - The input layer contains the context words and the output layer contains the current word. \n",
    "    - The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\n",
    "    - Several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "\n",
    "Skip Gram : \n",
    "    - Skip-Gram performs opposite of CBOW which implies that it predicts the given sequence or context from the word.\n",
    "    - Skip gram predicts the surrounding context words within specific window given current word. \n",
    "    - The input layer contains the current word and the output layer contains the context words. \n",
    "    - The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "    - Works well with small amount of the training data, represents well even rare words or phrases\n",
    "\n",
    "  \n",
    "Different Types of Word Embeddings :\n",
    "1. Bag of Words (Bow)\n",
    "2. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "3. Continuous BoW (CBOW) model and Skip Gram model embedding(Skip Gram)\n",
    "4. Pre-trained word embedding models \n",
    "1. Word2Vec (by Google)\n",
    "2. GloVe (by Stanford)\n",
    "3. fastText (by Facebook)\n",
    "     5.  Poincarré embedding\n",
    "       6. Node2Vec embedding based on Random Walk and Graph\n",
    "\n",
    "Feature engineering to represent text data.\n",
    "    1. Bag of Words Model\n",
    "    2. Bag of N-Gram model\n",
    "    3. TF-IDF model\n",
    "    4. Similarity features\n",
    "    5. Topic models\n",
    "    6. Word2Vec\n",
    "    7. GloVe\n",
    "    8. FastText\n",
    "    \n",
    "Which model to choose?\n",
    "    - CBOW is several times faster than skip gram and provides a better frequency for frequent words whereas skip gram needs a \n",
    "      small amount of training data and represents even rare words or phrases.\n",
    "      \n",
    "GloVe : The Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Relation between Word2Vec and NLTK?\n",
    "    - NLTK is a natural language toolkit.it is used for preprocessing of text\n",
    "    - Once can do different operation such as parts of speech tagging,lemmatization,stemming,stop word removal,removing rare \n",
    "      words or least used words.\n",
    "    - It helps in cleaning the text as well as help in preparing the features from the effective words.\n",
    "\n",
    "    - In the other way, word2vec is used for semantic (closely related items together) and syntactic (sequence) matching. \n",
    "      Using word2vec, one can find similar words, dissimilar words, dimensional reduction, and many others. \n",
    "      Another important feature of word2vec is to convert the higher dimensional representation of the text into lower \n",
    "      dimensional of vectors.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word Cloud:\n",
    "    - Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates \n",
    "      its frequency or importance. \n",
    "    - Significant textual data points can be highlighted using a word cloud. \n",
    "    - Word clouds are widely used for analyzing data from social network websites\n",
    "    - Visual representaion of texts in which the sizes of the data(tokens) represnts the number of times they have \n",
    "      occured or how imporants that specific word.\n",
    "\n",
    "Bag of word (BOW) :\n",
    "    - Representing each word respect to their term frequency\n",
    "    \n",
    "TF-IDF (Term Frequency -Inverse document Frequency) : \n",
    "    - using this we will be able to decide how important a word is to a given document in the present dataset or corpus.\n",
    "    - It indicates how many times a particular word appears in the dataset and what the important of the word is in order \n",
    "      to understand the document or dataset\n",
    "\n",
    "One Hot Encoding :\n",
    "    - One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML \n",
    "      algorithms to do a better job in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "\n",
    "Note :The Latest semantic analysis algorithms uses ‘term frequency – Inverse document Frequency’(tf - Idf) and the concept of \n",
    "linear algebra, such as cosine similarity and Euclidean distance, to find word with similar meanings.\n",
    "\n",
    "TF-IDF (Term Frequency -Inverse document Frequency) : using this we will be able to decide how important a word is to a given \n",
    "document in the present dataset or corpus.\n",
    "It indicates how many times a particular word appears in the dataset and what the important of the word is in order to \n",
    "understand the document or dataset.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = log10(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "(TF- TDF) = TF(t) * IDF(t)\n",
    "\n",
    "Note : TF-IDF  is the multiplication of TF and IDF, such as TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TF-IDF(Term frequency-Inverse term frequency) : tf-idf is a weighting factor which is used to get the important features from \n",
    "the documents(corpus).\n",
    "\n",
    "It actually tells us how important a word is to a document in a corpus, the importance of a word increases proportionally to \n",
    "the number of times the word appears in the individual document, this is called Term Frequency(TF)\n",
    "\n",
    "\n",
    "TfIDF Calculation\n",
    "\n",
    "Example : document 1\n",
    "\n",
    "“ Mady loves programming. He programs all day, he will be a world class programmer one day ”\n",
    "\n",
    "if we apply tokenization, steeming and stopwords (we discussed in the last story) to this document, we get features with high \n",
    "count like ? program(3), day(2),love(1) and etc\n",
    "\n",
    "TF = (no of times the word appear in the doc) / (total no of words in the doc)\n",
    "\n",
    "Here program is the highest frequent term in the document.\n",
    "so program is a good feature if we consider TF.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or \n",
    "rare across all documents.\n",
    "\n",
    "IDF?—?Log(total no of documents / no of documents with the term t in it).\n",
    "so TF-IDF = TF * IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What is word2vec?\n",
    "    - Word2vec is the technique/model to produce word embedding for better word representation.\n",
    "    - It captures a large number of precise syntactic and semantic word relationship.\n",
    "    - It is a shallow two-layered neural network.\n",
    "\n",
    "What word2vec does?\n",
    "    - Word2Vector represents word in vector space representation.words are represented in the form of vectors and placement \n",
    "      is done in such a way that similar words are appear together and dissimilat words are located far away.\n",
    "      This is also termed as a semantic relationship.\n",
    "    - Neural networks do not understand text instead they understand only numbers.word embedding provides a way to convert \n",
    "      text to numeric vector.\n",
    "    - Word2vec reconstructs the linguistic context of words.\n",
    "\n",
    "How Word2Vec Works?\n",
    "    - Work2vec learns word by predicting its surrounding context.\n",
    "    - Example : \"He Loves Footbal\"\n",
    "        - Word loves moves over each word in the corpus,Syntactic as well as the semantic relatioship berween words is encoded.\n",
    "          This help in finding similar and analogies words.\n",
    "        - All random features of the word loves\tis calculated.These features are changed or update concerning neighbor or \n",
    "          context words with the help of a back propagation method.\n",
    "\n",
    "Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words. Hence, by looking at \n",
    "its neighbouring words, we can attempt to predict the target word.\n",
    "\n",
    "The Word2vec representataion of words allows for exploring intersting mathematical relationships between vectors, which is also \n",
    "intutiive expression for words. Ex : we will be able to findout the value of this expreession by using word representation.\n",
    "\n",
    "King- man = queen - woman\n",
    "\n",
    "In the above example the word 'king' ocurs in a position aong with man, in a manner similar to how the word queen and woman \n",
    "are present with one another.\n",
    "\n",
    "\n",
    "Shallow and Deep Neural Network\n",
    "    - The shallow neural network consists of the only a hidden layer between input and output whereas deep neural network \n",
    "      contains multiple hidden layers between input and output.\n",
    "    - Input is subjected to nodes whereas the hidden layer, as well as the output layer, contains neurons.\n",
    "\n",
    "\n",
    "Another way of learning is that if the context of two words are similar or two words have similar features, \n",
    "then such words are related.\n",
    "\n",
    "\n",
    "Word2vec Architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentence Similarity:\n",
    "\n",
    "Cosine Similarity :\n",
    "    - The Cosine Similarity between two texts is the cosine of the angle between their vector representation.\n",
    "\n",
    "Jaccard Similarity : \n",
    "    - This is the ratio of the number of terms common between two texts documents to the total number of  unique terms present \n",
    "      in those texts.\n",
    "      \n",
    "Levenshtein distance\n",
    "    - Edit Distance between also called 'Levenshtein distance' is a metric used to measure the similarity between two distances.\n",
    "      Essentially,its a count of how many edit operations,deletions,insertions,or substitutions will transform of giving string\n",
    "      A to String B.\n",
    "\n",
    "Different Ways of computing sentence similarity between two sentences\n",
    "\n",
    "    1. Jaccard Similarity\n",
    "    2. Different Embedding + K-means\n",
    "    3. Different Embeddings + Cosine Similarity\n",
    "    4. Word2vec + Smooth Inverse frequency + Cosine Similarity\n",
    "    5. Different Embeddings + LSI + Cosine Similarity\n",
    "    6. Different Embeddings + LDA + Jensen-Shannon Distance\n",
    "    7. Different embeddings+ Word Mover Distance\n",
    "    8. Different embeddings+ Variational Auto Encoder (VAE) \n",
    "    9. Different embeddings+ Universal sentence encoder \n",
    "    10.Different embeddings+ Siamese Manhattan LSTM\n",
    "    11.Knowledge-based Measures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CountVectorizer \n",
    "\n",
    "'''\n",
    "input : [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
    "\n",
    "output: [[1 1 1 1 1 1 1 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IfIdf\n",
    "\n",
    "'''\n",
    "nput :[\"earth is the third planet from the sun\",\n",
    "       \"Jupiter is the largest\"]\n",
    "\n",
    "output:\n",
    "\n",
    "matrix([[0.36439074, 0.36439074, 0.25926702, 0.  , 0.  ,0.36439074, 0.36439074, 0.51853403, 0.36439074],\n",
    "        [0.    , 0.     , 0.40993715, 0.57615236, 0.57615236, 0.     , 0.    , 0.40993715,  0.     ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding Sequence\n",
    "\n",
    "'''\n",
    "input :\n",
    "    [24]\n",
    "    [24, 34]\n",
    "    [24, 34, 1]\n",
    "    [24, 34, 1, 9]\n",
    "    [24, 34, 1, 9, 56]\n",
    "    [24, 34, 1, 9, 56, 76]\n",
    "    [24, 34, 1, 9, 56, 76, 90]\n",
    "    [24, 34, 1, 9, 56, 76, 90, 11]\n",
    "    [24, 34, 1, 9, 56, 76, 90, 11, 67]\n",
    "    [24, 34, 1, 9, 56, 76, 90, 11, 67, 54]\n",
    "    [24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]\n",
    "\n",
    "output :\n",
    "   [[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24],\n",
    "   [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 24, 34],\n",
    "   [ 0,  0,  0,  0,  0,  0,  0,  0, 24, 34,  1],\n",
    "   [ 0,  0,  0,  0,  0,  0,  0, 24, 34,  1,  9],\n",
    "   [ 0,  0,  0,  0,  0,  0, 24, 34,  1,  9, 56],\n",
    "   [ 0,  0,  0,  0,  0, 24, 34,  1,  9, 56, 76],\n",
    "   [ 0,  0,  0,  0, 24, 34,  1,  9, 56, 76, 90],\n",
    "   [ 0,  0,  0, 24, 34,  1,  9, 56, 76, 90, 11],\n",
    "   [ 0,  0, 24, 34,  1,  9, 56, 76, 90, 11, 67],\n",
    "   [ 0, 24, 34,  1,  9, 56, 76, 90, 11, 67, 54],\n",
    "   [24, 34,  1,  9, 56, 76, 90, 11, 67, 54, 14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sequence (maxlen=2 and truncating='pre')\n",
    "\n",
    "'''\n",
    "input:\n",
    "  [['3', '18', '9', '3', '11', '5', '20'],\n",
    "   ['3', '8', '1', '12'],\n",
    "   ['18', '1', '8', '1'],\n",
    "   ['8', '1', '9', '14'],\n",
    "   ['25', '1', '8', '1'],\n",
    "   ['9','3']]\n",
    "\n",
    "output:\n",
    "    [[ 5 20]\n",
    "     [ 1 12]\n",
    "     [ 8  1]\n",
    "     [ 9 14]\n",
    "     [ 8  1]\n",
    "     [ 9  3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sequence (maxlen=2 and truncating='post')\n",
    "\n",
    "'''\n",
    "input:\n",
    "  [['3', '18', '9', '3', '11', '5', '20'],\n",
    "   ['3', '8', '1', '12'],\n",
    "   ['18', '1', '8', '1'],\n",
    "   ['8', '1', '9', '14'],\n",
    "   ['25', '1', '8', '1'],\n",
    "   ['9','3']]\n",
    "\n",
    "output:\n",
    "    [[ 3 18]\n",
    "     [ 3  8]\n",
    "     [18  1]\n",
    "     [ 8  1]\n",
    "     [25  1]\n",
    "     [9  3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "\n",
    "'''\n",
    "input :\n",
    "    text = '''Data are individual units of information.A datum describes a single quality or quantity of \\\n",
    "            some object or phenomenon. In analytical processes, data are represented by variables. Although the terms'''\n",
    "\n",
    "output :\n",
    "    Keys:\n",
    "    \n",
    "    ['Data', 'are', 'individual', 'units', 'of', 'information.', 'A', 'datum', 'describes', 'a', 'single', \n",
    "    'quality', 'or', 'quantity', 'some', 'object', 'phenomenon.', 'In', 'analytical', 'processes,', 'd', 'ata',\n",
    "    'represented', 'by', 'variables.', 'Although', 'the', 'terms']\n",
    "\n",
    "values:\n",
    "        [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Fequency / Word Count\n",
    "\n",
    "'''\n",
    "input :\n",
    "      'Monty Python ! And the holy Grail ! \\n'\n",
    "\n",
    "output :\n",
    "      {'Monty': 1, 'Python': 1, '!': 2, 'And': 1, 'the': 1, 'holy': 1, 'Grail': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Index\n",
    "\n",
    "'''\n",
    "input :\n",
    "\n",
    "sentences = [\"this is the first sentence for word2vec\",\n",
    "             \"this is the second sentence\",\n",
    "             \"yet another sentence\",\n",
    "             \"one more sentence\",\n",
    "             \"and the final sentence\"]\n",
    "\n",
    "output :\n",
    "        {'this': 1, 'another': 2, 'final': 3, 'word2vec': 4, 'second': 5, 'for': 6, 'and': 7, 'one': 8, 'the': 9, \n",
    "        'sentence': 10, 'yet': 11, 'is': 12, 'more': 13, 'first': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences to Matrix (mode = binary and num_words = 10)\n",
    "\n",
    "'''\n",
    "input :\n",
    "  [[1,2,3,4],\n",
    "   [4,5,],\n",
    "   [6,7,8]]\n",
    "\n",
    "output :\n",
    "     [[0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
    "      [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
    "      [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences to Matrix (mode = count and num_words = 10)\n",
    "\n",
    "'''\n",
    "input :\n",
    "  [[1,2,3,4,4],\n",
    "   [4,5,],\n",
    "   [6,7,8]]\n",
    "   \n",
    "output :\n",
    "     [[0., 1., 1., 1., 2., 0., 0., 0., 0., 0.],\n",
    "      [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
    "      [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences to Matrix (mode = freq and num_words = 10)\n",
    "\n",
    "'''\n",
    "input :\n",
    "    [[1,2,3,4,4],\n",
    "     [4,5,],\n",
    "     [6,7,8]]\n",
    "     \n",
    "output :\n",
    "    [0.        , 0.2       , 0.2       , 0.2       , 0.4       , 0.        , 0.        , 0.        , 0.        , 0.],\n",
    "    [0.        , 0.        , 0.        , 0.        , 0.5       ,0.5       , 0.        , 0.        , 0.        , 0.],\n",
    "    [0.        , 0.        , 0.        , 0.        , 0.        ,0.        , 0.33333333, 0.33333333, 0.33333333, 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to int\n",
    "\n",
    "'''\n",
    "input :\n",
    "  ['king james bible',\n",
    "   'old testament king james bible',\n",
    "   'first book moses called genesis',\n",
    "   'beginning god created heaven earth',\n",
    "   'earth without form void darkness upon face deep',\n",
    "   'spirit god moved upon face waters',\n",
    "   'god said let light light',\n",
    "   'god saw light good god divided light darkness',\n",
    "   'god called light day darkness called night',\n",
    "   'evening morning first day']\n",
    "\n",
    "Output:\n",
    "  [[5, 6, 7],\n",
    "   [13, 14, 5, 6, 7],\n",
    "   [8, 15, 16, 3, 17],\n",
    "   [18, 1, 19, 20, 9],\n",
    "   [9, 21, 22, 23, 4, 10, 11, 24],\n",
    "   [25, 1, 26, 10, 11, 27],\n",
    "   [1, 28, 29, 2, 2],\n",
    "   [1, 30, 2, 31, 1, 32, 2, 4],\n",
    "   [1, 3, 2, 12, 4, 3, 33],\n",
    "   [34, 35, 8, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CBOW\n",
    "'''\n",
    "\n",
    "input :\n",
    " [['beginning', 'god', 'created', 'heaven', 'earth'],\n",
    "  ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
    "  ['spirit', 'god', 'moved', 'upon', 'face', 'waters']]\n",
    "\n",
    "Output : \n",
    " input: [['god', 'created']]  output: ['beginning']\n",
    " input: [['beginning', 'created', 'heaven']]  output: ['god']\n",
    " input: [['beginning', 'god', 'heaven', 'earth']]  output: ['created']\n",
    " input: [['god', 'created', 'earth']]  output: ['heaven']\n",
    " input: [['created', 'heaven']]  output: ['earth']\n",
    " input: [['without', 'form']]  output: ['earth']\n",
    " input: [['earth', 'form', 'void']]  output: ['without']\n",
    " input: [['earth', 'without', 'void', 'darkness']]  output: ['form']\n",
    " input: [['without', 'form', 'darkness', 'upon']]  output: ['void']\n",
    " input: [['form', 'void', 'upon', 'face']]  output: ['darkness']\n",
    " input: [['void', 'darkness', 'face', 'deep']]  output: ['upon']\n",
    " input: [['darkness', 'upon', 'deep']]  output: ['face']\n",
    " input: [['upon', 'face']]  output: ['deep']\n",
    " input: [['god', 'moved']]  output: ['spirit']\n",
    " input: [['spirit', 'moved', 'upon']]  output: ['god']\n",
    " input: [['spirit', 'god', 'upon', 'face']]  output: ['moved']\n",
    " input: [['god', 'moved', 'face', 'waters']]  output: ['upon']\n",
    " input: [['moved', 'upon', 'waters']]  output: ['face']\n",
    " input: [['upon', 'face']]  output: ['waters']\n",
    "\n",
    " input: [[0 0 0 0 0 1 6]]  output: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 5 6 7]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 5 1 7 2]]  output: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 1 6 2]]  output: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 0 6 7]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 0 8 9]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 2  9 10]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 2 8 10 11]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 8 9 11 3]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 9 10 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 10 11 4 12]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 11 3 12]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 0 1 14]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
    " input: [[0 0 0 0 13 14 3]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 13 1 3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
    " input: [[0 0 0 1 14 4 15]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 14 3 15]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
    " input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Similarity\n",
    "\n",
    "'''\n",
    "Input : \n",
    "    doc1 = \"Julie loves John more than Linda loves John\"\n",
    "    doc2 = \"Jane loves John more than Julie loves John\"\n",
    "\n",
    "Output :\n",
    "    [vect1 = [0 1 0 1 2 2 1 1]\n",
    "    [vect2 = 1 1 1 0 1 2 1 1]]\n",
    "    \n",
    "    cosine_similarity(list(vec1),list(vec2))\n",
    "    \n",
    "    array([[0.82158384]]) # 82% confidence that both text are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "\n",
    "'''\n",
    "Input : \"Children should't drink a sugary drink before bed\"\n",
    "\n",
    "output :\n",
    "    [('Children', 'NNP'),\n",
    "     (\"should't\", 'VBZ'),\n",
    "     ('drink', 'VB'),\n",
    "     ('a', 'DT'),\n",
    "     ('sugary', 'JJ'),\n",
    "     ('drink', 'NN'),\n",
    "     ('before', 'IN'),\n",
    "     ('bed', 'NN'),\n",
    "     ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
