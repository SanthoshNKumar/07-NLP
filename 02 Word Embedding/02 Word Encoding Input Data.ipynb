{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding words is the process of representing each word as a vector\n",
    "\n",
    "# Multiple Technique to Prepare Input data in NLP\n",
    "\n",
    "    1. Split Senetence to Words\n",
    "    2. Unique words in Senetnce\n",
    "    3. Finding Vocabulary and Vocabulary size\n",
    "    4. Max length of the words in document\n",
    "    5. Get Legnth of the Word\n",
    "    6. Get word indexs in a Sentence\n",
    "    7. Encode the Text\n",
    "    8. Finding the Vobabulary,Vocabulary Size and Char to List Conversion\n",
    "    9. Word Frequency (Word Count)\n",
    "    10.Word Frequency (Word Count) using collections\n",
    "    11.Convert Char to Int\n",
    "    12.List of Words to List of Integer\n",
    "    13.Sequence of Sentences into List of Words\n",
    "    14.Sequence of Sentences into List of Words Using List comprehension\n",
    "    15.Sequence of Sentences to Seqeunce of Integer\n",
    "    16.Text to Integer Word Encoding\n",
    "    17.Count the words (Word Frequency)\n",
    "    18.Find Most Common word\n",
    "    19.N Gram Bigram and Trigram\n",
    "    20.N Gram from Sequence List\n",
    "    21.Label Encoder\n",
    "    22.Ordinal Encoder\n",
    "    23.Count Vectorizer\n",
    "    24.One Hot Encoder for Single Senetence\n",
    "    25.One Hot Ending for Multiple Sentence\n",
    "    26.Multi Dimenstional One Hot Encoding\n",
    "    27.Decode One Hot Encoded Array\n",
    "    28.Sequence to Matrix (Binary,Count,Frequency)\n",
    "    29.Bag of words\n",
    "    30.TFIDF\n",
    "    32.Padding\n",
    "    33.Pad Sequence (Pre and Post)\n",
    "    34.Truncating Sequence (Pre and Post)\n",
    "    35.Frequency Distribution\n",
    "    36.Union and Intersection\n",
    "    37.Generate Input for the RNN\n",
    "    38.Conversion of Dimention of data : specially for LSTM\n",
    "    39.Embedding of the Characters in the string : for LSTM\n",
    "    40.CBOW\n",
    "    41.Skip Gram\n",
    "    42.Loading Pretrained Word Embeddings : Glove\n",
    "    43.Creating word embedding for our Corpus\n",
    "    44.Word2Vec Using gensim Model\n",
    "    \n",
    "    # Pending\n",
    "        word2id\n",
    "        id2Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Different Functions in Keras for Word Embedding\n",
    "\n",
    "1. from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Split Senetence to Words\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jumping', 'is', 'the', 'dog', 'and', 'quick', 'he', 'brown', 'over', 'fox', 'lazy'}\n"
     ]
    }
   ],
   "source": [
    "# Unique words in Senetnce\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "print(set(sentence.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jumping', 'is', 'the', 'dog', 'and', 'quick', 'he', 'brown', 'over', 'fox', 'lazy'}\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Finding Vocabulary and Vocabulary size\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# define the document\n",
    "text = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "vocab = set(text_to_word_sequence(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "# integer encode the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "### Hash Vectroizer\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "### Hash Encoding with hashing_trick_Keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import hashing_trick\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "\n",
    "result = hashing_trick(text, round(vocab_size * 1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length of the words in document\n",
    "\n",
    "# Split Senetence to Words\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)\n",
    "\n",
    "np.max([len(x) for x in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_letters = abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;''\n",
      "Length of letters = 57\n"
     ]
    }
   ],
   "source": [
    "# Get Legnth of the Word\n",
    "\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \".,;''\"\n",
    "\n",
    "print(\"all_letters = {0}\".format(all_letters))\n",
    "\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "print(\"Length of letters = {0}\".format(n_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jumping': 0, 'is': 1, 'the': 2, 'The': 3, 'dog': 4, 'and': 5, 'quick': 6, 'he': 7, 'brown': 8, 'over': 9, 'fox': 10, 'lazy': 11}\n"
     ]
    }
   ],
   "source": [
    "# Get word indexs in a Sentence\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "corpus = sentence.split()\n",
    "\n",
    "uniq_text = set(corpus)\n",
    "text_to_int = {}\n",
    "\n",
    "for i, c in enumerate (uniq_text):\n",
    "    text_to_int.update({c: i})\n",
    "    \n",
    "print(text_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 3, 4, 1, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the Text\n",
    "\n",
    "text = ['Hi',\"How\",\"are\",\"you\",\"Hi\",\"coming\"]\n",
    "\n",
    "character_list = list(set(text))   # get all of the unique letters in our text variable\n",
    "vocabulary_size = len(character_list)   # count the number of unique elements\n",
    "\n",
    "# create a dictionary mapping each unique char to a number\n",
    "character_dictionary = {char:e for e, char in enumerate(character_list)}  \n",
    "encoded_chars = [character_dictionary[char] for char in text] #integer representation of our vocabulary \n",
    "\n",
    "encoded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 6, 7], [1, 3]]\n",
      "word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n",
      "['the', 'earth', 'is', 'an', 'awesome', 'place', 'live']\n"
     ]
    }
   ],
   "source": [
    "# Text to Integer Sequence \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "fit_text = [\"The earth is an awesome place live\"]\n",
    "\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "test_text1 = \"The earth is an great place live\"\n",
    "test_text2 = \"The is my program\"\n",
    "\n",
    "sequences = t.texts_to_sequences([test_text1, test_text2])\n",
    "\n",
    "print(sequences)\n",
    "\n",
    "print('word_index : ',t.word_index)\n",
    "\n",
    "print(list(t.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique Charatcers(Vocabulary) in the document:{'y', 'w', 'u', 'm', 'a', 'h', 'i', 'r', 'e', 'o', 's', 'n', 't'}\n",
      "Size of the Vocabulary=13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'h': 23,\n",
       " 'i': 1,\n",
       " 'o': 21,\n",
       " 'w': 4,\n",
       " 'a': 17,\n",
       " 'r': 14,\n",
       " 'e': 12,\n",
       " 'y': 15,\n",
       " 'u': 10,\n",
       " 'm': 11,\n",
       " 's': 22,\n",
       " 'n': 18,\n",
       " 't': 19}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the Vobabulary,Vocabulary Size and Char to List Conversaion\n",
    "\n",
    "data = ['hi','how','are','you','merry','santhosh']\n",
    "\n",
    "chars = []\n",
    "\n",
    "char_id = dict()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        chars.append(data[i][j])\n",
    "        \n",
    "print(\"unique Charatcers(Vocabulary) in the document:{0}\".format(set(chars)))\n",
    "print(\"Size of the Vocabulary={0}\".format(len(set(chars))))\n",
    "\n",
    "\n",
    "# Char to integer \n",
    "for (i,val) in enumerate(chars):\n",
    "    char_id[val] = i\n",
    "    \n",
    "char_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'brown': 1, 'fox': 1, 'is': 2, 'quick': 1, 'and': 1, 'he': 1, 'jumping': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency (Word Count)\n",
    "\n",
    "word_freq = {}\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "for tok in sentence.split():\n",
    "    if tok in word_freq:\n",
    "        word_freq[tok] +=1\n",
    "    else:\n",
    "        word_freq[tok] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 2, 'The': 1, 'brown': 1, 'fox': 1, 'quick': 1, 'and': 1, 'he': 1, 'jumping': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})\n",
      "\n",
      " Most common:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('is', 2),\n",
       " ('The', 1),\n",
       " ('brown', 1),\n",
       " ('fox', 1),\n",
       " ('quick', 1),\n",
       " ('and', 1),\n",
       " ('he', 1),\n",
       " ('jumping', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency (Word Count) using collections\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(sentence.split())\n",
    "\n",
    "print(counter)\n",
    "\n",
    "print(\"\\n Most common:\")\n",
    "\n",
    "# Find Most Common word\n",
    "counter.most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'w': 1,\n",
       " 'u': 2,\n",
       " 'm': 3,\n",
       " 'a': 4,\n",
       " 'h': 5,\n",
       " 'i': 6,\n",
       " ' ': 7,\n",
       " 'c': 8,\n",
       " 'r': 9,\n",
       " 'f': 10,\n",
       " 'v': 11,\n",
       " 'e': 12,\n",
       " 'o': 13,\n",
       " 'd': 14,\n",
       " 'n': 15,\n",
       " 'g': 16}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Char to Int\n",
    "\n",
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "char2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2id,id2word and wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Sample: [('the', 1), ('king', 2), ('james', 3), ('bible', 4), ('of', 5), ('old', 6), ('testament', 7), ('first', 8), ('book', 9), ('moses', 10), (':', 11), ('called', 12), ('genesis', 13)]\n",
      "\n",
      " vocab_size =13\n",
      "\n",
      "wids:\n",
      " [[1, 2, 3, 4], [1, 6, 7, 5, 1, 2, 3, 4], [1, 8, 9, 5, 10, 11, 12, 13]]\n",
      "\n",
      "word2id: \n",
      " {'the': 1, 'king': 2, 'james': 3, 'bible': 4, 'of': 5, 'old': 6, 'testament': 7, 'first': 8, 'book': 9, 'moses': 10, ':': 11, 'called': 12, 'genesis': 13}\n",
      "\n",
      "id2Word: \n",
      " {1: 'the', 2: 'king', 3: 'james', 4: 'bible', 5: 'of', 6: 'old', 7: 'testament', 8: 'first', 9: 'book', 10: 'moses', 11: ':', 12: 'called', 13: 'genesis'}\n"
     ]
    }
   ],
   "source": [
    "# List of Words to List of Integer\n",
    "\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "sdff = [['The', 'King', 'James', 'Bible'],\n",
    "        ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'],\n",
    "        ['The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis']]\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sdff)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id.items()\n",
    "\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "wids = [[word2id[y.lower()] for y in x] for x in sdff]\n",
    "\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "print('Vocabulary Sample:', list(word2id.items()))\n",
    "\n",
    "print(\"\\n vocab_size ={0}\".format(vocab_size))\n",
    "\n",
    "print(\"\\nwids:\\n\",wids)\n",
    "print(\"\\nword2id: \\n\",word2id)\n",
    "print(\"\\nid2Word: \\n\",id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Located': 1,\n",
       " 'on': 2,\n",
       " 'the': 9,\n",
       " 'southern': 4,\n",
       " 'tip': 5,\n",
       " 'of': 6,\n",
       " 'Lake': 7,\n",
       " 'Union,': 8,\n",
       " 'Hilton': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Integer\n",
    "\n",
    "list1 = ['Located', 'on', 'the', 'southern', 'tip', 'of', 'Lake', 'Union,', 'the', 'Hilton']\n",
    "\n",
    "text_int ={}\n",
    "\n",
    "for i,j in enumerate(list1):\n",
    "    text_int.update({j:(i+1)})\n",
    "    \n",
    "text_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'james', 'bible'],\n",
       " ['old', 'testament', 'king', 'james', 'bible'],\n",
       " ['first', 'book', 'moses', 'called', 'genesis'],\n",
       " ['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters'],\n",
       " ['god', 'said', 'let', 'light', 'light'],\n",
       " ['god', 'saw', 'light', 'good', 'god', 'divided', 'light', 'darkness'],\n",
       " ['god', 'called', 'light', 'day', 'darkness', 'called', 'night'],\n",
       " ['evening', 'morning', 'first', 'day']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence of Sentences into List of Words\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "[[w for w in text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'james', 'bible'],\n",
       " ['old', 'testament', 'king', 'james', 'bible'],\n",
       " ['first', 'book', 'moses', 'called', 'genesis'],\n",
       " ['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters'],\n",
       " ['god', 'said', 'let', 'light', 'light'],\n",
       " ['god', 'saw', 'light', 'good', 'god', 'divided', 'light', 'darkness'],\n",
       " ['god', 'called', 'light', 'day', 'darkness', 'called', 'night'],\n",
       " ['evening', 'morning', 'first', 'day']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence of Sentences into List of Words Using List comprehension\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "[[word for word in document.lower().split()] for document in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6, 7],\n",
       " [13, 14, 5, 6, 7],\n",
       " [8, 15, 16, 3, 17],\n",
       " [18, 1, 19, 20, 9],\n",
       " [9, 21, 22, 23, 4, 10, 11, 24],\n",
       " [25, 1, 26, 10, 11, 27],\n",
       " [1, 28, 29, 2, 2],\n",
       " [1, 30, 2, 31, 1, 32, 2, 4],\n",
       " [1, 3, 2, 12, 4, 3, 33],\n",
       " [34, 35, 8, 12]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence of Sentences to Seqeunce of Integer\n",
    "\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pavilion': 0,\n",
       " 'the': 1,\n",
       " 'energize': 2,\n",
       " 'Gates': 3,\n",
       " 'complimentary': 4,\n",
       " 'cocktail': 5,\n",
       " '24-hour': 6,\n",
       " 'Hilton': 7,\n",
       " 'perfectly': 8,\n",
       " 'over': 9,\n",
       " 'space': 10,\n",
       " 'center.': 11,\n",
       " 'one': 12,\n",
       " 'dinner': 13,\n",
       " 'rooms': 14,\n",
       " 'conference,': 15,\n",
       " 'staff': 16,\n",
       " 'to': 17,\n",
       " 'guarantee': 18,\n",
       " 'your': 19,\n",
       " 'restaurants': 20,\n",
       " 'Refresh': 21,\n",
       " 'visitors.': 22,\n",
       " 'wedding': 23,\n",
       " 'breakfast,': 24,\n",
       " 'leisure.': 25,\n",
       " '2,000': 26,\n",
       " 'Bill': 27,\n",
       " 'with': 28,\n",
       " 'Unwind': 29,\n",
       " 'Union,': 30,\n",
       " 'Garden': 31,\n",
       " 'Union': 32,\n",
       " 'our': 33,\n",
       " 'bar,': 34,\n",
       " 'sparkling': 35,\n",
       " 'neighborhood': 36,\n",
       " 'sq.': 37,\n",
       " 'restaurant.': 38,\n",
       " 'A/V': 39,\n",
       " 'home': 40,\n",
       " 'success.': 41,\n",
       " 'relax': 42,\n",
       " 'flooded': 43,\n",
       " 'located': 44,\n",
       " 'stocks': 45,\n",
       " 'State-of-the-art': 46,\n",
       " 'Pantry?': 47,\n",
       " 'A': 48,\n",
       " 'guest': 49,\n",
       " 'a': 50,\n",
       " 'decorated': 51,\n",
       " 'need': 52,\n",
       " 'natural': 53,\n",
       " 'companies': 54,\n",
       " 'reception': 55,\n",
       " 'visitors': 56,\n",
       " 'Our': 57,\n",
       " 'Tastefully': 58,\n",
       " 'latest': 59,\n",
       " 'hotel': 60,\n",
       " 'productive.': 61,\n",
       " 'Downtown': 62,\n",
       " 'wealth': 63,\n",
       " 'lunch': 64,\n",
       " 'technology': 65,\n",
       " 'for': 66,\n",
       " 'enjoy': 67,\n",
       " 'southern': 68,\n",
       " 'Lake': 69,\n",
       " 'area': 70,\n",
       " 'Located': 71,\n",
       " 'Seattle': 72,\n",
       " 'activities': 73,\n",
       " 'saltwater': 74,\n",
       " 'fitness': 75,\n",
       " 'light,': 76,\n",
       " 'snacks,': 77,\n",
       " 'helpful': 78,\n",
       " 'American': 79,\n",
       " 'drinks': 80,\n",
       " 'this': 81,\n",
       " 'major': 82,\n",
       " 'you': 83,\n",
       " 'versatile': 84,\n",
       " 'sundries': 85,\n",
       " 'on': 86,\n",
       " 'some': 87,\n",
       " 'will': 88,\n",
       " 'and': 89,\n",
       " 'kayaking': 90,\n",
       " 'tip': 91,\n",
       " 'like': 92,\n",
       " 'stay': 93,\n",
       " 'of': 94,\n",
       " 'allows': 95,\n",
       " 'is': 96,\n",
       " 'Inn': 97,\n",
       " 'ft.': 98,\n",
       " 'Melinda': 99,\n",
       " 'Pacific': 100,\n",
       " 'suites': 101,\n",
       " 'variety': 102,\n",
       " 'pool,': 103,\n",
       " 'Google': 104,\n",
       " 'by': 105,\n",
       " \"Northwest's\": 106,\n",
       " 'or': 107,\n",
       " 'make': 108,\n",
       " 'Foundation.': 109,\n",
       " 'numerous': 110,\n",
       " 'everything': 111,\n",
       " 'business': 112,\n",
       " 'including': 113,\n",
       " 'international': 114,\n",
       " 'out': 115,\n",
       " 'equipment': 116,\n",
       " 'Amazon,': 117,\n",
       " 'scenery': 118,\n",
       " 'outdoor': 119,\n",
       " 'in': 120,\n",
       " 'eclectic': 121,\n",
       " 'sought': 122,\n",
       " 'majestic': 123,\n",
       " 'cuisine': 124,\n",
       " 'take': 125,\n",
       " 'The': 126,\n",
       " '&': 127,\n",
       " 'sailing.': 128,\n",
       " 'offer': 129,\n",
       " 'proximity': 130,\n",
       " 'locals': 131,\n",
       " 'bars': 132,\n",
       " 'most': 133}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Integer Word Encoding\n",
    "\n",
    "text ='''Located on the southern tip of Lake Union, the Hilton Garden Inn Seattle Downtown hotel \n",
    "        is perfectly located for business and leisure. \\nThe neighborhood is home to numerous major \n",
    "        international companies including Amazon, Google and the Bill & Melinda Gates Foundation. \n",
    "        A wealth of eclectic restaurants and bars make this area of Seattle one of the most sought out \n",
    "        by locals and visitors. Our proximity to Lake Union allows visitors to take in some of the Pacific\n",
    "        Northwest's majestic scenery and enjoy outdoor activities like kayaking and sailing. over 2,000 sq. ft. \n",
    "        of versatile space and a complimentary business center. State-of-the-art A/V technology and our helpful \n",
    "        staff will guarantee your conference, cocktail reception or wedding is a success. Refresh in the sparkling \n",
    "        saltwater pool, or energize with the latest equipment in the 24-hour fitness center. Tastefully decorated \n",
    "        and flooded with natural light, our guest rooms and suites offer everything you need to relax and stay productive. \n",
    "        Unwind in the bar, and enjoy American cuisine for breakfast, lunch and dinner in our restaurant. The 24-hour Pavilion\n",
    "        Pantry? stocks a variety of snacks, drinks and sundries'''\n",
    "\n",
    "\n",
    "corpus = text.split()\n",
    "\n",
    "uniq_text = set(corpus)\n",
    "\n",
    "text_to_int = {}\n",
    "\n",
    "for i, c in enumerate (uniq_text):\n",
    "    text_to_int.update({c: i})\n",
    "    \n",
    "text_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Located': 1,\n",
       "         'on': 1,\n",
       "         'the': 9,\n",
       "         'southern': 1,\n",
       "         'tip': 1,\n",
       "         'of': 7,\n",
       "         'Lake': 2,\n",
       "         'Union,': 1,\n",
       "         'Hilton': 1,\n",
       "         'Garden': 1,\n",
       "         'Inn': 1,\n",
       "         'Seattle': 2,\n",
       "         'Downtown': 1,\n",
       "         'hotel': 1,\n",
       "         'is': 3,\n",
       "         'perfectly': 1,\n",
       "         'located': 1,\n",
       "         'for': 2,\n",
       "         'business': 2,\n",
       "         'and': 14,\n",
       "         'leisure.': 1,\n",
       "         'The': 2,\n",
       "         'neighborhood': 1,\n",
       "         'home': 1,\n",
       "         'to': 4,\n",
       "         'numerous': 1,\n",
       "         'major': 1,\n",
       "         'international': 1,\n",
       "         'companies': 1,\n",
       "         'including': 1,\n",
       "         'Amazon,': 1,\n",
       "         'Google': 1,\n",
       "         'Bill': 1,\n",
       "         '&': 1,\n",
       "         'Melinda': 1,\n",
       "         'Gates': 1,\n",
       "         'Foundation.': 1,\n",
       "         'A': 1,\n",
       "         'wealth': 1,\n",
       "         'eclectic': 1,\n",
       "         'restaurants': 1,\n",
       "         'bars': 1,\n",
       "         'make': 1,\n",
       "         'this': 1,\n",
       "         'area': 1,\n",
       "         'one': 1,\n",
       "         'most': 1,\n",
       "         'sought': 1,\n",
       "         'out': 1,\n",
       "         'by': 1,\n",
       "         'locals': 1,\n",
       "         'visitors.': 1,\n",
       "         'Our': 1,\n",
       "         'proximity': 1,\n",
       "         'Union': 1,\n",
       "         'allows': 1,\n",
       "         'visitors': 1,\n",
       "         'take': 1,\n",
       "         'in': 5,\n",
       "         'some': 1,\n",
       "         'Pacific': 1,\n",
       "         \"Northwest's\": 1,\n",
       "         'majestic': 1,\n",
       "         'scenery': 1,\n",
       "         'enjoy': 2,\n",
       "         'outdoor': 1,\n",
       "         'activities': 1,\n",
       "         'like': 1,\n",
       "         'kayaking': 1,\n",
       "         'sailing.': 1,\n",
       "         'over': 1,\n",
       "         '2,000': 1,\n",
       "         'sq.': 1,\n",
       "         'ft.': 1,\n",
       "         'versatile': 1,\n",
       "         'space': 1,\n",
       "         'a': 3,\n",
       "         'complimentary': 1,\n",
       "         'center.': 2,\n",
       "         'State-of-the-art': 1,\n",
       "         'A/V': 1,\n",
       "         'technology': 1,\n",
       "         'our': 3,\n",
       "         'helpful': 1,\n",
       "         'staff': 1,\n",
       "         'will': 1,\n",
       "         'guarantee': 1,\n",
       "         'your': 1,\n",
       "         'conference,': 1,\n",
       "         'cocktail': 1,\n",
       "         'reception': 1,\n",
       "         'or': 2,\n",
       "         'wedding': 1,\n",
       "         'success.': 1,\n",
       "         'Refresh': 1,\n",
       "         'sparkling': 1,\n",
       "         'saltwater': 1,\n",
       "         'pool,': 1,\n",
       "         'energize': 1,\n",
       "         'with': 2,\n",
       "         'latest': 1,\n",
       "         'equipment': 1,\n",
       "         '24-hour': 2,\n",
       "         'fitness': 1,\n",
       "         'Tastefully': 1,\n",
       "         'decorated': 1,\n",
       "         'flooded': 1,\n",
       "         'natural': 1,\n",
       "         'light,': 1,\n",
       "         'guest': 1,\n",
       "         'rooms': 1,\n",
       "         'suites': 1,\n",
       "         'offer': 1,\n",
       "         'everything': 1,\n",
       "         'you': 1,\n",
       "         'need': 1,\n",
       "         'relax': 1,\n",
       "         'stay': 1,\n",
       "         'productive.': 1,\n",
       "         'Unwind': 1,\n",
       "         'bar,': 1,\n",
       "         'American': 1,\n",
       "         'cuisine': 1,\n",
       "         'breakfast,': 1,\n",
       "         'lunch': 1,\n",
       "         'dinner': 1,\n",
       "         'restaurant.': 1,\n",
       "         'Pavilion': 1,\n",
       "         'Pantry?': 1,\n",
       "         'stocks': 1,\n",
       "         'variety': 1,\n",
       "         'snacks,': 1,\n",
       "         'drinks': 1,\n",
       "         'sundries': 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the words (Word Frequency)\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(text.split())\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 14),\n",
       " ('the', 9),\n",
       " ('of', 7),\n",
       " ('in', 5),\n",
       " ('to', 4),\n",
       " ('is', 3),\n",
       " ('a', 3),\n",
       " ('our', 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Most Common word\n",
    "counter.most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'brown'],\n",
       " ['brown', 'fox'],\n",
       " ['fox', 'is'],\n",
       " ['is', 'quick'],\n",
       " ['quick', 'and'],\n",
       " ['and', 'he'],\n",
       " ['he', 'is'],\n",
       " ['is', 'jumping'],\n",
       " ['jumping', 'over'],\n",
       " ['over', 'the'],\n",
       " ['the', 'lazy'],\n",
       " ['lazy', 'dog']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram (n =2): Bigram\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "words =sentence.split()\n",
    "\n",
    "n = 2\n",
    "output= []\n",
    "for i in range(len(words) - n+1):\n",
    "    output.append(words[i:i+n])\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'brown', 'fox'],\n",
       " ['brown', 'fox', 'is'],\n",
       " ['fox', 'is', 'quick'],\n",
       " ['is', 'quick', 'and'],\n",
       " ['quick', 'and', 'he'],\n",
       " ['and', 'he', 'is'],\n",
       " ['he', 'is', 'jumping'],\n",
       " ['is', 'jumping', 'over'],\n",
       " ['jumping', 'over', 'the'],\n",
       " ['over', 'the', 'lazy'],\n",
       " ['the', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram (n =3): Trigram\n",
    "\n",
    "words =sentence.split()\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "n = 3\n",
    "output= []\n",
    "for i in range(len(words) - n+1):\n",
    "    output.append(words[i:i+n])\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'quick', 'very', 'an', 'thanks', 'pleased', 'am', 'fast', 'your', 'i', 'is', 'and', 'service', 'report', 'excellent', 'with'} \n",
      "\n",
      "{'for': 1, 'quick': 2, 'very': 3, 'an': 4, 'thanks': 5, 'pleased': 6, 'am': 7, 'fast': 8, 'your': 9, 'i': 10, 'is': 11, 'and': 12, 'service': 13, 'report': 14, 'excellent': 15, 'with': 16} \n",
      "\n",
      "row=3 and column=7\n",
      "seq_embedding = \n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[ 5.  1.  4. 15. 14.  0.  0.]\n",
      " [ 9. 13. 11.  3.  2. 12.  8.]\n",
      " [10.  7.  6. 16.  9. 13.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example 2:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = [['Thanks', 'for', 'an', 'excellent', 'report'],\n",
    "        ['Your', 'service', 'is', 'very', 'quick', 'and', 'fast'],\n",
    "        ['I', 'am', 'pleased', 'with', 'your', 'service']]\n",
    "\n",
    "wrds = []\n",
    "\n",
    "[[wrds.append(w.lower()) for w in x] for x in data]\n",
    "\n",
    "wrds = set(wrds)\n",
    "\n",
    "print(wrds,\"\\n\")\n",
    "\n",
    "word_id = {}\n",
    "\n",
    "for i,w in enumerate(wrds,1):\n",
    "    word_id[w] = i\n",
    "    \n",
    "print(word_id,\"\\n\")\n",
    "\n",
    "row = len(data)\n",
    "\n",
    "column = np.max([len(x) for x in data])\n",
    "\n",
    "print(\"row={0} and column={1}\".format(row,column))\n",
    "\n",
    "seq_embedding = np.zeros((row,column))\n",
    "\n",
    "print(\"seq_embedding = \\n {0}\".format(seq_embedding))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(len(data)):\n",
    "    doc = data[i]\n",
    "    for j in range(len(doc)):\n",
    "        id = word_id[(doc[j]).lower()]\n",
    "        seq_embedding[i][j] = id\n",
    "        \n",
    "print(seq_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\n",
      "[24, 34]\n",
      "[24, 34, 1]\n",
      "[24, 34, 1, 9]\n",
      "[24, 34, 1, 9, 56]\n",
      "[24, 34, 1, 9, 56, 76]\n",
      "[24, 34, 1, 9, 56, 76, 90]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54, 14]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# N Gram from Sequence List\n",
    "\n",
    "lst = [24,34,1,9,56,76,90,11,67,54,14]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(0,len(lst)):\n",
    "    print(lst[:i+1])\n",
    "    data.append(lst[:i+1])\n",
    "\n",
    "# Padding\n",
    "pad_sequences(data,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "doc =  ['S','C','Q','S','C','Q','S','C','Q','Q','C','Q','C','Q','Q']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(['S','C','Q'])\n",
    "\n",
    "label_encoder.transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrdinalEncoder converts each string value to a whole number. \n",
    "# The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on.\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "doc =  [['S'],['C'],['Q'],['S'],['C'],['Q'],['S'],['C'],['Q'],['Q'],['C'],['Q'],['C'],['Q'],['Q']]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "encoder.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [2., 3.],\n",
       "       [1., 1.],\n",
       "       [0., 2.],\n",
       "       [2., 3.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrdinalEncoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[\"good\", \"london\"],\n",
    "             [\"good\", \"tokyo\"],\n",
    "             [\"bad\", \"paris\"],\n",
    "             [\"average\", \"so so\"],\n",
    "             [\"good\", \"tokyo\"]])\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "bag.toarray()\n",
    "\n",
    "# Index\n",
    "\n",
    "# and = 0\n",
    "# is =1\n",
    "# shining = 2\n",
    "# sun =3\n",
    "# sweet =4\n",
    "# the =5\n",
    "# wether =6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Document Category\n",
      "0                     The sky is blue and beautiful.  weather\n",
      "1                  Love this blue and beautiful sky!  weather\n",
      "2       The quick brown fox jumps over the lazy dog.  animals\n",
      "3  A king's breakfast has sausages, ham, bacon, e...     food\n",
      "4        I love green eggs, ham, sausages and bacon!     food\n",
      "5   The brown fox is quick and the blue dog is lazy!  animals\n",
      "6  The sky is very blue and the sky is very beaut...  weather\n",
      "7        The dog is lazy but the brown fox is quick!  animals\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary One Hot Encoder for Single Senetence\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Bag of Word\n",
    "\n",
    "# One Hot Encoding\n",
    "\n",
    "doc = \"Can I eat the Pizza\".lower().split()\n",
    "\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(doc)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Ending for Multiple Sentence\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "doc = [['can', 'i', 'eat', 'the', 'pizza'],['can', 'i', 'eat', 'the', 'pizza']]\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(doc)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi Dimenstional One Hot Encoding\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "x = [[1,2,3,4,5],\n",
    "     [1,4,8,4,4],\n",
    "     [1,2,3,3,5]]\n",
    "\n",
    "y = to_categorical(x, num_classes=10)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Array\n",
      "[[1, 2, 3, 4, 5], [1, 4, 8, 4, 4], [1, 2, 3, 3, 5]]\n",
      "\n",
      "\n",
      "One Hot Encoding of input:\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "Decoded output of One Hot Encoding\n",
      "[[1, 2, 3, 4, 5], [1, 4, 8, 4, 4], [1, 2, 3, 3, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Decode One Hot Encoded Array\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "x = [[1,2,3,4,5],\n",
    "     [1,4,8,4,4],\n",
    "     [1,2,3,3,5]]\n",
    "\n",
    "print(\"input Array\")\n",
    "print(x)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "one_hot_arry = to_categorical(x, num_classes=10)\n",
    "\n",
    "print(\"One Hot Encoding of input:\")\n",
    "print(one_hot_arry)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Decoded output of One Hot Encoding\")\n",
    "decoded_one_hot = [[argmax(aa) for aa in array] for array in one_hot_arry]\n",
    "\n",
    "print(decoded_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence to Matrix (mode = binary and num_words = 10)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences to Matrix (mode = count and num_words = 10)\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='count')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7         8    9\n",
       "0  0.0  0.4  0.2  0.2  0.2  0.0  0.000000  0.000000  0.000000  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.5  0.5  0.000000  0.000000  0.000000  0.0\n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.333333  0.333333  0.333333  0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences to Matrix (mode = freq and num_words = 10)\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='freq')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary={'worst', 'wisdom', 'the', 'time', 'best', 'age', 'It', 'times', 'foolishness', 'of', 'was'}\n",
      "Vicab_size = 11\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Word to Integer : = {'worst': 0, 'wisdom': 1, 'the': 2, 'time': 3, 'best': 4, 'age': 5, 'It': 6, 'times': 7, 'foolishness': 8, 'of': 9, 'was': 10}\n",
      "[[0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worst</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "      <th>best</th>\n",
       "      <th>age</th>\n",
       "      <th>It</th>\n",
       "      <th>times</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>of</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worst  wisdom  the  time  best  age   It  times  foolishness   of  was\n",
       "0    0.0     0.0  1.0   1.0   1.0  0.0  1.0    0.0          0.0  1.0  1.0\n",
       "1    1.0     0.0  1.0   0.0   0.0  0.0  1.0    1.0          0.0  1.0  1.0\n",
       "2    0.0     1.0  1.0   0.0   0.0  1.0  1.0    0.0          0.0  1.0  1.0\n",
       "3    0.0     0.0  1.0   0.0   0.0  1.0  1.0    0.0          1.0  1.0  1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of words\n",
    "\n",
    "import numpy as np\n",
    "docs = [\"It was the best of time\",\"It was the worst of times\",\"It was the age of wisdom\",\"It was the age of foolishness\"]\n",
    "\n",
    "row = len(docs)\n",
    "\n",
    "words = []\n",
    "for i in range(len(docs)):\n",
    "    spli_sentenc = docs[i].split()\n",
    "    for j in range(len(spli_sentenc)):\n",
    "        words.append(spli_sentenc[j])\n",
    "        \n",
    "vocab = set(words)\n",
    "print(\"Vocabulary={0}\".format(vocab))\n",
    "print(\"Vicab_size = {0}\".format(vocab_size))\n",
    "vocab_size = (len(vocab))\n",
    "column = vocab_size\n",
    "\n",
    "tensor = np.zeros((row,column))\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "words_id = {}\n",
    "\n",
    "for i,wrd in enumerate(vocab):\n",
    "    words_id[wrd] = i\n",
    "    \n",
    "print(\"Word to Integer : = {0}\".format(words_id))\n",
    "    \n",
    "for i in range(len(docs)):\n",
    "    words = docs[i].split()\n",
    "    for j in range(len(words)):\n",
    "        tensor[i][words_id[words[j]]] = 1\n",
    "        \n",
    "print(tensor)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(tensor,columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>arts</th>\n",
       "      <th>at</th>\n",
       "      <th>becomes</th>\n",
       "      <th>between</th>\n",
       "      <th>both</th>\n",
       "      <th>brained</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>left</th>\n",
       "      <th>natural</th>\n",
       "      <th>of</th>\n",
       "      <th>overlap</th>\n",
       "      <th>part</th>\n",
       "      <th>processing</th>\n",
       "      <th>right</th>\n",
       "      <th>science</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249322</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183502</td>\n",
       "      <td>0.351643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         an       and       are      arts        at   becomes   between  \\\n",
       "0  0.403328  0.257439  0.000000  0.257439  0.000000  0.000000  0.403328   \n",
       "1  0.000000  0.159139  0.498644  0.159139  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.224449  0.000000  0.224449  0.351643  0.351643  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       both   brained      data  ...  language      left   natural        of  \\\n",
       "0  0.000000  0.000000  0.317989  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.498644  0.000000  ...  0.000000  0.249322  0.000000  0.000000   \n",
       "2  0.351643  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.308872  ...  0.391765  0.000000  0.391765  0.391765   \n",
       "\n",
       "    overlap      part  processing     right   science      time  \n",
       "0  0.403328  0.000000    0.000000  0.000000  0.420947  0.000000  \n",
       "1  0.000000  0.000000    0.000000  0.249322  0.130107  0.000000  \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.183502  0.351643  \n",
       "3  0.000000  0.391765    0.391765  0.000000  0.204439  0.000000  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF\n",
    "\n",
    "# tf-idf = tf(i,d) * idf(t,d)\n",
    "\n",
    "# td(i,d) = term frequency\n",
    "\n",
    "# idf(t,d) = \tlog(n/1+df(t,d)) :  inverse document frequency  df(t,d) : no. of documents d that contain the term t\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Data Science is an overlap between Arts and Science\",\n",
    "          \"Generally,Arts graduates are right-brained and Science graduates are left-brained\",\n",
    "          \"Excelling in both Arts and Science at a time becomes difficult\",\n",
    "          \"Natural Language Processing is a part of Data Science\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "features = tfidf_model.fit_transform(corpus)\n",
    "df = pd.DataFrame(features.todense(),columns= sorted(tfidf_model.vocabulary_))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Document Category\n",
      "0                     The sky is blue and beautiful.  weather\n",
      "1                  Love this blue and beautiful sky!  weather\n",
      "2       The quick brown fox jumps over the lazy dog.  animals\n",
      "3  A king's breakfast has sausages, ham, bacon, e...     food\n",
      "4        I love green eggs, ham, sausages and bacon!     food\n",
      "5   The brown fox is quick and the blue dog is lazy!  animals\n",
      "6  The sky is very blue and the sky is very beaut...  weather\n",
      "7        The dog is lazy but the brown fox is quick!  animals\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a corpus of documents\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'\n",
    "          ]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "print(corpus_df)\n",
    "\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lowercase and remove special characters\\whitespace\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 14)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 14)\t1\n",
      "  (4, 16)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 9)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 15)\t1\n",
      "  (5, 5)\t1\n",
      "  (5, 8)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 6)\t1\n",
      "  (6, 17)\t2\n",
      "  (6, 3)\t1\n",
      "  (6, 2)\t1\n",
      "  (6, 19)\t1\n",
      "  (7, 15)\t1\n",
      "  (7, 5)\t1\n",
      "  (7, 8)\t1\n",
      "  (7, 13)\t1\n",
      "  (7, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix\n",
    "\n",
    "# view non-zero feature positions in the sparse matrix\n",
    "print(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dense representation\n",
    "\n",
    "# warning might give a memory error if data is too big\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n",
       "0      0      0          1     1          0      0    0     0    0      0   \n",
       "1      0      0          1     1          0      0    0     0    0      0   \n",
       "2      0      0          0     0          0      1    1     0    1      0   \n",
       "3      1      1          0     0          1      0    0     1    0      0   \n",
       "4      1      0          0     0          0      0    0     1    0      1   \n",
       "5      0      0          0     1          0      1    1     0    1      0   \n",
       "6      0      0          1     1          0      0    0     0    0      0   \n",
       "7      0      0          0     0          0      1    1     0    1      0   \n",
       "\n",
       "   ham  jumps  kings  lazy  love  quick  sausages  sky  toast  today  \n",
       "0    0      0      0     0     0      0         0    1      0      0  \n",
       "1    0      0      0     0     1      0         0    1      0      0  \n",
       "2    0      1      0     1     0      1         0    0      0      0  \n",
       "3    1      0      1     0     0      0         1    0      1      0  \n",
       "4    1      0      0     0     1      0         1    0      0      0  \n",
       "5    0      0      0     1     0      1         0    0      0      0  \n",
       "6    0      0      0     0     0      0         0    2      0      1  \n",
       "7    0      0      0     1     0      1         0    0      0      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon eggs</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>beautiful today</th>\n",
       "      <th>blue beautiful</th>\n",
       "      <th>blue dog</th>\n",
       "      <th>blue sky</th>\n",
       "      <th>breakfast sausages</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog lazy</th>\n",
       "      <th>eggs ham</th>\n",
       "      <th>...</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>love blue</th>\n",
       "      <th>love green</th>\n",
       "      <th>quick blue</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>sausages bacon</th>\n",
       "      <th>sausages ham</th>\n",
       "      <th>sky beautiful</th>\n",
       "      <th>sky blue</th>\n",
       "      <th>toast beans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon eggs  beautiful sky  beautiful today  blue beautiful  blue dog  \\\n",
       "0           0              0                0               1         0   \n",
       "1           0              1                0               1         0   \n",
       "2           0              0                0               0         0   \n",
       "3           1              0                0               0         0   \n",
       "4           0              0                0               0         0   \n",
       "5           0              0                0               0         1   \n",
       "6           0              0                1               0         0   \n",
       "7           0              0                0               0         0   \n",
       "\n",
       "   blue sky  breakfast sausages  brown fox  dog lazy  eggs ham  ...  lazy dog  \\\n",
       "0         0                   0          0         0         0  ...         0   \n",
       "1         0                   0          0         0         0  ...         0   \n",
       "2         0                   0          1         0         0  ...         1   \n",
       "3         0                   1          0         0         0  ...         0   \n",
       "4         0                   0          0         0         1  ...         0   \n",
       "5         0                   0          1         1         0  ...         0   \n",
       "6         1                   0          0         0         0  ...         0   \n",
       "7         0                   0          1         1         0  ...         0   \n",
       "\n",
       "   love blue  love green  quick blue  quick brown  sausages bacon  \\\n",
       "0          0           0           0            0               0   \n",
       "1          1           0           0            0               0   \n",
       "2          0           0           0            1               0   \n",
       "3          0           0           0            0               0   \n",
       "4          0           1           0            0               1   \n",
       "5          0           0           1            0               0   \n",
       "6          0           0           0            0               0   \n",
       "7          0           0           0            0               0   \n",
       "\n",
       "   sausages ham  sky beautiful  sky blue  toast beans  \n",
       "0             0              0         1            0  \n",
       "1             0              0         0            0  \n",
       "2             0              0         0            0  \n",
       "3             1              0         0            1  \n",
       "4             0              0         0            0  \n",
       "5             0              0         0            0  \n",
       "6             0              1         1            0  \n",
       "7             0              0         0            0  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAG OF N-GRAMS MODEL\n",
    "\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "\n",
    "vocab = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm=\"l2\",use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  83   91    1  645 1253  927]\n",
      " [  73    8 3215   55  927    0]\n",
      " [ 711  632   71    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],[73, 8, 3215, 55, 927],[711, 632, 71]]\n",
    "\n",
    "vocab_size = np.max([len(x) for x in raw_inputs])\n",
    "\n",
    "row = len(raw_inputs)\n",
    "column = vocab_size\n",
    "\n",
    "tensor = np.zeros((row,column),dtype=np.int32)\n",
    "\n",
    "for i in range(len(raw_inputs)):\n",
    "    for j in range(len(raw_inputs[i])):\n",
    "        tensor[i][j] = raw_inputs[i][j]\n",
    "        \n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [  73,    8, 3215,   55,  927,    0],\n",
       "       [ 711,  632,   71,    0,    0,    0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "# pad_sequences is used to ensure that all sequences in a list have the same length. \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "raw_inputs = [\n",
    "              [83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]\n",
    "             ]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='post')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9., 56.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9., 56., 76.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9., 56., 76., 90.,  0.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9., 56., 76., 90., 11.,  0.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9., 56., 76., 90., 11., 67.,  0.,  0.],\n",
       "       [24., 34.,  1.,  9., 56., 76., 90., 11., 67., 54.,  0.],\n",
       "       [24., 34.,  1.,  9., 56., 76., 90., 11., 67., 54., 14.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "\n",
    "a = [[24],\n",
    "     [24, 34],\n",
    "     [24, 34, 1],\n",
    "     [24, 34, 1, 9],\n",
    "     [24, 34, 1, 9, 56],\n",
    "     [24, 34, 1, 9, 56, 76],\n",
    "     [24, 34, 1, 9, 56, 76, 90],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67, 54],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]]\n",
    "\n",
    "import numpy as np\n",
    "np.max([len(x) for x in a])\n",
    "\n",
    "row = 11\n",
    "column = len(a)\n",
    "\n",
    "tensor = np.zeros((row,column))\n",
    "\n",
    "for i in range(len(a)):\n",
    "    lst = a[i]\n",
    "    for j in range(len(lst)):\n",
    "        tensor[i][j] = lst[j]\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [   0,   73,    8, 3215,   55,  927],\n",
       "       [   0,    0,    0,  711,  632,   71]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequence (Pre)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='pre')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [  73,    8, 3215,   55,  927,    0],\n",
       "       [ 711,  632,   71,    0,    0,    0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequence (Post)\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='post')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\n",
      "[24, 34]\n",
      "[24, 34, 1]\n",
      "[24, 34, 1, 9]\n",
      "[24, 34, 1, 9, 56]\n",
      "[24, 34, 1, 9, 56, 76]\n",
      "[24, 34, 1, 9, 56, 76, 90]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54, 14]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram from Sequence List\n",
    "\n",
    "lst = [24,34,1,9,56,76,90,11,67,54,14]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(0,len(lst)):\n",
    "    print(lst[:i+1])\n",
    "    data.append(lst[:i+1])\n",
    "\n",
    "# Padding\n",
    "pad_sequences(data,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 20]\n",
      " [ 1 12]\n",
      " [ 8  1]\n",
      " [ 9 14]\n",
      " [ 8  1]\n",
      " [ 9  3]]\n"
     ]
    }
   ],
   "source": [
    "# Truncating Sequence (Pre)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data_vec = [['3', '18', '9', '3', '11', '5', '20'],\n",
    "            ['3', '8', '1', '12'],\n",
    "            ['18', '1', '8', '1'],\n",
    "            ['8', '1', '9', '14'],\n",
    "            ['25', '1', '8', '1'],\n",
    "            ['9','3']]\n",
    "\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(data_vec, maxlen=2)\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 18]\n",
      " [ 3  8]\n",
      " [18  1]\n",
      " [ 8  1]\n",
      " [25  1]\n",
      " [ 9  3]]\n"
     ]
    }
   ],
   "source": [
    "# Truncating Sequence (Post)\n",
    "\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(data_vec, maxlen=2,truncating='post')\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union {'fine', 'How', 'about', 'Hello', 'are', 'Hi', 'am', 'You', 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Union\n",
    "\n",
    "document1 = ['Hi', 'How','are','You', 'I','am']\n",
    "document2 = ['Hello','I','am','fine','How','about','You']\n",
    "\n",
    "#Union : words from two documents\n",
    "print(\"Union\",set(document1) | set(document2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection: {'I', 'am', 'How', 'You'}\n"
     ]
    }
   ],
   "source": [
    "# Intersection\n",
    "\n",
    "document1 = ['Hi', 'How','are','You', 'I','am']\n",
    "document2 = ['Hello','I','am','fine','How','about','You']\n",
    "\n",
    "#Intersection : common words across two documents\n",
    "print(\"Intersection:\",set(document1) & set(document2))       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(1, 1, 57)\n",
      "\n",
      "\n",
      "\n",
      "Example 2\n",
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(2, 1, 57)\n",
      "\n",
      "\n",
      "\n",
      "Example 3\n",
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(5, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "# Generate Input for the RNN\n",
    "import numpy as np\n",
    "\n",
    "def name_intensor(name):\n",
    "    name_in_tensor = np.zeros((len(name),1,n_letters))\n",
    "    \n",
    "    for i,letter in enumerate(name):\n",
    "        name_in_tensor[i][0][all_letters.find(letter)] = 1\n",
    "    \n",
    "    return name_in_tensor\n",
    "\n",
    "print(\"Example 1\")\n",
    "print(name_intensor('a'))\n",
    "print(name_intensor('a').shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Example 2\")\n",
    "print(name_intensor('af'))\n",
    "print(name_intensor('af').shape)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Example 3\")\n",
    "print(name_intensor('anand'))\n",
    "print(name_intensor('anand').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion of Dimention of data : specially for LSTM\n",
    "\n",
    "# Ex : list to array of (6,) to (1,1,6)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "one_hot1 = [[0,0,0,1,0,0],\n",
    "           [0,0,0,1,0,1],\n",
    "           [0,1,0,1,0,0]]# This is List of list type\n",
    "\n",
    "one_hot2 = [[0,0,0,1,0,0],\n",
    "           [0,0,0,1,0,1],\n",
    "           [0,1,0,1,0,0]]# This is List of list type\n",
    "\n",
    "# convert to format it can be used in the LSTM input\n",
    "\n",
    "liss = [one_hot1,one_hot2]\n",
    "\n",
    "np.asarray(liss).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['klein', 'ist', 'das', 'Haus']\n",
      "['the', 'house', 'is', 'small']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Alignment([])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.api import AlignedSent\n",
    "\n",
    "algnsent = AlignedSent(['klein', 'ist', 'das', 'Haus'], ['the', 'house', 'is', 'small'])\n",
    "\n",
    "print(algnsent.words)\n",
    "\n",
    "print(algnsent.mots)\n",
    "\n",
    "algnsent.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8890388023094323\n",
      "0.061668228784061314\n",
      "0.11318685275683199\n",
      "0.07278526278031176\n",
      "['das', 'buch', 'ist', 'ja', 'klein']\n",
      "['the', 'book', 'is', 'small']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 0), (1, 1), (2, 2), (3, 2), (4, 3)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lexical translation model that ignores word orde\n",
    "# It is Used for Machine Translation\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.translate.ibm1 import IBMModel1\n",
    "from nltk.translate.api import AlignedSent\n",
    "\n",
    "bitext = []\n",
    "\n",
    "bitext.append(AlignedSent(['klein', 'ist', 'das', 'haus'], ['the', 'house', 'is', 'small']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'haus', 'ist', 'ja', 'groß'], ['the', 'house', 'is', 'big']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'buch', 'ist', 'ja', 'klein'], ['the', 'book', 'is', 'small']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'haus'], ['the', 'house']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'buch'], ['the', 'book']))\n",
    "\n",
    "bitext.append(AlignedSent(['ein', 'buch'], ['a', 'book']))\n",
    "\n",
    "ibm1 = IBMModel1(bitext, 5)\n",
    "\n",
    "print(ibm1.translation_table['buch']['book'])\n",
    "\n",
    "print(ibm1.translation_table['das']['book'])\n",
    "\n",
    "print(ibm1.translation_table['buch'][None])\n",
    "\n",
    "print(ibm1.translation_table['ja'][None])\n",
    "\n",
    "test_sentence = bitext[2]\n",
    "\n",
    "print(test_sentence.words)\n",
    "\n",
    "print(test_sentence.mots)\n",
    "\n",
    "test_sentence.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding of the Characters in the string : for LSTM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "word_vec_length = 10 # maximum length of the name\n",
    "\n",
    "char_to_int = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, \n",
    "               'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, \n",
    "               'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, \n",
    "               'w': 22, 'x': 23, 'y': 24, 'z': 25, 'ö': 26, 'ä': 27, 'ü': 28,\n",
    "               '-': 29, 'ß': 30, ' ': 31}\n",
    "\n",
    "char_vec_length = len(char_to_int)\n",
    "\n",
    "\n",
    "# Returns a list of n lists with n = word_vec_length\n",
    "def name_encoding(name):\n",
    "\n",
    "    # Encode input data to int, e.g. a->1, z->26\n",
    "    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i < word_vec_length]\n",
    "    \n",
    "    # Start one-hot-encoding\n",
    "    onehot_encoded = list()\n",
    "    \n",
    "    for value in integer_encoded:\n",
    "        # create a list of n zeros, where n is equal to the number of accepted characters\n",
    "        letter = [0 for _ in range(char_vec_length)]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "        \n",
    "    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n",
    "    for _ in range(word_vec_length - len(name)):\n",
    "        onehot_encoded.append([0 for _ in range(char_vec_length)])\n",
    "        \n",
    "    return onehot_encoded\n",
    "\n",
    "names = ['santhosh','anand','sachin','prashanth']\n",
    "\n",
    "# use asarray to use change the shape of the array which suits for LSTM\n",
    "enc = np.asarray([np.asarray(name_encoding(name)) for name in names])\n",
    "\n",
    "print(enc.shape)\n",
    "\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"santhosh\",\"kumar\",\"anand\",\"joshi\"] # Samples\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# get the alphabest\n",
    "string.ascii_lowercase\n",
    "\n",
    "# List of Vocabulary\n",
    "vocabs_ = [x for x in string.ascii_lowercase] # Features\n",
    "\n",
    "# vocabulary length\n",
    "vocab_length = len(string.ascii_lowercase)\n",
    "\n",
    "# Max Sequence Length\n",
    "max_seq_length = max([len(x) for x in names])\n",
    "\n",
    "char_2_int = {} \n",
    "int_2_char = {}\n",
    "\n",
    "# Char to Interger \n",
    "char_2_int = dict([(x,i) for i,x in enumerate(vocabs_)])\n",
    "\n",
    "# Integer to Char\n",
    "int_2_char = dict([(i,x) for i,x in enumerate(vocabs_)])\n",
    "\n",
    "# Create Zero array for encoding\n",
    "\n",
    "encoding = np.zeros((len(names),max_seq_length,vocab_length))\n",
    "\n",
    "print(encoding.shape)  # (4 Samples,max length of samples,vocabulary Length\n",
    "\n",
    "# Update the encoder matrix for the samples\n",
    "\n",
    "for i,name in enumerate(names): # i = index value and i = sample and j =time step\n",
    "    for j,char in enumerate(name):\n",
    "        encoding[i,j,char_2_int[char]] = 1.0\n",
    "        \n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projection: [0.59865848 0.15601864 0.15599452]\n",
      "\n",
      "\n",
      "output_weight_matrix: [[0.43194502 0.29122914 0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.45606998 0.78517596 0.19967378 0.51423444 0.59241457 0.04645041]\n",
      " [0.60754485 0.17052412 0.06505159 0.94888554 0.96563203 0.80839735]]\n",
      "\n",
      "\n",
      "output_array_for_input_tape_and_orange_output_context: [0.42451664 0.32344971 0.40759145 0.31176029 0.41795589 0.35267831]\n",
      "\n",
      "\n",
      "[('duct', 0.42451663675598933), ('tape', 0.32344971050993737), ('work', 0.4075914505752598), ('anywher', 0.3117602853605092), ('magic', 0.41795589389125587), ('worship', 0.35267831257488347)]\n"
     ]
    }
   ],
   "source": [
    "# Building Word Embedding Using Word Projection\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "v = [\"duct\", \"tape\", \"work\", \"anywher\", \"magic\", \"worship\"]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "input_array_tape=np.array([0,1,0,0,0,0]) #\"tape\"\n",
    "\n",
    "input_weight_matrix = np.random.random_sample((6,3))\n",
    "\n",
    "projection = np.dot(input_array_tape,input_weight_matrix)\n",
    "\n",
    "print(\"projection:\",projection)\n",
    "\n",
    "output_weight_matrix = np.random.random_sample((3,6))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"output_weight_matrix:\",output_weight_matrix)\n",
    "\n",
    "output_array_for_input_tape_and_orange_output_context = np.dot(projection, output_weight_matrix)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"output_array_for_input_tape_and_orange_output_context:\",output_array_for_input_tape_and_orange_output_context)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(list(zip(v, output_array_for_input_tape_and_orange_output_context)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CBOW\n",
    "\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "norm_bible = ['beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "wids = [[w for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 6, 7, 2], [2, 8, 9, 10, 11, 3, 4, 12], [13, 1, 14, 3, 4, 15]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id\n",
    "\n",
    "# build vocabulary of unique words\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 6, 7, 2]\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '5'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "Input: [1, 6]\n",
      "Output: 5\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 6]\n",
      "context_words1 : [[1, 6]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '1'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '5'\n",
      "Word at range index at 0 is '5'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "Input: [5, 6, 7]\n",
      "Output: 1\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [5, 6, 7]\n",
      "context_words1 : [[5, 6, 7]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '6'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '5'\n",
      "Word at range index at 0 is '5'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 4 is '2'\n",
      "Input: [5, 1, 7, 2]\n",
      "Output: 6\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [5, 1, 7, 2]\n",
      "context_words1 : [[5, 1, 7, 2]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '7'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 4 is '2'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [1, 6, 2]\n",
      "Output: 7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 6, 2]\n",
      "context_words1 : [[1, 6, 2]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '2'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [6, 7]\n",
      "Output: 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [6, 7]\n",
      "context_words1 : [[6, 7]]\n",
      "*****************************\n",
      "###############################################################################\n",
      "[2, 8, 9, 10, 11, 3, 4, 12]\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '2'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "Input: [8, 9]\n",
      "Output: 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [8, 9]\n",
      "context_words1 : [[8, 9]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '8'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 0 is '2'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "Input: [2, 9, 10]\n",
      "Output: 8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [2, 9, 10]\n",
      "context_words1 : [[2, 9, 10]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '9'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 0 is '2'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "Input: [2, 8, 10, 11]\n",
      "Output: 9\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [2, 8, 10, 11]\n",
      "context_words1 : [[2, 8, 10, 11]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '10'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "Input: [8, 9, 11, 3]\n",
      "Output: 10\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [8, 9, 11, 3]\n",
      "context_words1 : [[8, 9, 11, 3]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '11'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "Input: [9, 10, 3, 4]\n",
      "Output: 11\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [9, 10, 3, 4]\n",
      "context_words1 : [[9, 10, 3, 4]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 5\n",
      "Word at index 5 : '3'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 3\n",
      "end: 8\n",
      "Choosing context word based on condition:\n",
      "[3, 4, 5, 6, 7]\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '12'\n",
      "Word at range index at 7 is '12'\n",
      "Input: [10, 11, 4, 12]\n",
      "Output: 3\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [10, 11, 4, 12]\n",
      "context_words1 : [[10, 11, 4, 12]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 6\n",
      "Word at index 6 : '4'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 4\n",
      "end: 9\n",
      "Choosing context word based on condition:\n",
      "[4, 5, 6, 7, 8]\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '12'\n",
      "Word at range index at 7 is '12'\n",
      "range index: 8\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [11, 3, 12]\n",
      "Output: 4\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [11, 3, 12]\n",
      "context_words1 : [[11, 3, 12]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 7\n",
      "Word at index 7 : '12'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 5\n",
      "end: 10\n",
      "Choosing context word based on condition:\n",
      "[5, 6, 7, 8, 9]\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 8\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 9\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [3, 4]\n",
      "Output: 12\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [3, 4]\n",
      "context_words1 : [[3, 4]]\n",
      "*****************************\n",
      "###############################################################################\n",
      "[13, 1, 14, 3, 4, 15]\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '13'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "Input: [1, 14]\n",
      "Output: 13\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 14]\n",
      "context_words1 : [[1, 14]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '1'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '13'\n",
      "Word at range index at 0 is '13'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "Input: [13, 14, 3]\n",
      "Output: 1\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [13, 14, 3]\n",
      "context_words1 : [[13, 14, 3]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '14'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '13'\n",
      "Word at range index at 0 is '13'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "Input: [13, 1, 3, 4]\n",
      "Output: 14\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [13, 1, 3, 4]\n",
      "context_words1 : [[13, 1, 3, 4]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '3'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '15'\n",
      "Word at range index at 5 is '15'\n",
      "Input: [1, 14, 4, 15]\n",
      "Output: 3\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 14, 4, 15]\n",
      "context_words1 : [[1, 14, 4, 15]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '4'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '15'\n",
      "Word at range index at 5 is '15'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [14, 3, 15]\n",
      "Output: 4\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [14, 3, 15]\n",
      "context_words1 : [[14, 3, 15]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 5\n",
      "Word at index 5 : '15'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 3\n",
      "end: 8\n",
      "Choosing context word based on condition:\n",
      "[3, 4, 5, 6, 7]\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [3, 4]\n",
      "Output: 15\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [3, 4]\n",
      "context_words1 : [[3, 4]]\n",
      "*****************************\n",
      "###############################################################################\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "for words in wids:\n",
    "    print(words)\n",
    "    sentence_length = len(words)\n",
    "    for index,word in enumerate(words):\n",
    "        print(\"sentence_length:\",sentence_length)\n",
    "        print(\"WindowSize:\",window_size)\n",
    "        print(\"index:\",index)\n",
    "        print(\"Word at index {0} : '{1}'\".format(index,word))\n",
    "        \n",
    "        context_words =[]\n",
    "        context_words1 = []\n",
    "       \n",
    "        print(\"start = index - window_size\")\n",
    "        print(\"end = index + window_size + 1\")\n",
    "        \n",
    "        start = index - window_size\n",
    "        end = index + window_size + 1\n",
    "        \n",
    "\n",
    "        print(\"start:\",start)\n",
    "        print(\"end:\",end)\n",
    "       \n",
    "        print(\"Choosing context word based on condition:\")\n",
    "        \n",
    "        print([x for x in range(start, end)])\n",
    "        for i in range(start, end):\n",
    "            \n",
    "            print(\"range index:\",i)\n",
    "            \n",
    "            if 0 <= i:\n",
    "                print(\"Statisfied the condition '0 <= i' \")\n",
    "                \n",
    "                if i < sentence_length:\n",
    "                    print(\"Statisfied the condition 'i < sentence_length' \")\n",
    "                    \n",
    "                    if i != index:\n",
    "                        print(\"Statisfied the condition 'i != index'\")\n",
    "                        print(\"Selected Word: '{0}'\".format(words[i]))\n",
    "            \n",
    "            if  0 <= i < sentence_length and i != index:\n",
    "                print(\"Word at range index at {0} is '{1}'\".format(i,words[i]))\n",
    "                context_words.append(words[i])\n",
    "                \n",
    "        \n",
    "        print(\"Input:\",context_words)   \n",
    "        print(\"Output:\",words[index])\n",
    "        \n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    \n",
    "        context_words1.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"context_words :\",context_words)\n",
    "        print(\"context_words1 :\",context_words1)\n",
    "        print(\"*****************************\")\n",
    "    print(\"###############################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "def generator():\n",
    "    for words in wids:\n",
    "        sentence_length = len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_words =[]\n",
    "            label_word = []\n",
    "\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=7)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[0 0 0 0 0 1 6]]  output: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 5 6 7]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 5 1 7 2]]  output: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 1 6 2]]  output: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 6 7]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 8 9]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0  2  9 10]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  2  8 10 11]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  8  9 11  3]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  9 10  3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0 10 11  4 12]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0 11  3 12]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0  0  1 14]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "input: [[ 0  0  0  0 13 14  3]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0 13  1  3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "input: [[ 0  0  0  1 14  4 15]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0 14  3 15]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for x ,y in generator():\n",
    "    print(\"input: {0}  output: {1}\".format(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(heaven (7), earth (2)) -> 1\n",
      "(heaven (7), beginning (5)) -> 1\n",
      "(god (1), waters (15)) -> 0\n",
      "(earth (2), without (8)) -> 0\n",
      "(created (6), earth (2)) -> 1\n",
      "(beginning (5), heaven (7)) -> 1\n",
      "(created (6), void (10)) -> 0\n",
      "(created (6), darkness (11)) -> 0\n",
      "(heaven (7), beginning (5)) -> 0\n",
      "(god (1), earth (2)) -> 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16\n",
      "Vocabulary Sample: [('god', 1), ('earth', 2), ('upon', 3), ('face', 4), ('beginning', 5), ('created', 6), ('heaven', 7), ('without', 8), ('form', 9), ('void', 10)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Glove : Pretained Word Embedding\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "corpus = [\n",
    "            'This is an excellent movie',\n",
    "            'The move was fantastic I like it',\n",
    "            'You should watch it is brilliant',\n",
    "            'Exceptionally good',\n",
    "            'Wonderfully directed and executed I like it',\n",
    "            'Its a fantastic series',\n",
    "            'Never watched such a brillent movie',\n",
    "            'It is a Wonderful movie',\n",
    "        ]\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 2, 9, 10, 3],\n",
       " [11, 12, 13, 5, 6, 7, 1],\n",
       " [14, 15, 16, 1, 2, 17],\n",
       " [18, 19],\n",
       " [20, 21, 22, 23, 6, 7, 1],\n",
       " [24, 4, 5, 25],\n",
       " [26, 27, 28, 4, 29, 3],\n",
       " [1, 2, 4, 30, 3]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pretrained Word Embeddings : Glove\n",
    "\n",
    "# The smallest file is named \"Glove.6B.zip\". The size of the file is 822 MB. \n",
    "# The file contains 50, 100, 200, and 300 dimensional word vectors for 400k words. We will be using the 100 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "# Empty dictionary that will store our word embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "# Loading the Pretrained Glove Word Embedding Model\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "#  Here notice that we loaded glove.6B.100d.txt file. This file contains 100 dimensional word embeddings. \n",
    "# We will create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values, \n",
    "#  in the form of an array.\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "    \n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dictionary embeddings_dictionary now contains words and corresponding GloVe embeddings for all the words\n",
    "\n",
    "embeddings_dictionary['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1766   ,  0.093851 ,  0.24351  ,  0.44313  , -0.39037  ,\n",
       "        0.12524  , -0.19918  ,  0.59855  , -0.82035  ,  0.28006  ,\n",
       "        0.54231  ,  0.023079 ,  0.12837  , -0.044489 ,  0.3837   ,\n",
       "       -0.75659  ,  0.40254  , -0.4462   , -0.81599  , -0.0091513,\n",
       "        0.65219  , -0.043656 ,  0.54919  , -0.16696  ,  0.73028  ,\n",
       "       -0.20703  , -0.069863 , -0.31259  ,  0.27226  ,  0.084905 ,\n",
       "       -0.60498  ,  0.42826  ,  0.60134  ,  0.50953  , -0.39073  ,\n",
       "        0.44608  , -0.36331  ,  0.50858  , -0.20308  , -0.43503  ,\n",
       "       -0.086827 , -0.86581  , -1.0151   , -0.35725  , -0.12993  ,\n",
       "        0.3324   ,  0.3026   ,  0.067277 , -0.52948  , -0.81223  ,\n",
       "        0.39562  , -0.79537  ,  0.24331  ,  1.2506   , -1.0169   ,\n",
       "       -3.3391   , -0.79691  , -0.33877  ,  1.366    ,  0.87513  ,\n",
       "       -0.63701  ,  0.68381  , -0.057432 ,  0.12541  , -0.8258   ,\n",
       "       -0.56117  ,  0.30807  ,  0.1545   ,  0.61473  ,  0.67403  ,\n",
       "       -0.60833  , -0.25911  , -0.35619  , -0.71189  , -0.31207  ,\n",
       "        0.035238 ,  0.22488  , -0.33492  , -1.1586   , -0.17373  ,\n",
       "        0.95937  ,  0.24479  , -0.46205  , -0.075941 , -1.0844   ,\n",
       "        0.093676 ,  0.48546  ,  0.13008  ,  0.23455  , -0.27964  ,\n",
       "       -0.24481  , -0.016213 ,  0.46302  ,  1.0291   , -0.81817  ,\n",
       "        0.17522  ,  0.06797  ,  0.056305 ,  1.2312   ,  0.40695  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dictionary['at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:this and index:1\n",
      "[-0.57058   0.44183   0.70102  -0.41713  -0.34058   0.02339  -0.071537\n",
      "  0.48177  -0.013121  0.16834  -0.13389   0.040626  0.15827  -0.44342\n",
      " -0.019403 -0.009661 -0.046284  0.093228 -0.27331   0.2285    0.33089\n",
      " -0.36474   0.078741  0.3585    0.44757  -0.2299    0.18077  -0.6265\n",
      "  0.053852 -0.29154  -0.4256    0.62903   0.14393  -0.046004 -0.21007\n",
      "  0.48879  -0.057698  0.37431  -0.030075 -0.34494  -0.29702   0.15095\n",
      "  0.28248  -0.16578   0.076131 -0.093016  0.79365  -0.60489  -0.18874\n",
      " -1.0173    0.31962  -0.16344   0.54177   1.1725   -0.47875  -3.3842\n",
      " -0.081301 -0.3528    1.8372    0.44516  -0.52666   0.99786  -0.32178\n",
      "  0.033462  1.1783   -0.072905  0.39737   0.26166   0.33111  -0.35629\n",
      " -0.16558  -0.44382  -0.14183  -0.37976   0.28994  -0.029114 -0.35169\n",
      " -0.27694  -1.344     0.19555   0.16887   0.040237 -0.80212   0.23366\n",
      " -1.3837   -0.023132  0.085395 -0.74051  -0.073934 -0.58838  -0.085735\n",
      " -0.10525  -0.51571   0.15038  -0.16694  -0.16372  -0.22702  -0.66102\n",
      "  0.47197   0.37253 ]\n",
      "word:is and index:2\n",
      "[-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n",
      "word:an and index:3\n",
      "[-0.4214   -0.18797   0.46241  -0.17605   0.36212   0.36701   0.27924\n",
      "  0.14634  -0.054227  0.45834   0.065416 -0.33725   0.067505 -0.36316\n",
      "  0.50302  -0.010361  0.72826  -0.17564  -0.33996   0.072864  0.64481\n",
      " -0.23908   0.38383   0.13858   1.0994   -0.24883  -0.15078  -0.48738\n",
      " -0.23042   0.064788 -0.70183   0.82654   0.06128   0.18531  -0.30162\n",
      " -0.022151  0.34302   0.80331   0.17135   0.15462  -0.50759   0.39572\n",
      "  0.054291 -0.53081   0.48252   0.086205  0.59585  -0.22377  -0.3955\n",
      " -0.73036  -0.10279  -0.39166   1.229     1.2129   -1.0365   -3.4971\n",
      "  0.10923  -1.0084    1.9998    0.7964    0.3881    0.43746   0.085194\n",
      "  0.38549   0.61993  -1.032     0.70119  -0.2246    0.079435  0.09126\n",
      " -0.21196  -0.55429  -0.053352 -0.80201   0.46798  -0.05005  -0.57422\n",
      " -0.084822 -1.7227   -0.94286   0.98667   0.31211  -0.37735   0.068674\n",
      " -0.77838  -0.28486   0.81047   0.46596  -0.11865  -0.93411   0.33722\n",
      "  0.037906 -0.18273  -0.019941  0.20494  -0.47718  -0.49253  -0.56518\n",
      "  0.72558  -0.15913 ]\n",
      "word:excellent and index:4\n",
      "[-0.2816     0.18427   -0.06755    0.27694   -0.066775  -0.41389\n",
      "  0.30757   -0.11097   -0.84585   -0.17047    0.0062422 -0.65395\n",
      "  0.28771   -0.12409   -0.26717    0.026893   0.17115   -0.46256\n",
      "  0.19549    1.1399    -0.46206    0.39222   -0.18622   -0.53259\n",
      "  0.073297   0.0045262 -0.45476    0.16952   -0.41111   -0.31766\n",
      " -0.73616    0.56228   -0.26528   -0.088054   0.93175    0.46633\n",
      " -0.36245    0.46954   -0.18022   -0.07036    0.55793    0.13965\n",
      "  0.38983   -0.04636    0.55198    0.020288   0.34741   -0.61839\n",
      "  0.051404  -0.83699    0.05628   -0.24738   -0.19872    0.75093\n",
      "  0.068948  -1.7575     0.70621    0.079792   1.1462    -0.63423\n",
      " -0.68378    0.51682   -0.68141    0.29451    0.25864   -0.25839\n",
      "  0.36467    0.026786   0.35151   -0.13273    0.18513    0.52367\n",
      "  1.0416    -0.12293    0.77957    0.41305   -0.28948    0.19722\n",
      "  0.41088   -0.35153    0.48837    0.58854   -0.37273   -0.45631\n",
      " -0.98976   -0.070993   0.48845   -0.082542  -0.49009   -0.29484\n",
      "  0.031072  -0.38779    0.15321   -0.0078053 -0.43276    0.37334\n",
      " -0.76888   -0.82518    0.28753    0.76069  ]\n",
      "word:movie and index:5\n",
      "[ 0.38251    0.14821    0.60601   -0.51533    0.43992    0.061053\n",
      " -0.62716   -0.025385   0.1643    -0.22101    0.14423   -0.37213\n",
      " -0.21683   -0.08895    0.097904   0.6561     0.64455    0.47698\n",
      "  0.83849    1.6486     0.88922   -0.1181    -0.012465  -0.52082\n",
      "  0.77854    0.48723   -0.014991  -0.14127   -0.34747   -0.29595\n",
      "  0.1028     0.57191   -0.045594   0.026443   0.53816    0.32257\n",
      "  0.40788   -0.043599  -0.146     -0.48346    0.32036    0.55086\n",
      " -0.76259    0.43269    0.61753   -0.36503   -0.60599   -0.79615\n",
      "  0.3929    -0.23668   -0.34719   -0.61201    0.54747    0.94812\n",
      "  0.20941   -2.7771    -0.6022     0.8495     1.2549     0.017893\n",
      " -0.041901   2.1147    -0.026618  -0.28104    0.68124   -0.14165\n",
      "  0.99249    0.49879   -0.67538    0.6417     0.42303   -0.27913\n",
      "  0.063403   0.68909   -0.36183    0.053709  -0.16806    0.19422\n",
      " -0.47073   -0.14803   -0.58986   -0.2797     0.16792    0.10568\n",
      " -1.7601     0.0088254 -0.83326   -0.5836    -0.37079   -0.56591\n",
      "  0.20699    0.071315   0.055586  -0.29757   -0.072659  -0.25596\n",
      "  0.42688    0.058921   0.091112   0.47283  ]\n",
      "word:the and index:6\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "word:move and index:7\n",
      "[-0.021606  -0.10639    0.11104   -0.19535   -0.15463   -0.021794\n",
      " -0.49271    0.037687   0.03788    0.079461   0.11757    0.21303\n",
      "  0.057177  -0.59581   -0.12174    0.058865  -0.3029     0.33552\n",
      " -0.41281   -0.069985   0.75037    0.11583   -0.27081    0.46937\n",
      "  0.21231   -0.11562   -0.21439   -0.33636    0.12305    0.019461\n",
      " -0.39404    0.8718    -0.17674   -0.13337    0.0030434  0.27575\n",
      "  0.22314   -0.37808   -0.35052   -0.23319   -0.4751    -0.54627\n",
      " -0.21481   -0.35311   -0.25409    0.13777    0.088258   0.27232\n",
      "  0.17943   -0.79158    0.4327     0.029326  -0.0076881  1.2184\n",
      "  0.048029  -2.5612    -0.24577   -0.39375    1.6298     0.23657\n",
      " -0.46253    0.49255   -0.16006   -0.01842    0.88971   -0.022171\n",
      " -0.19875    0.72445   -0.22658   -0.56365    0.083209  -0.90564\n",
      " -0.39346   -1.0547     0.35143   -0.32817   -0.2591     0.39502\n",
      " -0.2802     0.15717    0.42952   -0.26731   -0.56228   -0.35319\n",
      " -1.026     -0.51456    0.70897    0.23269    0.21666   -0.69723\n",
      " -0.43846   -0.17088   -0.39565    0.060127  -0.72325   -0.16621\n",
      "  0.28695    0.11868    0.6948     0.21722  ]\n",
      "word:was and index:8\n",
      "[ 1.3717e-01 -5.4287e-01  1.9419e-01 -2.9953e-01  1.7545e-01  8.4672e-02\n",
      "  6.7752e-01  9.8295e-02 -3.5611e-02  2.1334e-01  5.1663e-01  2.0687e-01\n",
      "  4.4082e-01 -3.3655e-01  5.6025e-01 -6.8790e-01  5.1957e-01 -2.1258e-01\n",
      " -5.2708e-01 -1.2249e-01  3.3099e-01  2.6448e-02  5.9007e-01  6.5469e-03\n",
      "  4.5405e-01 -3.3884e-01 -2.8261e-01 -2.4633e-01  1.0847e-01  3.1640e-01\n",
      " -1.5368e-01  7.3503e-01  1.1858e-01  7.0842e-01  7.5081e-02  2.9738e-01\n",
      " -1.1395e-01  4.0807e-01 -4.2531e-02 -2.1301e-01 -7.9849e-01 -1.2703e-01\n",
      "  7.5200e-01 -4.1746e-01  4.6615e-01 -3.9097e-02  6.5961e-01 -3.2336e-01\n",
      "  4.4200e-01 -9.4137e-01 -2.3125e-01 -3.0604e-01  7.9912e-01  1.4581e+00\n",
      " -8.8199e-01 -3.0041e+00 -7.5243e-01 -2.0503e-01  1.1998e+00  9.4881e-01\n",
      "  3.0649e-01  4.8411e-01 -7.5720e-01  6.5856e-01  7.0107e-01 -9.3141e-01\n",
      "  5.2928e-01  2.3323e-01  1.8857e-01  3.8691e-01  1.1489e-02 -3.1937e-01\n",
      "  1.1858e-02  2.2944e-01  1.7764e-01  1.6868e-01  1.4003e-01  5.8647e-01\n",
      " -1.5447e+00 -6.4425e-02 -6.4711e-04  1.3606e-01 -3.2695e-01  1.0043e-01\n",
      " -1.5460e+00 -5.4760e-01  2.1027e-01 -6.7195e-01 -1.5970e-01 -6.8271e-01\n",
      " -2.2043e-01 -8.7088e-01 -1.6248e-01  8.3086e-01 -2.3045e-01  1.9864e-01\n",
      " -5.1892e-02 -5.2057e-01  2.5434e-01 -2.3759e-01]\n",
      "word:fantasti and index:9\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating word embedding for our Corpus\n",
    "\n",
    "corpus = [\n",
    "            'This is an excellent movie',\n",
    "            'The move was fantasti',\n",
    "        ]\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "\n",
    "for word,index in word_tokenizer.word_index.items():\n",
    "    print(\"word:{0} and index:{1}\".format(word,index))\n",
    "    print(embeddings_dictionary.get(word))\n",
    "    \n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 100)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n",
    "\n",
    "# 31 : My corpus Vocabulary\n",
    "# 100 : Dimention of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec Using gensim Model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "norm_bible = ['beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters']\n",
    "\n",
    "norm_bible = [x.split() for x in norm_bible]\n",
    "\n",
    "norm_bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beginning',\n",
       " 'created',\n",
       " 'darkness',\n",
       " 'deep',\n",
       " 'earth',\n",
       " 'face',\n",
       " 'form',\n",
       " 'god',\n",
       " 'heaven',\n",
       " 'moved',\n",
       " 'spirit',\n",
       " 'upon',\n",
       " 'void',\n",
       " 'waters',\n",
       " 'without'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary\n",
    "_vocab = set([x for y in norm_bible for x in y])\n",
    "\n",
    "_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocab size\n",
    "len(_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Word2Vec \n",
    "\n",
    "model = Word2Vec(sentences=norm_bible, max_vocab_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you save the model you can continue training it later:\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03288836, -0.08910534,  0.15980722, -0.15729724,  0.0761887 ,\n",
       "        0.09141698, -0.02383908,  0.15236385,  0.1672206 , -0.09261578,\n",
       "       -0.10185118, -0.11413173, -0.13354973, -0.05155852, -0.09462849,\n",
       "       -0.14115852,  0.01324356,  0.05065757,  0.10851171, -0.0444713 ,\n",
       "       -0.07533497,  0.02113773,  0.00662196,  0.13730705,  0.00309225,\n",
       "        0.12232942, -0.13980244,  0.14266157, -0.03195277,  0.14718859,\n",
       "       -0.12884632,  0.03038765,  0.01787149,  0.00077822, -0.08632658,\n",
       "       -0.15643395, -0.12288143, -0.13450196,  0.03237257,  0.00809372,\n",
       "       -0.03067101,  0.12044464, -0.04187879, -0.02279109, -0.15056182,\n",
       "       -0.16789816,  0.15138787, -0.09733356, -0.10780554,  0.08795309,\n",
       "        0.11282953, -0.11556396,  0.01623527, -0.10163928,  0.02786644,\n",
       "       -0.07255738, -0.05820448,  0.03697271,  0.14651914,  0.11381263,\n",
       "       -0.16369697, -0.09510347,  0.13330361,  0.03365194, -0.07199534,\n",
       "        0.0101295 ,  0.16105644, -0.01865355, -0.15942708,  0.02720784,\n",
       "        0.10542644,  0.1062725 ,  0.06921428, -0.09557937, -0.00627064,\n",
       "       -0.00093576,  0.07733643, -0.13603143, -0.13563764,  0.0044785 ,\n",
       "       -0.1456179 ,  0.09845367, -0.0070677 ,  0.16867231, -0.09039867,\n",
       "       -0.00822354,  0.13121346, -0.06881312, -0.08484891,  0.02689762,\n",
       "        0.04483908, -0.04338882,  0.10906629, -0.12957565,  0.05740541,\n",
       "        0.00828831,  0.1477135 ,  0.10120351,  0.11528857,  0.13232607],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector('heaven', norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulding the vector : Numpy array for the word \n",
    "\n",
    "model.wv['god']  # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['god','heaven']  # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get other similar words\n",
    "\n",
    "sims = model.wv.most_similar('god', topn=10)  \n",
    "\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between Word2Vec and Doc2Vec\n",
    "# 1. As the names suggest Word2Vec is trained on single words while Doc2vec is trained texts of variable length,\n",
    "\n",
    "# Glove and word2vec, these are unsupervised approaches for building vectors from text data.\n",
    "# The main difference is the functioning of both models. Glove is Count based model, and Word2vec is predictive modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
