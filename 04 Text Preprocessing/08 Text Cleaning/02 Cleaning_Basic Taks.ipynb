{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter is the most miserable , lonely boy you can imagine . he 's shunned by his relatives , the dursley 's , that have raised him since he was an infant . he 's forced to live in the cupboard under the stairs , forced to wear his cousin dudley 's hand-me-down clothes , and forced to go to his neighbour 's house when the rest of the family is doing something fun . yes , he 's just about as miserable as you can get .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def to_lower(text):\n",
    "    \"\"\"\n",
    "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
    "\n",
    "\n",
    "text = \"\"\"Harry Potter is the most miserable, lonely boy you can imagine. He's shunned by his relatives, \\\n",
    "          the Dursley's, that have raised him since he was an infant. He's forced to live in the cupboard under the \n",
    "          stairs, forced to wear his cousin Dudley's hand-me-down clothes, and forced to go to his neighbour's house when \n",
    "          the rest of the family is doing something fun. Yes, he's just about as miserable as you can get.\"\"\"\n",
    "\n",
    "print(to_lower(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'great', 'day', '.', 'It', 'even', 'better', 'yesterday', '.', 'And', 'yesterday', 'best', 'day', 'ever', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=\"Today is a great day. It is even better than yesterday. And yesterday was the best day ever!\"\n",
    "stopwords=set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words=word_tokenize(text)\n",
    "\n",
    "wordsFiltered=[]\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word Removal using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "word_tokenize(example_sent)\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "[x for x in word_tokenize(example_sent) if x not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopward from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joe',\n",
       " 'waited',\n",
       " 'train',\n",
       " 'train',\n",
       " 'late',\n",
       " 'mary',\n",
       " 'samantha',\n",
       " 'took',\n",
       " 'bus',\n",
       " 'looked',\n",
       " 'mary',\n",
       " 'samantha',\n",
       " 'bus',\n",
       " 'station',\n",
       " 'mary',\n",
       " 'samantha',\n",
       " 'arrived',\n",
       " 'bus',\n",
       " 'station',\n",
       " 'early',\n",
       " 'waited',\n",
       " 'noon',\n",
       " 'bus']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "vocab = ['Joe', 'waited', 'for', 'the', 'train', 'The', 'train', 'was', 'late', 'Mary', \n",
    "         'and', 'Samantha', 'took', 'the', 'bus', 'I', 'looked', 'for', 'Mary', 'and', \n",
    "         'Samantha', 'at', 'the', 'bus', 'station', 'Mary', 'and', 'Samantha', 'arrived', \n",
    "         'at', 'the', 'bus', 'station', 'early', 'but', 'waited', 'until', 'noon', 'for', \n",
    "         'the', 'bus']\n",
    "         \n",
    "[v for v in (v.lower() for v in vocab) if v not in nltk.corpus.stopwords.words('english') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword_Remove_Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love car</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>view amazing</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feel great morning</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excited concert</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best friend</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet     class\n",
       "0            love car  positive\n",
       "1        view amazing  positive\n",
       "2  feel great morning  positive\n",
       "3     excited concert  positive\n",
       "4         best friend  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pos_tweets = [('I love this car', 'positive'),\n",
    "              ('This view is amazing', 'positive'),\n",
    "              ('I feel great this morning', 'positive'),\n",
    "              ('I am so excited about the concert', 'positive'),\n",
    "              ('He is my best friend', 'positive')]\n",
    "\n",
    "test = pd.DataFrame(pos_tweets)\n",
    "\n",
    "\n",
    "test = pd.DataFrame(pos_tweets)\n",
    "test.columns = [\"tweet\",\"class\"]\n",
    "\n",
    "test[\"tweet\"] = test[\"tweet\"].str.lower().str.split()\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "test['tweet'] = test['tweet'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "test['tweet'] = test['tweet'].apply(lambda y: ' '.join([x for x in y]))\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was  people standing right next to me at pm.\n"
     ]
    }
   ],
   "source": [
    "text = \"There was 200 people standing right next to me at 2pm.\"\n",
    "output = ''.join(c for c in text if not c.isdigit())\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text,remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you doing\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "text = \"Hello! how are you doing?\"\n",
    "\n",
    "print (strip_punctuation(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Accented Charaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def Remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return text\n",
    "\n",
    "Remove_accented_chars(\"Sèv asdaç\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"<head><body>hello world!</body></head>\"\"\"\n",
    "cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "print (cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'likes',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'movies',\n",
       " 'mary',\n",
       " 'too',\n",
       " 'also',\n",
       " 'football',\n",
       " 'games']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "corpus = [\"John likes to watch movies. Mary likes movies too.\",\n",
    "          \"John also likes to watch football games.\",]\n",
    "\n",
    "vocab = []\n",
    "\n",
    "# Generate  Vocabuary \n",
    "for sentence in corpus:\n",
    "    w = nltk.word_tokenize(sentence)\n",
    "    vocab.extend(w)\n",
    "\n",
    "# Remove the Punctuatin\n",
    "from string import punctuation\n",
    "vocab = [v for v in vocab if v not in punctuation]\n",
    "\n",
    "# lowercase conversion\n",
    "vocab = [v.lower() for v in vocab]\n",
    "\n",
    "# Remove Duplicates\n",
    "vocab = list(dict.fromkeys(vocab))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Repeating Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "def remove_repeated_characters(token):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    \n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution,old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    correct_tokens = [replace(word) for word in token]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "sample_sentence = \"My Schoool is realllllyyyy amaaaaazinggg\"\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
    "' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import re\n",
    "import nltk\n",
    "from contractions import contractions_dict\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "        if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def main():\n",
    "    text = \"\"\"I ain't going there. You'll have to go alone.\"\"\"\n",
    "    \n",
    "    text=expand_contractions(text,contractions_dict)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    print (tokenized_sentences)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Words Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I had / I would like to know how I had / I would done that!\n",
      "We're going to the zoo and I do not think I shall / I will be home for dinner.\n",
      "Theyre going to the zoo and she shall / she will be home for dinner.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I will not let you get away with that',\n",
       " 'I am a bad person',\n",
       " 'It is his cat anyway',\n",
       " 'It is not what you think',\n",
       " \"It is a man's world\",\n",
       " \"Catherine's been thinking about it\",\n",
       " 'It will be done',\n",
       " \"Who'd've thought!\",\n",
       " 'She said she had / she would go.',\n",
       " 'She said she had / she would gone.',\n",
       " \"Y'all'd've a great time\",\n",
       " 'My name is Jack.',\n",
       " \"'Tis questionable whether Ma'am should be going.\",\n",
       " \"As history tells, 'twas the night before Christmas.\",\n",
       " \"Martha, Peter and Christine've been indulging in a menage-à-trois.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"It's\": \"It is\",\n",
    "\"It'll\" : \"It will\"\n",
    "}\n",
    "\n",
    "\n",
    "# Contractions conversions\n",
    "def expand_text(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    new_text = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in contractions:\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return ' '.join(new_text)\n",
    "\n",
    "print(expand_text(\"I'd like to know how I'd done that!\"))\n",
    "\n",
    "print(expand_text(\"We're going to the zoo and I don't think I'll be home for dinner.\"))\n",
    "\n",
    "print(expand_text(\"Theyre going to the zoo and she'll be home for dinner.\"))\n",
    "\n",
    "\n",
    "words =  [\"I won't let you get away with that\",\n",
    "    \"I'm a bad person\",\n",
    "    \"It's his cat anyway\",\n",
    "    \"It's not what you think\",\n",
    "    \"It's a man's world\",\n",
    "    \"Catherine's been thinking about it\",\n",
    "    \"It'll be done\",\n",
    "    \"Who'd've thought!\",\n",
    "    \"She said she'd go.\",\n",
    "    \"She said she'd gone.\",\n",
    "    \"Y'all'd've a great time\",\n",
    "    \" My name is Jack.\",\n",
    "    \"'Tis questionable whether Ma'am should be going.\",\n",
    "    \"As history tells, 'twas the night before Christmas.\",\n",
    "    \"Martha, Peter and Christine've been indulging in a menage-à-trois.\",] \n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "newwords = []\n",
    "for word in words:\n",
    "    newwords.append(expand_text(word))\n",
    "\n",
    "newwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sugar', 'bad', 'consume', 'sister', 'like', 'sugar', 'father'],\n",
       " ['father',\n",
       "  'spends',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'driving',\n",
       "  'sister',\n",
       "  'around',\n",
       "  'dance',\n",
       "  'practice'],\n",
       " ['doctor',\n",
       "  'suggest',\n",
       "  'driving',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'increased',\n",
       "  'stress',\n",
       "  'blood',\n",
       "  'pressure'],\n",
       " ['sometimes',\n",
       "  'feel',\n",
       "  'pressure',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'father',\n",
       "  'never',\n",
       "  'seems',\n",
       "  'drive',\n",
       "  'sister',\n",
       "  'better'],\n",
       " ['health', 'expert', 'say', 'sugar', 'good', 'lifestyle']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "\n",
    "# Cleaning anf Preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#Stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def Clean(doc):\n",
    "    \n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    \n",
    "    stop_free = [v for v in tokens if v not in stop]\n",
    "    \n",
    "    punc_free = [v for v in stop_free if v not in exclude]\n",
    "    \n",
    "    normalized= [WordNetLemmatizer().lemmatize(v) for v in punc_free]\n",
    "    return ' '.join(v for v in normalized)\n",
    "    \n",
    "doc_clean = [Clean(doc).split() for doc in doc_complete]\n",
    "    \n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Visited United States from United Kingdonm on 22-10-2018'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Normalization : Replace the selected words with new words\n",
    "sentence = \"I Visited US from UK on 22-10-18\"\n",
    "\n",
    "sentence.replace(\"US\",\"United States\").replace('UK',\"United Kingdonm\").replace(\"-18\",\"-2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "['This', 'is', 'a', 'world', 'of', 'hope']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "from autocorrect import spell\n",
    "\n",
    "text = \"This is a wrld of hope\"\n",
    "spells = [spell(w) for w in (word_tokenize(text))]\n",
    "print (spells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "This is a great day , It is even better than \n",
      "This is a great day , It is even better than yesterday.And yester\n"
     ]
    }
   ],
   "source": [
    "# Search for the keyword using NLTK\n",
    "import nltk\n",
    "text = \"This is a great day,It is even better than yesterday.And yesterday was the best day ever.\"\n",
    "\n",
    "text = nltk.Text(nltk.word_tokenize(text))\n",
    "match = text.concordance('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "file = 'C:\\\\MyWork\\\\MyLearning\\\\ML\\\\Files\\\\DataSet\\\\SampleText.txt'\n",
    "file = open(file , 'r')\n",
    "text = file.read()\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "text = re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "tokenized_words_with_stopwords = word_tokenize(text)\n",
    "\n",
    "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "\n",
    "tokenized_words = [wordlemmatizer.lemmatize(x) for x in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet this is a retweeted tweet by Shivam Bansal\n"
     ]
    }
   ],
   "source": [
    "# Object Standardization\n",
    "# Text data often contains words or phrases which are not present in any standard lexical dictionaries. \n",
    "# These pieces are not recognized by search engines and models.\n",
    "\n",
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', 'awsm' : 'awesome', 'luv' :'love'}\n",
    "\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "             word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "    \n",
    "print(_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
