{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenization : \n",
    "    - Processig of Splitting sentences into its words.\n",
    "\n",
    "Tweet Tokenizer : \n",
    "    - Specifically designed for tokenizing tweets. It is capable of dealing with emotions and expression of sentiment.\n",
    "    \n",
    "MWE Tokenizer(Multi word Expression): \n",
    "    - Here certain group of multiple words are trated as one entity during tokenization.\n",
    "    \n",
    "Regular expression tokenizer : \n",
    "    - The tokenizer are devloped using regular expression.\n",
    "    - sentenses are split based on the occurrence of a particular pattern.\n",
    "\n",
    "PunktWordTokenizer : \n",
    "    - It splits on punctuation, but keeps it with the word instead of creating separate tokens\n",
    "\n",
    "Whitespace tokenizer : \n",
    "    - Splits  a string whenever a space,tab,or newline character present\n",
    "    \n",
    "WordPunctTokenizer : \n",
    "    - This splits text into list of alphabetical characters,digits, and non-alphabetical characters\n",
    "    \n",
    "Stemming : \n",
    "    - Stemming is a technique to remove affixes from a word, ending up with the stem.\n",
    "    \n",
    "Lemmatization : \n",
    "    - Problem with stemming is that often, stemmed words do not carry any meaning.\n",
    "    - Lemmatization deals with such cases where it returns base form of words that carries dectionary meaning\n",
    "\n",
    "Sentence Segmentation : Break the piece of text in various sentences\n",
    "\n",
    "Unigram : Extrcating one word at a time\n",
    "\n",
    "Bigram : Extrcat two tokens at a time. Set of two tokens\n",
    "\n",
    "POS : Part of Speech: It is a process of tagging words within sentences into their respective parts of speech and then \n",
    "      finally labeling them.\n",
    "\n",
    "Stop words : Stop word are commmon words that are just used support to construction of sentences.\n",
    "\n",
    "Text Normalization : It is a process wherein different variations of text get converted into a standard form.\n",
    "\n",
    "Various Steps in while creating NLP pipeline\n",
    "      1. Sentence Segmentation\n",
    "          - Break the piece of text in various sentences\n",
    "      2. Word Tokenization\n",
    "          - Breaking the sentences into indivisual words called as tokens.\n",
    "      3. Predicting parts of Speech for each token(POS Tags)\n",
    "          - Predicting wther the word is a noun,verd,adjective,adverb,pronoun etc\n",
    "      4. Stemmming\n",
    "      \n",
    "      5. Lemmatization\n",
    "      \n",
    "      6. Identifying the Stop words\n",
    "      \n",
    "      7. 1.Dependency parsing\n",
    "      \n",
    "      8. 2.Finding Noun Phrases\n",
    "      \n",
    "      9. Named Entity Recognition(NER)\n",
    "      \n",
    "      10. Coreference Resolution\n",
    "      \n",
    "Let us understand this using a simple example.\n",
    "\n",
    "D1: He is a lazy boy. She is also lazy.\n",
    "\n",
    "D2: Neeraj is a lazy person.\n",
    " \n",
    "The dictionary created may be a list of unique tokens(words) in the corpus =[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]\n",
    " \n",
    "Here, D=2, N=6\n",
    "\n",
    "The count matrix M of size 2 X 6 will be represented as –\n",
    "\n",
    "He\tShe\tlazy\tboy\tNeeraj\tperson\n",
    "D1\t1\t1\t2\t1\t0\t0\n",
    "D2\t0\t0\t1\t0\t1\t1\n",
    "\n",
    "Exp: the word vector for ‘lazy’ in the above matrix is [2,1] \n",
    "\n",
    "TF-IDF Vectorization :\n",
    "Example :  List the count of terms(tokens/words) in two documents.\n",
    "\n",
    "Document 1:\n",
    "Term     Count\n",
    "this      1\n",
    "is        1\n",
    "about     2\n",
    "Messi     4\n",
    "\n",
    "\n",
    "Document 2:\n",
    "Term     Count\n",
    "This      1\n",
    "is        2\n",
    "about     1\n",
    "tf-idf    1\n",
    "\n",
    "\n",
    "TF = (Number of times term t appears in a document)/(Number of terms in the Document)\n",
    "\n",
    "TF(This, Document 1)  = 1/8\n",
    "TF(This,Document 2) = 1/5\n",
    "\n",
    "Tf(about,Document 1) = 2/8\n",
    "TF(tf-idf,Doument 2) = 1/5\n",
    "\n",
    "IDF = log(N/n) where N is the number of documents and n is the number of doucments a term t has appeared in.\n",
    "\n",
    "where N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "\n",
    "IDF(This) = log(2/2) = 0.\n",
    "\n",
    "Let us compute IDF for the word ‘Messi’.\n",
    "\n",
    "IDF(Messi) = log(2/1) = 0.301.\n",
    "\n",
    "Now, let us compare the TF-IDF for a common word ‘This’ and a word ‘Messi’ which seems to be of relevance to Document 1.\n",
    "\n",
    "TF-IDF(This,Document1) = (1/8) * (0) = 0\n",
    "\n",
    "TF-IDF(This, Document2) = (1/5) * (0) = 0\n",
    "\n",
    "TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Tokenize Text into Sentences \n",
    "\n",
    "2. Tokenize Sentences into words(Tokens)\n",
    "    - MWE\n",
    "    - Reg Exp\n",
    "    - Line Tokenizer\n",
    "    - Treebank Tokenizer\n",
    "    - Tweet Tokenizer\n",
    "    - WordPunct Tokenizer\n",
    "    \n",
    "3. Remove the stop words\n",
    "\n",
    "4. Finding the Synsets(Synonyms) \n",
    "    - Defination\n",
    "    - Example\n",
    "    - Hypernyms\n",
    "    - Hyponyms\n",
    "    - Annotation\n",
    "    - Lemmas\n",
    "    \n",
    "5. Finding N Grams (Bi and tri Grams)\n",
    "\n",
    "6. Visualizing Sentence tokens using worldColud\n",
    "\n",
    "7. Stemming a tokens\n",
    "    - Porter Stemmer\n",
    "    - Lancester Stemmer\n",
    "    - Snowball Stemmer\n",
    "    \n",
    "9. Lemmatization for tokens\n",
    "\n",
    "10.Finding text Similary using Cosine and Jaccard Distance\n",
    "\n",
    "11. Finding Most common words in senetcnes\n",
    "\n",
    "12. Check whether 'Wh' words present in the sentence\n",
    "\n",
    "13. Calculateing BOW and IF-IDF for sentences\n",
    "\n",
    "14. Feature Endineering_CBOW\n",
    "\n",
    "15. Feature Endineering_SKIP GRAM\n",
    "\n",
    "16. Feature Endineering_OneHotEncoding\n",
    "\n",
    "17. Finding the diatance between two strings\n",
    "\n",
    "18. Contract Expansion\n",
    "\n",
    "19. Spell Check\n",
    "\n",
    "20. Text Cleaning_Remove Tags\n",
    "\n",
    "21. Text Cleaning_remove Special Characters\n",
    "\n",
    "22. Text Cleaning_remove Punctuations\n",
    "\n",
    "23. Text Cleaning_Remove Duplicate Words\n",
    "\n",
    "24. Text Cleaning_Remove Digits\n",
    "\n",
    "25. Text Cleaning_Convert to lower\n",
    "\n",
    "26. Text Cleaning_Correct Repeating Characters\n",
    "\n",
    "27. Text Cleaning_RemoveAccentedCharaters\n",
    "\n",
    "28. Reading PDF file from Python\n",
    "\n",
    "29. Reading word documents in Python\n",
    "\n",
    "30. Read Contents from an RSS feed\n",
    "\n",
    "31. Processing two short stories and extracting the common vocabulary between two of them\n",
    "\n",
    "32. Advanced NLP Task\n",
    "\n",
    "33. Conversion of Plural to Singular\n",
    "\n",
    "Removal of emojis\n",
    "\n",
    "Removal of emoticons\n",
    "\n",
    "Conversion of emoticons to words\n",
    "\n",
    "Conversion of emojis to words\n",
    "\n",
    "Removal of URLs\n",
    "\n",
    "Removal of HTML tags\n",
    "\n",
    "Chat words conversion\n",
    "\n",
    "Spelling correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Looking up Synsets for a word in Wordnet\n",
    "#1.2 Looking up lemmas and synonyms in Wordnet\n",
    "#1.3 Calaculating Synset Similarity \n",
    "#1.4 Discovering the word collocations\n",
    "#1.5 Bigram Collaction Finder\n",
    "#1.5 Trigram Collaction Finder\n",
    "\n",
    "#1.1 Tokenization text into sentence\n",
    "#1.2 Tokenization sentecne into words\n",
    "#1.3 Tokenization Sentences using Regular Expression\n",
    "#1.4 Trainig sentecne tokenizer\n",
    "#1.5 Stop word removal from the Tokenized sentences\n",
    "\n",
    "#1.1 Stemming words\n",
    "#1.2 Lemmatizing  words with WordNet\n",
    "#1.3 Replacing words matching regular expression\n",
    "#1.4 Removing Repeating Characters\n",
    "#1.5 Spelling Correction with Enchnat\n",
    "#1.6 Replacing synonyms\n",
    "#1.7 Replacing Negations and antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NLTK Library\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize import regexp_tokenize, \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import PunktWordTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import POSTagger\n",
    "from nltk.tag.sequential import RegexpTagger\n",
    "from nltk.tag.stanford import NERTagger\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "from nltk import CFG\n",
    "\n",
    "from nltk.chunk.regexp import *\n",
    "from catchunked import CategorizedChunkedCorpusReader\n",
    "from catchunked import CategorizedConllChunkCorpusReader\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "from nltk.corpus.reader import ChunkedCorpusReader\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "from nltk.corpus.reader import CategorizedCorpusReader\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from nltk.corpus.reader import ConllChunkCorpusReader\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# punctuation\n",
    "\n",
    "from string import punctuation\n",
    "print(list(punctuation))\n",
    "\n",
    "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', \n",
    " '^',  '_', '`', '{', '|', '}', '~']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
