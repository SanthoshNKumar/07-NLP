{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN                                       Noun,Singular\n",
    "# IN                                       Preposition / Subordinating Conjunction\n",
    "# CC                                       Coordinating Conjunction\n",
    "# PRP                                      Personal Pronoun\n",
    "# NNP                                      Proper Noun\n",
    "# MD                                       Modal\n",
    "# NNS                                      Noun Plural\n",
    "\n",
    "\n",
    "# NN                                       Noun,Singular\n",
    "# NNS                                      Noun Plural\n",
    "# NNP                                      Proper Noun,Singular\n",
    "# NNPs                                     Proper Noun,Plural\n",
    "# [('cat', 'NN'), ('cats', 'NNS')]\n",
    "# [('Santhosh', 'NNP')]\n",
    "\n",
    "# PRP                                      Personal Pronoun\n",
    "# PRP$                                     Possessive Pronoun\n",
    "# [('He', 'PRP'), ('she', 'PRP'), ('we', 'PRP'), ('him', 'PRP'), ('himself', 'PRP')]\n",
    "# [('our', 'PRP$'), ('his', 'PRP$'), ('her', 'PRP$'), ('mine', 'PRP$'), ('my', 'PRP$')]\n",
    "\n",
    "# JJ                                      adjective\n",
    "# JJR                                     adjective,comparative\n",
    "# JJS                                     adjective,superlative\n",
    "# [('good', 'JJ'), ('better', 'JJR'), ('best', 'JJS')]\n",
    "# [('bad', 'JJ'), ('worse', 'JJR'), ('worst', 'JJS')]\n",
    "\n",
    "# VB                                       VERB\n",
    "# VBD                                      VERB past tense\n",
    "# VBG                                      VERB gerund\n",
    "# VBN                                      VERB past participle\n",
    "# [('go', 'VB'), ('went', 'VBD'), ('going', 'VBG'), ('gone', 'VBN')]\n",
    "\n",
    "# RB                                       adverb\n",
    "# RBR                                      adverb,comparative\n",
    "# RBS                                      adverb,superlative\n",
    "# [('fast', 'RB'), ('faster', 'RBR'), ('fastest', 'RBS')]\n",
    "# [('late', 'RB'), ('later', 'RB'), ('latest', 'RBS')]\n",
    "\n",
    "# MD                                       Modal Verbs\n",
    "# [('can', 'MD'), ('may', 'MD'), ('must', 'MD'), ('shall', 'MD'), ('will', 'MD'), \n",
    "#  ('could', 'MD'), ('would', 'MD'), ('should', 'MD'), ('might', 'MD')]\n",
    "\n",
    "# DT                                     Determiner Words\n",
    "# [('this', 'DT'), ('that', 'IN'), ('a', 'DT'), ('an', 'DT'), ('those', 'DT'), \n",
    "#  ('these', 'DT'), ('the', 'DT'), ('some', 'DT')]\n",
    "\n",
    "# PDT                                   Predeterminer\n",
    "\n",
    "\n",
    "# IN                                     Preposition\n",
    "# [('at', 'IN'), ('above', 'RB'), ('after', 'IN'), ('across', 'IN'), ('onto', 'IN'), ('with', 'IN'), \n",
    "#  ('from', 'IN'), ('for', 'IN'), ('like', 'IN'), ('inside', 'NN'), ('during', 'IN'), ('beside', 'NN')]\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WP                                       Wh-pronoun\n",
    "# [('What', 'WP'), ('who', 'WP'), ('whom', 'WP'), ('which', 'WDT')]\n",
    "\n",
    "#WRB                                       wh-adverb\n",
    "# [('how', 'WRB'), ('when', 'WRB'), ('where', 'WRB'), ('why', 'WRB')]\n",
    "\n",
    "# A modal verb is a type of verb that is used to indicate modality â€“ \n",
    "# that is: likelihood, ability, permission, request, capacity, suggestions, order and obligation, advice,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('US', 'NNP'), ('unveils', 'VBZ'), ('word', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), ('.', '.'), ('beats', 'NNS'), ('China', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Words</th>\n",
       "      <td>US</td>\n",
       "      <td>unveils</td>\n",
       "      <td>word</td>\n",
       "      <td>'s</td>\n",
       "      <td>most</td>\n",
       "      <td>powerful</td>\n",
       "      <td>supercomputer</td>\n",
       "      <td>.</td>\n",
       "      <td>beats</td>\n",
       "      <td>China</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS tag</th>\n",
       "      <td>NNP</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>NN</td>\n",
       "      <td>POS</td>\n",
       "      <td>RBS</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>.</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NNP</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1     2    3     4         5              6  7      8   \\\n",
       "Words     US  unveils  word   's  most  powerful  supercomputer  .  beats   \n",
       "POS tag  NNP      VBZ    NN  POS   RBS        JJ             NN  .    NNS   \n",
       "\n",
       "            9  10  \n",
       "Words    China  .  \n",
       "POS tag    NNP  .  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"US unveils word's most powerful supercomputer. beats China.\"\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "pos_tag = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(pos_tag)\n",
    "\n",
    "pd.DataFrame(pos_tag,columns=['Words','POS tag']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "# get help\n",
    "nltk.help.upenn_tagset('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Post Tags\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Myself', 'PRP'), ('Santhosh', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Myself Santhosh\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('coming', 'VBG'), ('tomorrows', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "text =  \"I will be coming tomorrows\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('HE', 'NNP'), ('Spoke', 'VBD'), ('with', 'IN'), ('Harish', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"HE Spoke with Harish\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('go', 'VB'), ('went', 'VBD'), ('going', 'VBG'), ('gone', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"go went going gone\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('begin', 'NN'), ('began', 'VBD'), ('beginning', 'VBG'), ('begun', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"begin began beginning begun\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'WRB'), ('when', 'WRB'), ('where', 'WRB'), ('why', 'WRB')]\n"
     ]
    }
   ],
   "source": [
    "text = \"how when where why\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'WRB'), ('when', 'WRB'), ('where', 'WRB'), ('why', 'WRB')]\n"
     ]
    }
   ],
   "source": [
    "text = \"how when where why\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Already', 'RB'), ('ago', 'RB'), ('before', 'IN'), ('yet', 'RB'), ('never', 'RB'), ('yesterday', 'NN'), ('soon', 'RB'), ('lately', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Already ago before yet never yesterday soon lately\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('always', 'RB'), ('usually', 'RB'), ('normally', 'RB'), ('often', 'RB'), ('sometimes', 'VBZ'), ('occasionally', 'RB'), ('once', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "text = \"always usually normally often sometimes occasionally once\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('here', 'RB'), ('everywhere', 'RB'), ('upward', 'RB'), ('away', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "text = \"here everywhere upward away\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 'JJ'), ('better', 'JJR'), ('best', 'JJS')]\n",
      "[('bad', 'JJ'), ('worse', 'JJR'), ('worst', 'JJS')]\n",
      "[('little', 'RB'), ('less', 'JJR'), ('least', 'JJS')]\n",
      "[('far', 'RB'), ('further', 'RB'), ('furthest', 'JJS')]\n"
     ]
    }
   ],
   "source": [
    "text = \"good better best\".split()\n",
    "print(pos_tag(text))\n",
    "\n",
    "text = \"bad worse worst\".split()\n",
    "print(pos_tag(text))\n",
    "\n",
    "text = \"little less least\".split()\n",
    "print(pos_tag(text))\n",
    "\n",
    "text = \"far further furthest\".split()\n",
    "print(pos_tag(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'NN'), ('cats', 'NNS')]\n",
      "[('Santhosh', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"cat cats\".split()\n",
    "print(pos_tag(text))\n",
    "\n",
    "text = \"Santhosh\".split()\n",
    "print(pos_tag(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('she', 'PRP'), ('we', 'PRP'), ('him', 'PRP'), ('himself', 'PRP')]\n",
      "[('our', 'PRP$'), ('his', 'PRP$'), ('her', 'PRP$'), ('mine', 'NN'), ('my', 'PRP$')]\n"
     ]
    }
   ],
   "source": [
    "text = \"He she we him himself\".split()\n",
    "print(pos_tag(text))\n",
    "\n",
    "text = \"our his her mine my\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('late', 'RB'), ('later', 'RB'), ('latest', 'JJS')]\n"
     ]
    }
   ],
   "source": [
    "text = \"late later latest\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('can', 'MD'), ('may', 'MD'), ('must', 'MD'), ('shall', 'MD'), ('will', 'MD'), ('could', 'MD'), ('would', 'MD'), ('should', 'MD'), ('might', 'MD')]\n"
     ]
    }
   ],
   "source": [
    "text = \"can may must shall will could would should might\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'DT'), ('that', 'IN'), ('a', 'DT'), ('an', 'DT'), ('those', 'DT'), ('these', 'DT'), ('the', 'DT'), ('some', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "text = \"this that a an those these the some\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('at', 'IN'), ('above', 'RB'), ('after', 'IN'), ('across', 'IN'), ('onto', 'IN'), ('with', 'IN'), ('from', 'IN'), ('for', 'IN'), ('like', 'IN'), ('inside', 'NN'), ('during', 'IN'), ('beside', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"at above after across onto with from for like inside during beside\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('goobye', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = \"goobye\".split()\n",
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TV']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all words which POS Tags of 'NN' and 'NNP'\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "s = \"I was Watching TV\"\n",
    "\n",
    "pos_tags = nltk.pos_tag(word_tokenize(s))\n",
    "\n",
    "[x for (x,y) in pos_tags if y in ['NN','NNP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('watching', 'VBG'), ('TV', 'NN')]\n",
      "['TV']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all the Noun POS Tags words\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "s = 'I was watching TV'\n",
    "tagged = nltk.pos_tag(word_tokenize(s))\n",
    "print(tagged)\n",
    "\n",
    "allnoun = [word for word,pos in tagged if pos in ['NN','NNP']]\n",
    "print(allnoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Childern', 'NNP'),\n",
       " (\"should't\", 'NN'),\n",
       " ('dare', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('sugar', 'NN'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagging with NLTK\n",
    "text1 = \"Childern should't dare a sugar drink before bed.\"\n",
    "text13 = nltk.word_tokenize(text1)\n",
    "\n",
    "nltk.pos_tag(text13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " (\"should't\", 'VBZ'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagging with NLTK\n",
    "\n",
    "import nltk\n",
    "text11 = \"Children should't drink a sugary drink before bed.\"\n",
    "text13 = nltk.word_tokenize(text11)\n",
    "\n",
    "nltk.pos_tag(text13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Ulm', 'NNP'),\n",
       " (',', ','),\n",
       " ('Germany', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1879', 'CD')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS tags\n",
    "\n",
    "import nltk\n",
    "sent = \"Albert Einstein was born in Ulm,Germany in 1879\"\n",
    "nltk.pos_tag(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guru99', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('sites', 'NNS'), ('to', 'TO'), ('learn', 'VB'), ('web', 'NN'), (',', ','), ('sap', 'NN'), (',', ','), ('ethical', 'JJ'), ('hacking', 'NN'), ('and', 'CC'), ('much', 'RB'), ('more', 'JJR'), ('online', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'NN': 5,\n",
       "         'VBZ': 1,\n",
       "         'CD': 1,\n",
       "         'IN': 1,\n",
       "         'DT': 1,\n",
       "         'JJS': 1,\n",
       "         'NNS': 1,\n",
       "         'TO': 1,\n",
       "         'VB': 1,\n",
       "         ',': 2,\n",
       "         'JJ': 1,\n",
       "         'CC': 1,\n",
       "         'RB': 1,\n",
       "         'JJR': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "text = \" Guru99 is one of the best sites to learn WEB, SAP, Ethical Hacking and much more online.\"\n",
    "\n",
    "lower_case = text.lower()\n",
    "\n",
    "tokens = nltk.word_tokenize(lower_case)\n",
    "\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "\n",
    "Counter([y for (x,y) in tags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Tagger : Tag and Untag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries \n",
    "from nltk.tag import DefaultTagger \n",
    "  \n",
    "# Defining Tag \n",
    "tagging = DefaultTagger('NN') \n",
    "  \n",
    "# Tagging \n",
    "tagging.tag(['Hello', 'Geeks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('welcome', 'NN'), ('to', 'NN'), ('.', 'NN')],\n",
       " [('Geeks', 'NN'), ('for', 'NN'), ('Geeks', 'NN')]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagging Sentences\n",
    "\n",
    "# Loading Libraries \n",
    "from nltk.tag import DefaultTagger \n",
    "  \n",
    "# Defining Tag \n",
    "tagging = DefaultTagger('NN') \n",
    "  \n",
    "tagging.tag_sents([['welcome', 'to', '.'], ['Geeks', 'for', 'Geeks']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Geeks', 'for', 'Geeks']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# : Illustrating how to untag.\n",
    "\n",
    "from nltk.tag import untag \n",
    "untag([('Geeks', 'NN'), ('for', 'NN'), ('Geeks', 'NN')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mysore', 'NN'), ('is', 'NN'), ('an', 'NN'), ('amazing', 'NN'), ('place', 'NN'), ('on', 'NN'), ('the', 'NN'), ('earth.I', 'NN'), ('have', 'NN'), ('visited', 'NN'), ('more', 'NN'), ('than', 'NN'), ('10', 'NN'), ('times', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def learnDefaulttagger(simpleSentence):\n",
    "    wordsInSentences = nltk.word_tokenize(simpleSentence)\n",
    "    tagger = nltk.DefaultTagger(\"NN\")\n",
    "    posEnabledtags = tagger.tag(wordsInSentences)\n",
    "    print(posEnabledtags)\n",
    "\n",
    "\n",
    "learnDefaulttagger(\"Mysore is an amazing place on the earth.I have visited more than 10 times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  unveils/JJ\n",
      "  (NP world/NN 's/POS most/RBS)\n",
      "  powerful/JJ\n",
      "  (NP supercomputer/NN ,/,)\n",
      "  beats/VBZ\n",
      "  (NP China/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "import nltk\n",
    "from nltk import RegexpParser\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer,beats China.\"\n",
    "tagged_pos = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(\"POS Tags:\",tagged_pos)\n",
    "\n",
    "chink_grammer = \"\"\" NP :  {<.*>+}\n",
    "                            }<VBZ|VBD|JJ|IN>+{ \n",
    "                            \"\"\"\n",
    "\n",
    "rc = RegexpParser(chink_grammer)\n",
    "c = rc.parse(tagged_pos)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  (mychunk US/NNP)\n",
      "  (mychunk unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  (mychunk powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (mychunk China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "import nltk\n",
    "from nltk import RegexpParser\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer,beats China.\"\n",
    "tagged_pos = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(\"POS Tags:\",tagged_pos)\n",
    "\n",
    "chunk_grammer = \"\"\"mychunk:{<DT>?<JJ>*<NN.*>}\"\"\"\n",
    "\n",
    "rc = RegexpParser(chunk_grammer)\n",
    "c = rc.parse(tagged_pos)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"PRP$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"IN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"VBZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"JJ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"WP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset(\"CC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
