{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'is',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'is',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Geeks', 'for', 'Geeks'], ['is'], ['best', 'computer', 'science', 'portal']]\n",
      "[['Geeks', 'for', 'Geeks'], ['is'], ['best', 'computer', 'science', 'portal']]\n"
     ]
    }
   ],
   "source": [
    "test_list = ['Geeks for Geeks', 'is', 'best computer science portal']\n",
    "\n",
    "print([x.split() for x in test_list])\n",
    "\n",
    "print([y for y in [x.split() for x in test_list]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenize : Split Paragraph or Corpus into list of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lotter.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"God is Great! I won a lotter.\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lotter', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"God is Great! I won a lotter.\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE Tokenizer : Multi-Word Expression\n",
    "##### Here Certain group of multiple words are treated as one entity during tokenization, such as 'United States of America' ,'People's replublic of China', 'not only' 'but also' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']\n",
      "['I', 'Live', 'in', 'United_States_of_America']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "\n",
    "print(tokenizer.tokenize(word_tokenize('In a little or a little bit or a lot in spite of')))\n",
    "\n",
    "tokeniser1 = MWETokenizer([('United','States','of','America')])\n",
    "\n",
    "print(tokeniser1.tokenize(word_tokenize(\"I Live in United States of America\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RegexpTokenizer \n",
    "#### Extract the tokens from string by using regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'see', 'how', \"it's\", 'working']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
    "\n",
    "text = \"Let's see how it's working.\"\n",
    "tokenizer.tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'Python']\n"
     ]
    }
   ],
   "source": [
    "# import RegexpTokenizer() method from nltk \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "    \n",
    "# Create a reference variable for Class RegexpTokenizer \n",
    "tk = RegexpTokenizer('\\s+', gaps = True) \n",
    "    \n",
    "# Create a string input \n",
    "gfg = \"I love Python\"\n",
    "    \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "    \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Whitespace Tokenizer:\n",
    "#### This Tokenizer splits a string whenever a space, tab, or newline character is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '2',\n",
       " 'QUICK',\n",
       " 'Brown-Foxes',\n",
       " 'jumped',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " \"dog's\",\n",
       " 'bone.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WhitespaceTokenizer : we are able to extract the tokens from string of words or sentences without whitespaces, \n",
    "#                       new line and tabs\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "sentence =  \"The 2 QUICK Brown-Foxes\\tjumped over\\nthe lazy dog's\\n\\nbone.\"\n",
    "\n",
    "WhitespaceTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GeeksforGeeks', 'is', 'for', 'geeks']\n"
     ]
    }
   ],
   "source": [
    "# import WhitespaceTokenizer() method from nltk \n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer \n",
    "tk = WhitespaceTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"GeeksforGeeks \\nis\\t for geeks\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer\n",
    "\n",
    "##### This is specifically designed for tokenizing tweets.  it is capable of dealing with emotions and expressions od sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "sentence = \"This is a cooool #dummysmiley:-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunkt Tokenizer\n",
    "\n",
    "##### This splits a text into a list of alphabetical characters, digits, and non - aphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.', '12']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPunctTokenizer:Extract the tokens from string of words or sentences in the form of Alphabetic and Non-Alphabetic character\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenize = WordPunctTokenizer()\n",
    "tokenize.tokenize(\"Let's see how it's working. 12\")\n",
    "\n",
    "# Here observe above It separates the punctuations from the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'price', 'of', 'burger', 'in', 'BurgerKing', 'is', 'Rs', '.', '36', '.']\n"
     ]
    }
   ],
   "source": [
    "# import WordPunctTokenizer() method from nltk \n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "     \n",
    "# Create a reference variable for Class WordPunctTokenizer \n",
    "tk = WordPunctTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"The price\\t of burger \\nin BurgerKing is Rs.36.\\n\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LineTokenizer : Tokenize based on the '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is maximum Decimus Meridius. Commander of the Armier of the North, General of the Felix Legions         and loyal servant to the true emperor, Mercus Aurelius. ', 'Father to a murdered son. husband to a murdered         wife. ', 'And I will have my vengeance, in this']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# LineTokenizer : Tokenize based on the '\\n'\n",
    "\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "text = \"My name is maximum Decimus Meridius. Commander of the Armier of the North, General of the Felix Legions \\\n",
    "        and loyal servant to the true emperor, Mercus Aurelius. \\nFather to a murdered son. husband to a murdered \\\n",
    "        wife. \\nAnd I will have my vengeance, in this\"\n",
    "\n",
    "print(LineTokenizer().tokenize(text))\n",
    "print(len(LineTokenizer().tokenize(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Tokenizer : Tokenize based on the 'space'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'maximum', 'Decimus', 'Meridius.', 'Commander', 'of', 'the', 'Armier', 'of', 'the', 'North,', 'General', 'of', 'the', 'Felix', 'Legions', '', '', '', '', '', '', '', '', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor,', 'Mercus', 'Aurelius.', '\\nFather', 'to', 'a', 'murdered', 'son.', 'husband', 'to', 'a', 'murdered', '', '', '', '', '', '', '', '', 'wife.', '\\nAnd', 'I', 'will', 'have', 'my', 'vengeance,', 'in', 'this']\n"
     ]
    }
   ],
   "source": [
    "# SpaceTokenizer : we are able to extract the tokens from string of words on the basis of space between them\n",
    "\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "text = \"My name is maximum Decimus Meridius. Commander of the Armier of the North, General of the Felix Legions \\\n",
    "        and loyal servant to the true emperor, Mercus Aurelius. \\nFather to a murdered son. husband to a murdered \\\n",
    "        wife. \\nAnd I will have my vengeance, in this\"\n",
    "\n",
    "print(SpaceTokenizer().tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geeksfor', 'Geeks..', '.$$&*', '\\nis\\t', 'for', 'geeks']\n"
     ]
    }
   ],
   "source": [
    "# import SpaceTokenizer() method from nltk \n",
    "from nltk.tokenize import SpaceTokenizer \n",
    "     \n",
    "# Create a reference variable for Class SpaceTokenizer \n",
    "tk = SpaceTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"Geeksfor Geeks.. .$$&* \\nis\\t for geeks\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SExprTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['( a * ( b + c ))', 'ab', '( a-c )']\n"
     ]
    }
   ],
   "source": [
    "# SExprTokenizer : It actually looking for proper brackets to make tokens.\n",
    "\n",
    "# import SExprTokenizer() method from nltk \n",
    "from nltk.tokenize import SExprTokenizer \n",
    "     \n",
    "# Create a reference variable for Class SExprTokenizer \n",
    "tk = SExprTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"( a * ( b + c ))ab( a-c )\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabTokenizer : \n",
    "##### Extract the tokens from string of words on the basis of tabs between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geeksfor', 'Geeks..', '.$$&* \\nis', ' for geeks']\n"
     ]
    }
   ],
   "source": [
    "# import TabTokenizer() method from nltk \n",
    "from nltk.tokenize import TabTokenizer \n",
    "     \n",
    "# Create a reference variable for Class TabTokenizer \n",
    "tk = TabTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"Geeksfor\\tGeeks..\\t.$$&* \\nis\\t for geeks\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
