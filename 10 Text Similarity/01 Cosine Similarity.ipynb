{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Smaller the angle, the higher the similarity” — Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Cosine Similarity between Two Sentences\n",
    "\n",
    "# Step 1: Find the Counter Vectorizer the document\n",
    "# Step 2: find cosine_similarity of two vectors from the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Julie loves John more than Linda loves John',\n",
       " 'Jane loves John more than Julie loves John']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"Julie loves John more than Linda loves John\"\n",
    "doc2 = \"Jane loves John more than Julie loves John\"\n",
    "\n",
    "doc = [doc1,doc2]\n",
    "\n",
    "vocab = []\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'julie': 2, 'loves': 4, 'john': 1, 'more': 5, 'than': 6, 'linda': 3, 'jane': 0}\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(doc)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1, 1, 2, 1, 1],\n",
       "       [1, 2, 1, 0, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = vectorizer.transform(doc)\n",
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1: [0 2 1 1 2 1 1]\n",
      "d2: [1 2 1 0 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "d1 = vector.toarray()[0,:]\n",
    "\n",
    "d2 = vector.toarray()[1,:]\n",
    "\n",
    "print(\"d1:\",d1)\n",
    "print(\"d2:\",d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d1.reshape(1,-1)\n",
    "d2 = d2.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1, 1, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91666667]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine angle (the smaller the angle) between the two vectors' value is 0.9166 which is nearest to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-f9b8f6537f8a>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-f9b8f6537f8a>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    cosine_similaritylist(vec1),list(vec2))\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Example 2: using Count Vectorization\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "print(\"Step 1: Get the coupus Data\")\n",
    "data = ['Julie loves me more than Linda loves me','Jane likes me more than Julie loves me']\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 2: Build the vector matrix using counter vectorizers\")\n",
    "vocabulary=vectorizer.fit(data)\n",
    "X= vectorizer.transform(data)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vocabulary.get_feature_names())\n",
    "\n",
    "vec1 = X.toarray()[0,:]\n",
    "print(\"vec1={0}\".format(vec1))\n",
    "\n",
    "vec2 = X.toarray()[1,:]\n",
    "print(\"vec2={0}\".format(vec2))\n",
    "\n",
    "vec1 = vec1.reshape(1,-1)\n",
    "vec2 = vec2.reshape(1,-1)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 3: Find Similarity between sentences using cosine_similarity\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similaritylist(vec1),list(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Get the corpus data\n",
      "Corpus : ['What you do defines you', 'Yours deeds defines you', 'Once upon a time there lived a king.', 'Who is your queen?', 'He is desperate', 'Is he not desperate?']\n",
      "\n",
      "\n",
      "Step 2: Build the Vector matrix using TFSIDF\n",
      "[[0.         0.3541223  0.         0.43184893 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.43184893 0.         0.7082446  0.\n",
      "  0.        ]]\n",
      "\n",
      "\n",
      "[[0.54677906 0.44836665 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44836665 0.\n",
      "  0.54677906]]\n",
      "\n",
      "\n",
      "Step 3:Find the Cosine Similarity between sentences\n",
      "Similarity Between Sentence1 and Sentence2: [[0.47632989]]\n",
      "Similarity Between Sentence3 and Sentence4: [[0.]]\n",
      "Similarity Between Sentence5 and Sentence6: [[0.80368547]]\n"
     ]
    }
   ],
   "source": [
    "# Example 3: using TFIDF Vectorization\n",
    "\n",
    "pair1 = [\"What you do defines you\",\n",
    "         \"Yours deeds defines you\"]\n",
    "\n",
    "pair2 = [\"Once upon a time there lived a king.\",\n",
    "         \"Who is your queen?\"]\n",
    "\n",
    "pair3 = [\"He is desperate\",\n",
    "         \"Is he not desperate?\"]\n",
    "\n",
    "print(\"Step 1: Get the corpus data\")\n",
    "corpus = [pair1[0],pair1[1],pair2[0],pair2[1],pair3[0],pair3[1]]\n",
    "\n",
    "print(\"Corpus :\",corpus)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 2: Build the Vector matrix using TFSIDF\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_results = tfidf_model.fit_transform(corpus).todense()\n",
    "\n",
    "#print(tfidf_results.shape)\n",
    "\n",
    "print(tfidf_results[0])\n",
    "print(\"\\n\")\n",
    "print(tfidf_results[1])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 3:Find the Cosine Similarity between sentences\")\n",
    "print(\"Similarity Between Sentence1 and Sentence2:\",cosine_similarity(tfidf_results[0],tfidf_results[1]))\n",
    "print(\"Similarity Between Sentence3 and Sentence4:\",cosine_similarity(tfidf_results[2],tfidf_results[3]))\n",
    "print(\"Similarity Between Sentence5 and Sentence6:\",cosine_similarity(tfidf_results[4],tfidf_results[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 : Get the sentences\n",
      "Sent1: Hi how are you\n",
      "Sent2: Hi i am fine\n",
      "\n",
      "\n",
      "Step2 : Tokenize the sentences\n",
      "tokenized_sent1: ['Hi', 'how', 'are', 'you']\n",
      "tokenized_sent2: ['Hi', 'i', 'am', 'fine']\n",
      "\n",
      "\n",
      "Step3 : Get all the words\n",
      "All words: ['w', 'r', 'y', 'H', 'f', 'u', 'h', ' ', 'i', 'n', 'e', 'a', 'o', 'm']\n",
      "\n",
      "\n",
      "Step 4: Transofrm docs to Vector\n",
      "vector1: [1, 1, 1, 1, 0, 1, 1, 3, 1, 0, 1, 1, 2, 0]\n",
      "vector2: [0, 0, 0, 1, 1, 0, 0, 3, 3, 1, 1, 1, 0, 1]\n",
      "\n",
      "\n",
      "Step 5: Find the Cosine Similarity\n",
      "Cosine Distance Between two vectors: 0.34720879016613315\n"
     ]
    }
   ],
   "source": [
    "# Example 3:\n",
    "\n",
    "import nltk \n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "print(\"Step1 :\",\"Get the sentences\")\n",
    "sent1 = \"Hi how are you\"\n",
    "sent2 = \"Hi i am fine\"\n",
    "print(\"Sent1:\",sent1)\n",
    "print(\"Sent2:\",sent2)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Step2 :\",\"Tokenize the sentences\")\n",
    "\n",
    "tokenized_sent1 = nltk.tokenize.word_tokenize(sent1)\n",
    "tokenized_sent2 = nltk.tokenize.word_tokenize(sent2)\n",
    "\n",
    "print(\"tokenized_sent1:\",tokenized_sent1)\n",
    "print(\"tokenized_sent2:\",tokenized_sent2)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step3 :\",\"Get all the words\")\n",
    "# List all the words\n",
    "all_words = list(set(sent1 + sent2))\n",
    "print(\"All words:\",all_words)\n",
    "\n",
    "# Create Vector\n",
    "vector1 = [0] * len(all_words)\n",
    "vector2 = [0] * len(all_words)\n",
    "\n",
    "# Get the word Frequency\n",
    "for w in sent1:\n",
    "    vector1[all_words.index(w)] +=1\n",
    "    \n",
    "for w in sent2:\n",
    "    vector2[all_words.index(w)] +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Step 4: Transofrm docs to Vector\")\n",
    "print(\"vector1:\",vector1)\n",
    "print(\"vector2:\",vector2)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 5: Find the Cosine Similarity\")\n",
    "cosine_distance(vector1,vector2)\n",
    "print(\"Cosine Distance Between two vectors:\",cosine_distance(vector1,vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 1: Build Document vetcor\n",
      "['Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin', 'President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election', 'Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career']\n",
      "\n",
      "\n",
      "Step 2 : Build vectorizer using countvectorizer\n",
      "[[1 0 1 0 0 0 0 0 1 0 0 2 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 2 0 1 1 0 0 0\n",
      "  1 1 2 1 0 2 0 0 0 1 0 1]\n",
      " [0 0 0 1 0 1 1 0 2 0 1 0 2 2 0 0 1 2 1 0 0 0 1 1 0 1 1 2 0 2 0 2 0 0 2 0\n",
      "  0 0 2 0 1 1 0 1 1 0 1 1]\n",
      " [0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 2 1 2 0 1 0 1\n",
      "  0 0 1 0 0 0 1 0 0 0 0 0]]\n",
      "\n",
      "\n",
      "Step 3 : Build Data frame for counter vectorizer for the doucments\n",
      "              after  as  became  by  career  claimed  do  earlier  election  \\\n",
      "doc_trump         1   0       1   0       0        0   0        0         1   \n",
      "doc_election      0   0       0   1       0        1   1        0         2   \n",
      "doc_putin         0   1       1   0       1        0   0        1         0   \n",
      "\n",
      "              elections  ...  the  though  to  trump  vladimir  was  who  \\\n",
      "doc_trump             0  ...    2       1   0      2         0    0    0   \n",
      "doc_election          0  ...    2       0   1      1         0    1    1   \n",
      "doc_putin             1  ...    1       0   0      0         1    0    0   \n",
      "\n",
      "              winning  witchhunt  with  \n",
      "doc_trump           1          0     1  \n",
      "doc_election        0          1     1  \n",
      "doc_putin           0          0     0  \n",
      "\n",
      "[3 rows x 48 columns]\n",
      "\n",
      "\n",
      "Step 4 : Find the cosine Similarity Matrix for the data frame\n",
      "[[1.         0.51480485 0.38890873]\n",
      " [0.51480485 1.         0.38829014]\n",
      " [0.38890873 0.38829014 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Cosine Similariry for the Dataframe\n",
    "\n",
    "# Define the documents\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
    "\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
    "\n",
    "documents = [doc_trump, doc_election, doc_putin]\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Step 1: Build Document vetcor\")\n",
    "print(documents)\n",
    "\n",
    "# To compute the cosine similarity, you need the word count of the words in each document. \n",
    "# The CountVectorizer or the TfidfVectorizer from scikit learn lets us compute this. \n",
    "# The output of this comes as a sparse_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 2 : Build vectorizer using countvectorizer\")\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "\n",
    "print(doc_term_matrix)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 3 : Build Data frame for counter vectorizer for the doucments\")\n",
    "df = pd.DataFrame(doc_term_matrix, columns=count_vectorizer.get_feature_names(),index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Step 4 : Find the cosine Similarity Matrix for the data frame\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Read the corpus Data\n",
      "In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\n",
      "\n",
      "\n",
      "Step 2: Sentence Tokenize the Corpus\n",
      "['In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills.', 'Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services.', 'As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses.', 'The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset.', 'This will require more collaborations and training and working with AI.', 'That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies.', 'The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\"', 'The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry.', 'Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public.', 'The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well.', 'This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.']\n",
      "\n",
      "\n",
      "Step 3: Build similarity matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent1</th>\n",
       "      <th>Sent2</th>\n",
       "      <th>Sent3</th>\n",
       "      <th>Sent4</th>\n",
       "      <th>Sent5</th>\n",
       "      <th>Sent6</th>\n",
       "      <th>Sent7</th>\n",
       "      <th>Sent8</th>\n",
       "      <th>Sent9</th>\n",
       "      <th>Sent10</th>\n",
       "      <th>Sent11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Sent1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319438</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.129099</td>\n",
       "      <td>0.239046</td>\n",
       "      <td>0.279508</td>\n",
       "      <td>0.230940</td>\n",
       "      <td>0.134164</td>\n",
       "      <td>0.167705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent2</td>\n",
       "      <td>0.319438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.336199</td>\n",
       "      <td>0.557657</td>\n",
       "      <td>0.174964</td>\n",
       "      <td>0.247436</td>\n",
       "      <td>0.076360</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.295084</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent3</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.336199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.229668</td>\n",
       "      <td>0.160128</td>\n",
       "      <td>0.113228</td>\n",
       "      <td>0.209657</td>\n",
       "      <td>0.196116</td>\n",
       "      <td>0.253185</td>\n",
       "      <td>0.156893</td>\n",
       "      <td>0.294174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent4</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.557657</td>\n",
       "      <td>0.229668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199205</td>\n",
       "      <td>0.112687</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.073193</td>\n",
       "      <td>0.352767</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>0.170783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent5</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.174964</td>\n",
       "      <td>0.160128</td>\n",
       "      <td>0.199205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.210819</td>\n",
       "      <td>0.163299</td>\n",
       "      <td>0.306186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent6</td>\n",
       "      <td>0.129099</td>\n",
       "      <td>0.247436</td>\n",
       "      <td>0.113228</td>\n",
       "      <td>0.112687</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077152</td>\n",
       "      <td>0.144338</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.115470</td>\n",
       "      <td>0.216506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent7</td>\n",
       "      <td>0.239046</td>\n",
       "      <td>0.076360</td>\n",
       "      <td>0.209657</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>0.077152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200446</td>\n",
       "      <td>0.138013</td>\n",
       "      <td>0.106904</td>\n",
       "      <td>0.200446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent8</td>\n",
       "      <td>0.279508</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.196116</td>\n",
       "      <td>0.073193</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.144338</td>\n",
       "      <td>0.200446</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129099</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent9</td>\n",
       "      <td>0.230940</td>\n",
       "      <td>0.295084</td>\n",
       "      <td>0.253185</td>\n",
       "      <td>0.352767</td>\n",
       "      <td>0.210819</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.138013</td>\n",
       "      <td>0.129099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154919</td>\n",
       "      <td>0.258199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent10</td>\n",
       "      <td>0.134164</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.156893</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>0.163299</td>\n",
       "      <td>0.115470</td>\n",
       "      <td>0.106904</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.154919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sent11</td>\n",
       "      <td>0.167705</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.294174</td>\n",
       "      <td>0.170783</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.216506</td>\n",
       "      <td>0.200446</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Sent1     Sent2     Sent3     Sent4     Sent5     Sent6     Sent7  \\\n",
       "Sent1   1.000000  0.319438  0.175412  0.218218  0.091287  0.129099  0.239046   \n",
       "Sent2   0.319438  1.000000  0.336199  0.557657  0.174964  0.247436  0.076360   \n",
       "Sent3   0.175412  0.336199  1.000000  0.229668  0.160128  0.113228  0.209657   \n",
       "Sent4   0.218218  0.557657  0.229668  1.000000  0.199205  0.112687  0.052164   \n",
       "Sent5   0.091287  0.174964  0.160128  0.199205  1.000000  0.235702  0.109109   \n",
       "Sent6   0.129099  0.247436  0.113228  0.112687  0.235702  1.000000  0.077152   \n",
       "Sent7   0.239046  0.076360  0.209657  0.052164  0.109109  0.077152  1.000000   \n",
       "Sent8   0.279508  0.178571  0.196116  0.073193  0.102062  0.144338  0.200446   \n",
       "Sent9   0.230940  0.295084  0.253185  0.352767  0.210819  0.149071  0.138013   \n",
       "Sent10  0.134164  0.114286  0.156893  0.097590  0.163299  0.115470  0.106904   \n",
       "Sent11  0.167705  0.214286  0.294174  0.170783  0.306186  0.216506  0.200446   \n",
       "\n",
       "           Sent8     Sent9    Sent10    Sent11  \n",
       "Sent1   0.279508  0.230940  0.134164  0.167705  \n",
       "Sent2   0.178571  0.295084  0.114286  0.214286  \n",
       "Sent3   0.196116  0.253185  0.156893  0.294174  \n",
       "Sent4   0.073193  0.352767  0.097590  0.170783  \n",
       "Sent5   0.102062  0.210819  0.163299  0.306186  \n",
       "Sent6   0.144338  0.149071  0.115470  0.216506  \n",
       "Sent7   0.200446  0.138013  0.106904  0.200446  \n",
       "Sent8   1.000000  0.129099  0.200000  0.250000  \n",
       "Sent9   0.129099  1.000000  0.154919  0.258199  \n",
       "Sent10  0.200000  0.154919  1.000000  0.300000  \n",
       "Sent11  0.250000  0.258199  0.300000  1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build sentence_Similarity Matrix\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "file = open(\"C:\\MyWork\\MyLearning\\Career Growth\\ML\\Files\\DataSet\\SampleText01.txt\",'r')\n",
    "\n",
    "text = file.read()\n",
    "\n",
    "print(\"Step 1: Read the corpus Data\")\n",
    "print(text)\n",
    "\n",
    "text = text.replace(\"[^a-zA-z]\",'')\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Step 2: Sentence Tokenize the Corpus\")\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "def sentence_similarity(sent1,sent2):\n",
    "    sent1 = word_tokenize(sent1.lower())\n",
    "    sent2 = word_tokenize(sent2.lower())\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    for w in sent1:\n",
    "        if w in stopword:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] +=1\n",
    "    \n",
    "    for w in sent2:\n",
    "        if w in stopword:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] +=1\n",
    "        \n",
    "    return  1 - cosine_distance(vector1,vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            #if idx1 == idx2:\n",
    "                #continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "            \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "print(\"Step 3: Build similarity matrix\")\n",
    "\n",
    "pd.DataFrame(build_similarity_matrix(sentences),columns=['Sent1','Sent2','Sent3','Sent4','Sent5','Sent6','Sent7','Sent8','Sent9','Sent10','Sent11'],\n",
    "              index=['Sent1','Sent2','Sent3','Sent4','Sent5','Sent6','Sent7','Sent8','Sent9','Sent10','Sent11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: Get the docoument data\n",
      "\n",
      "Step2: Get the couspus data\n",
      "['What you do defines you', 'Yours deeds defines you', 'Once upon a time there lived a king.', 'Who is your queen?', 'He is desperate', 'Is he not desperate?']\n",
      "\n",
      "Step3: Build the Vector matrix using TfidfVectorizer\n",
      "[[0.         0.3541223  0.         0.43184893 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.43184893 0.         0.7082446  0.\n",
      "  0.        ]\n",
      " [0.54677906 0.44836665 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44836665 0.\n",
      "  0.54677906]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.40824829 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.37115593\n",
      "  0.         0.         0.         0.         0.53611046 0.\n",
      "  0.         0.         0.         0.53611046 0.         0.53611046\n",
      "  0.        ]\n",
      " [0.         0.         0.60714432 0.         0.60714432 0.51259296\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.48795307 0.         0.48795307 0.41196351\n",
      "  0.         0.         0.59505434 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "\n",
      "Step 4: Find Senetnce Similarity Using Cosine Similarity\n",
      "[[0.47632989]]\n",
      "[[0.]]\n",
      "[[0.80368547]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Step1: Get the docoument data\")\n",
    "pair1 = [\"What you do defines you\",\"Yours deeds defines you\"]\n",
    "pair2 = [\"Once upon a time there lived a king.\",\"Who is your queen?\"]\n",
    "pair3 = [\"He is desperate\",\"Is he not desperate?\"]\n",
    "\n",
    "print(\"\\nStep2: Get the couspus data\")\n",
    "corpus = [pair1[0],pair1[1],pair2[0],pair2[1],pair3[0],pair3[1]]\n",
    "print(corpus)\n",
    "\n",
    "print(\"\\nStep3: Build the Vector matrix using TfidfVectorizer\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_results = tfidf_model.fit_transform(corpus).todense()\n",
    "print(tfidf_results)\n",
    "\n",
    "print(\"\\nStep 4: Find Senetnce Similarity Using Cosine Similarity\")\n",
    "print(cosine_similarity(tfidf_results[0],tfidf_results[1]))\n",
    "print(cosine_similarity(tfidf_results[2],tfidf_results[3]))\n",
    "print(cosine_similarity(tfidf_results[4],tfidf_results[5]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
