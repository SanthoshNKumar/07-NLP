{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:\\MyWork\\MyLearning\\Career Growth\\ML\\Files\\DataSet\\Reviews.csv\",encoding=\"latin-1\")[:35173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35173, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        0\n",
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               1\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Score                     0\n",
       "Time                      0\n",
       "Summary                   1\n",
       "Text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Summary','Text']]\n",
    "\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_tags(text):\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',text,flags=re.MULTILINE)\n",
    "    \n",
    "    text = re.sub(r'<br />',' ',text)\n",
    "    \n",
    "    text = re.sub(r'<a href',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['Text'][25]\n",
    "\n",
    "s = remove_tags(data['Text'][25])\n",
    "\n",
    "def remove_Single_Length_Char(text):\n",
    "    return ' '.join([y for y in [x for x in text.split()] if len(y)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary    0\n",
       "Text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"It's\": \"It is\",\n",
    "\"It'll\" : \"It will\",\n",
    "\"Im\" : \"I am\",\n",
    "\"It`s\":\"It is\"\n",
    "}\n",
    "\n",
    "\n",
    "# Contractions conversions\n",
    "def expand_text(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    new_text = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in contractions:\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return ' '.join(new_text)\n",
    "\n",
    "def replace_Special_Characters(text):\n",
    "    \n",
    "    clean_word = []\n",
    "    for c in text:\n",
    "        if c in string.punctuation:\n",
    "            clean_word.append(\" \")\n",
    "        else:\n",
    "            clean_word.append(c)\n",
    "            \n",
    "    return ''.join([char for char in clean_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the text\n",
    "data['Cleaned_Summary'] = data['Summary'].str.lower()\n",
    "\n",
    "# contractions expension\n",
    "data['Cleaned_Summary'] = data['Cleaned_Summary'].apply(lambda y :expand_text(y))\n",
    "\n",
    "data['Cleaned_Summary'] = data['Cleaned_Summary'].apply(lambda x : remove_tags(x))\n",
    "\n",
    "# remove the Punctioans\n",
    "#data['Cleaned_Summary'] = data['Cleaned_Summary'].apply(lambda y : ''.join([x for x in y if x not in string.punctuation]))\n",
    "data['Cleaned_Summary'] = data['Cleaned_Summary'].apply(lambda text : ' '.join([replace_Special_Characters(y) for y in [x for x in text.split()]]))\n",
    "\n",
    "# Remove stop words : not removing stop words from summary\n",
    "#data['Cleaned_Summary'] = data['Cleaned_Summary'].apply(lambda y :' '.join([x for x in y.split() if x not in stopwords]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>not as advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>delight  says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "         Cleaned_Summary  \n",
       "0  good quality dog food  \n",
       "1      not as advertised  \n",
       "2   delight  says it all  \n",
       "3         cough medicine  \n",
       "4            great taffy  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the text\n",
    "data['Cleaned_Text'] = data['Text'].str.lower()\n",
    "\n",
    "# contractions expension\n",
    "data['Cleaned_Text'] = data['Cleaned_Text'].apply(lambda y :expand_text(y))\n",
    "\n",
    "# Remove tags\n",
    "data['Cleaned_Text'] = data['Cleaned_Text'].apply(lambda x : remove_tags(x))\n",
    "\n",
    "data['Cleaned_Text'] = data['Cleaned_Text'].apply(lambda x : remove_Single_Length_Char(x))\n",
    "\n",
    "# remove the Punctioans\n",
    "# data['Cleaned_Text'] = data['Cleaned_Text'].apply(lambda y : ''.join([x for x in y if x not in string.punctuation]))\n",
    "data['Cleaned_Text'] = data['Cleaned_Text'].apply(lambda text : ' '.join([replace_Special_Characters(y) for y in [x for x in text.split()]]))\n",
    "\n",
    "# Remove stop words\n",
    "data['Cleaned_Text'] = data['Cleaned_Text'].apply(lambda y : ' '.join([x for x in y.split() if x not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pork rinds cooked fairly well microwave smell less appetizing taste fair best'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Cleaned_Text'][13861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The pork rinds cooked up fairly well in the microwave, but the smell was less than appetizing and the taste was fair at best.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Text'].str.contains(\"less than appetizing\")]['Text'][13861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product received advertised\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'twizzlers   strawberry'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['Cleaned_Text'][25])\n",
    "data['Cleaned_Summary'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summary: good quality dog food\n",
      "\n",
      "\n",
      "Text: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summary: not as advertised\n",
      "\n",
      "\n",
      "Text: confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n",
      "Summary:  delight  says it all\n",
      "\n",
      "\n",
      "Text: looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n",
      "Summary: cough medicine\n",
      "\n",
      "\n",
      "Text: great taffy great price wide assortment yummy taffy delivery quick taffy lover deal\n",
      "Summary: great taffy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Text:\",data['Cleaned_Text'][i])\n",
    "    print(\"Summary:\",data['Cleaned_Summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1504746"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the words from Cleaned Summary Column\n",
    "words_Cleaned_Summary =  [k for v in [x.split() for x in data['Cleaned_Summary']] for k in v]\n",
    "\n",
    "# get the words from Cleaned Text Column\n",
    "words_Cleaned_Text = [k for v in [x.split() for x in data['Cleaned_Text']] for k in v]\n",
    "\n",
    "words_Summary_text = words_Cleaned_Summary + words_Cleaned_Text\n",
    "\n",
    "len(words_Summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1357025"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove single letter word\n",
    "\n",
    "words_Summary_text = [x for x in words_Summary_text if len(x)>1]\n",
    "\n",
    "len(words_Cleaned_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(words_Cleaned_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32593"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count) # Unique words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddding_index = {}\n",
    "\n",
    "with open(r\"C:\\MyWork\\MyLearning\\Career Growth\\ML\\Files\\DataSet\\numberbatch-en.txt\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.array(values[1:],dtype='float32')\n",
    "        embeddding_index[word] = embedding        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417195"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 300 Dimensiovak Vector\n",
    "embeddding_index['on'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0563,  0.0019, -0.0236,  0.0054, -0.1098,  0.0523,  0.0244,\n",
       "       -0.1334, -0.1026,  0.0137, -0.0695, -0.0527,  0.0411,  0.0305,\n",
       "        0.0143,  0.0594,  0.0931,  0.0852, -0.0775,  0.0602, -0.0615,\n",
       "       -0.0691,  0.0087,  0.041 ,  0.1409, -0.0268, -0.0486, -0.0208,\n",
       "        0.0779,  0.0518,  0.0058, -0.094 , -0.0068, -0.0632,  0.1071,\n",
       "        0.0857,  0.0055, -0.0575, -0.0297,  0.1046,  0.0281, -0.0749,\n",
       "        0.0467, -0.112 ,  0.0675,  0.1291,  0.0431,  0.1141,  0.047 ,\n",
       "        0.0385, -0.0938, -0.0408, -0.0124,  0.0019, -0.1012,  0.0769,\n",
       "       -0.0098,  0.084 ,  0.0915,  0.0054, -0.1028, -0.0246, -0.0271,\n",
       "       -0.0161,  0.0819,  0.0462,  0.0757, -0.0353, -0.0005, -0.0374,\n",
       "        0.0742,  0.0223, -0.0946, -0.1229,  0.0592, -0.0906,  0.0152,\n",
       "        0.0079,  0.0082,  0.0798,  0.0627,  0.039 , -0.0375, -0.0349,\n",
       "        0.0622,  0.0475,  0.15  , -0.0187,  0.0322, -0.022 , -0.0316,\n",
       "        0.0112, -0.047 , -0.0203, -0.0388, -0.065 ,  0.0092,  0.0348,\n",
       "       -0.0659, -0.0013, -0.0193,  0.0135,  0.107 , -0.0036,  0.1151,\n",
       "       -0.0305,  0.0351,  0.048 , -0.0557,  0.0119, -0.0525,  0.0223,\n",
       "       -0.0481,  0.0016,  0.    , -0.064 , -0.0134,  0.0215,  0.0379,\n",
       "        0.0132, -0.0662, -0.052 ,  0.008 , -0.07  , -0.0339, -0.0841,\n",
       "       -0.0654,  0.0335, -0.0075,  0.0223, -0.0652,  0.0867,  0.0641,\n",
       "        0.0096, -0.0016, -0.0759,  0.065 ,  0.0489,  0.002 , -0.018 ,\n",
       "        0.0956, -0.0514, -0.0756, -0.0415,  0.0291,  0.037 ,  0.1755,\n",
       "       -0.1165,  0.0315,  0.0062,  0.0755,  0.0042, -0.0289, -0.0303,\n",
       "        0.005 ,  0.0044, -0.0292,  0.0854, -0.0633, -0.0785, -0.107 ,\n",
       "        0.0359,  0.0976,  0.0862, -0.1313, -0.0934,  0.028 , -0.0032,\n",
       "       -0.0621, -0.0291, -0.0442,  0.0139, -0.0586, -0.0524,  0.0477,\n",
       "       -0.0016, -0.0896, -0.0908, -0.0112, -0.0044, -0.0746, -0.0027,\n",
       "       -0.0836,  0.0198,  0.0784, -0.0483, -0.0066,  0.0225,  0.0222,\n",
       "       -0.0449,  0.099 , -0.0449, -0.0031, -0.0109,  0.118 ,  0.0721,\n",
       "        0.0149,  0.0304,  0.0647, -0.0752,  0.0055, -0.0797, -0.0299,\n",
       "        0.0683, -0.0884,  0.0812, -0.052 ,  0.0357, -0.0038, -0.0097,\n",
       "       -0.0773, -0.0007, -0.0419, -0.0506,  0.0127,  0.0013,  0.026 ,\n",
       "       -0.0425,  0.0599,  0.0858,  0.0184,  0.0362, -0.0343, -0.0092,\n",
       "        0.0718,  0.0069,  0.0876,  0.032 ,  0.0162, -0.032 , -0.0303,\n",
       "       -0.0261,  0.0084, -0.0513, -0.0613, -0.0052,  0.0526, -0.0562,\n",
       "        0.0257,  0.007 , -0.0097, -0.0123, -0.0587,  0.0189,  0.0582,\n",
       "       -0.0358,  0.1046,  0.0476,  0.0572,  0.0823, -0.0117,  0.0491,\n",
       "        0.0832, -0.0165, -0.1312, -0.0089, -0.0404, -0.0476,  0.0269,\n",
       "        0.0548,  0.0015, -0.0486,  0.0395,  0.0197, -0.0518, -0.0533,\n",
       "       -0.0094, -0.0095, -0.0011, -0.0729, -0.011 , -0.0041,  0.0659,\n",
       "        0.0643,  0.0749, -0.0636,  0.0035, -0.0275, -0.0581,  0.0679,\n",
       "       -0.0089,  0.0413, -0.0111,  0.0976,  0.0965,  0.0088,  0.018 ,\n",
       "       -0.0112,  0.0805, -0.0398, -0.0136, -0.0451,  0.0702, -0.0511,\n",
       "        0.0086,  0.0126, -0.0557,  0.0273,  0.0354, -0.0313],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddding_index['laptop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words missing : 164\n",
      "percentage of words missing from vocabulary: 0.5031755284877121\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "missing_words = 0\n",
    "threshold =20\n",
    "\n",
    "for word,count in word_count.items():\n",
    "    if count>threshold:\n",
    "        if word not in embeddding_index:\n",
    "            missing_words +=1\n",
    "            \n",
    "missing_ratio = (missing_words/len(word_count))*100\n",
    "\n",
    "print(\"Total words missing :\",missing_words)\n",
    "print(\"percentage of words missing from vocabulary:\",missing_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 32593\n",
      "Number of words we will use: 24821\n",
      "Percentage of words we will use:76.14999999999999%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {}\n",
    "\n",
    "value = 0\n",
    "\n",
    "for word,count in word_count.items():\n",
    "    if count>=threshold or word in embeddding_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value +=1\n",
    "\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
    "\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "    \n",
    "int_to_vocab = {}\n",
    "for word,value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "    \n",
    "usage_ratio = round(len(vocab_to_int) / len(word_count),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\",len(word_count))\n",
    "print(\"Number of words we will use:\",len(vocab_to_int))\n",
    "print(\"Percentage of words we will use:{}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding shape: (24821, 300)\n",
      "\n",
      "\n",
      "[[ 0.0827      0.0553      0.0454     ...  0.0376     -0.0696\n",
      "   0.0053    ]\n",
      " [ 0.0807      0.0509      0.0147     ... -0.0614     -0.0278\n",
      "  -0.0197    ]\n",
      " [ 0.1107      0.0117      0.0679     ... -0.0424      0.0088\n",
      "  -0.0626    ]\n",
      " ...\n",
      " [ 0.9389164  -0.7318939   0.26046044 ... -0.76705766 -0.587462\n",
      "   0.5616212 ]\n",
      " [ 0.365709    0.25560367 -0.6678523  ...  0.42861637  0.42082453\n",
      "  -0.9860329 ]\n",
      " [-0.73814917 -0.4024849  -0.80661803 ...  0.74043435  0.07302696\n",
      "   0.26868775]]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "word_embedding_matrix = np.zeros((nb_words,embedding_dim),dtype = np.float32)\n",
    "\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddding_index:\n",
    "        word_embedding_matrix[i] = embeddding_index[word]\n",
    "    else:\n",
    "        new_embedding = np.array(np.random.uniform(-1.0,1.0,embedding_dim))\n",
    "        \n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "        \n",
    "print(\"Word embedding shape:\",word_embedding_matrix.shape)\n",
    "print(\"\\n\")\n",
    "print(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24820"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int[\"<GO>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text,word_count,unk_count,eos=False):\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count +=1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count +=1\n",
    "                \n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_intb_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints,word_count,unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1504746\n",
      "Total number of UNKs in headlines: 62614\n",
      "Percent of words that are UNK:4.16%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "clean_summaries = list(data['Cleaned_Summary'].values)\n",
    "clean_text = list(data['Cleaned_Text'].values)\n",
    "\n",
    "int_summaries,word_count,unk_count = convert_to_ints(clean_summaries,word_count,unk_count)\n",
    "int_texts,word_count,unk_count = convert_to_ints(clean_text,word_count,unk_count)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\",word_count)\n",
    "print(\"Total number of UNKs in headlines:\",unk_count)\n",
    "print(\"Percent of words that are UNK:{}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length(text):\n",
    "    lengths=[]\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths,columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_summaries = create_length(int_summaries)\n",
    "lengths_texts = create_length(int_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  35172.000000\n",
      "mean       4.199960\n",
      "std        2.685725\n",
      "min        0.000000\n",
      "25%        2.000000\n",
      "50%        4.000000\n",
      "75%        5.000000\n",
      "max       30.000000\n",
      "Texts:\n",
      "             counts\n",
      "count  35172.000000\n",
      "mean      38.582537\n",
      "std       36.879902\n",
      "min        0.000000\n",
      "25%       16.000000\n",
      "50%       27.000000\n",
      "75%       47.000000\n",
      "max      745.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count +=1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31111\n",
      "31111\n"
     ]
    }
   ],
   "source": [
    "sorted_summares = []\n",
    "sorted_texts = []\n",
    "max_text_length = 199\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 100\n",
    "unk_summary_limit = 100\n",
    "\n",
    "for length in range(min(lengths_texts.counts),max_text_length):\n",
    "    for count,words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count])>= min_length and \n",
    "            len(int_summaries[count])<= max_summary_length and \n",
    "            len(int_texts[count])>= min_length and \n",
    "            unk_counter(int_summaries[count])<=unk_summary_limit and\n",
    "            unk_counter(int_texts[count])<=unk_text_limit and \n",
    "            length == len(int_texts[count])\n",
    "            ):\n",
    "                sorted_summares.append(int_summaries[count])\n",
    "                sorted_texts.append(int_texts[count])\n",
    "                \n",
    "# Compare the lenth to enusre they match\n",
    "print(len(sorted_summares))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Building the Model\n",
    "\n",
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  inputs_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    \n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                              attn_mech,\n",
    "                                                              rnn_size)\n",
    "    \n",
    "    initial_state =  dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "#     initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n",
    "#                                                                     _zero_state_tensors(rnn_size, \n",
    "#                                                                                         batch_size, \n",
    "#                                                                                         tf.float32))\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "#         training_logits = training_decoding_layer(dec_embed_input, \n",
    "#                                                   targets_length, \n",
    "#                                                   dec_cell, \n",
    "#                                                   initial_state,\n",
    "#                                                   output_layer,\n",
    "#                                                   vocab_size, \n",
    "#                                                   max_target_length)\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=targets_length,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer) \n",
    "        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                            output_time_major=False,\n",
    "                                            impute_finished=True,\n",
    "                                            maximum_iterations=max_target_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "#         inference_logits = inference_decoding_layer(embeddings,  \n",
    "#                                                     vocab_to_int['<GO>'], \n",
    "#                                                     vocab_to_int['<EOS>'],\n",
    "#                                                     dec_cell, \n",
    "#                                                     initial_state, \n",
    "#                                                     output_layer,\n",
    "#                                                     max_target_length,\n",
    "#                                                     batch_size)\n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                            output_time_major=False,\n",
    "                                            impute_finished=True,\n",
    "                                            maximum_iterations=max_target_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cell 33:'''\n",
    "# Set the Hyperparameters\n",
    "epochs = 50 # use 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 3\n",
    "learning_rate = 0.008\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding_layer() missing 1 required positional argument: 'direction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-97260ddacd34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                                       \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                                       \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                                                       batch_size)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Create tensors for the training logits and inference logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-605aa3e33839>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[1;34m(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m                                                         \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                                                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                                                         num_layers)\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtraining_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding_layer() missing 1 required positional argument: 'direction'"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.seq2seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
